{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Classification_student.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fIdNNcRgKVCo",
        "colab_type": "text"
      },
      "source": [
        "# Week 5: Introduction to classification methods \n",
        "\n",
        "----------------------------------------------------\n",
        "Machine Learning     Year 2019/2020\n",
        "\n",
        "*Vanessa Gómez Verdejo vanessa@tsc.uc3m.es* and *Pablo M. Olmos olmos@tsc.uc3m.es*\n",
        "\n",
        "----------------------------------------------------\n",
        "\n",
        "In this lab session we are going to go deep in our knowledge about classifiers by introducing the most well-known classification algorithms. Besides, we are going to review some useful techniques, such as the cross validation process, which will allow us to adjust the free parameters of these methods. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SyVl4XXas5nu",
        "colab_type": "text"
      },
      "source": [
        "## Classification formulation\n",
        "\n",
        "\n",
        "Consider we have a **training** database of $N$ entries of the form $(\\mathbf{x}^{(i)},y^{(i)})$, where $\\mathbf{x}\\in\\mathbb{R}^D$ is a sample and $y$  is its associated label. As we are working with classification problem, the label $y$ only belongs to **two possible categories**, $y\\in\\{0,1\\}$, in binary problems or to **M categories**, $y\\in\\{0,1, \\ldots, M-1\\}$,  in multiclass scenarios. \n",
        "\n",
        "Our goal in classification is to propose a hipothesis function $h(\\mathbf{x},y)$ that will be used to estimate the most likely class of a new point $\\mathbf{x}^*$ as follows\n",
        "\n",
        "\\begin{align}\n",
        "\\hat{y}^* =  {\\arg \\max}_{y\\in\\{0,1,\\ldots, M-1\\}} h(\\mathbf{x}^*,y)\n",
        "\\end{align}\n",
        "\n",
        "In case the classifier output matches the real target value ($\\hat{y}^*=y^*$) there is hit, otherwise ($\\hat{y}^*\\neq y^*$) the decision is an error. So, in the ideal case the classifier desing should be aimed to optimize a loss or cost function that minimize the averaged number of missclassified patterns (**classification error, CE**), \n",
        "$$ CE = \\frac{1}{N}\\sum_{i=1}^N \\mathbb{I}[y^{(i)}\\neq \\hat{y}^{(i)}] $$\n",
        "where $\\mathbb{I}[\\cdot]$ is the indicator function that takes value $1$ if the condition within the brakets is met, and zero otherwise.\n",
        "\n",
        "\n",
        "However, as we will see later, the minimization of this cost function is usually not simple (or feasible) and ML techniques prefer to minimize any  upperbound of the classification error. \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Xo3M4oV12nr2",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "In other cases, the classifier design resorts to **Bayesian decision theory**, where it is known that the decisor with minimum probability of error  is given by the MAP (Maximum a Posteriori) decision maker:\n",
        "\\begin{align}\n",
        "\\hat{y}^* =  {\\arg \\min}_{y\\in\\{0,1,\\ldots, M-1\\}} P(Y \\neq y|\\mathbf{x}^*) = {\\arg \\max}_{y\\in\\{0,1,\\ldots, M-1\\}} P(Y = y|\\mathbf{x}^*) \n",
        "\\end{align}\n",
        "\n",
        "being $P(Y=y|\\mathbf{x}^*)$ the posterior probability of class $y$. So, these classifiers firstly estimates the posterior probabilities of each class in order to be able to apply the previous decision criterion.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TRV_NKTvKm9l",
        "colab_type": "text"
      },
      "source": [
        "## Load and prepare the data\n",
        "\n",
        "For this lab session, lest's work over the [Iris data set](https://scikit-learn.org/stable/datasets/index.html#iris-plants-dataset) which consists of 150 patterns corresponding to 3 different types of irises: Setosa, Versicolour, and Virginica. Each pattern contains the sepal and petal lengths and widths. Despite having four input features, for display purposes, we are going to start working with the two features: sepal width (feature 1) and sepal width (feature 3). \n",
        "\n",
        "For prepare the data, next code cell let you:\n",
        "* Load the dataset\n",
        "* Create training and testing partitions with the 60% and 40% of the original data\n",
        "* Normalize the data to zero mean and unitary standard deviation \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hM30AytfKrAQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "%matplotlib inline\n",
        "\n",
        "# Initialize the random generator seed to compare results\n",
        "np.random.seed(0)\n",
        "\n",
        "# Load Iris data set\n",
        "iris = datasets.load_iris()\n",
        "sel_feat = [1, 3]\n",
        "X = iris.data[:,sel_feat]  # we only take features 1 and 3.\n",
        "Y = iris.target\n",
        "\n",
        "# Create data partitions\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=.4)\n",
        "\n",
        "# Normalize the data\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9nQaT6L2MIrW",
        "colab_type": "text"
      },
      "source": [
        "## 1. K-NN Classifier\n",
        "\n",
        "In this introduction to classifier methos, let's start with a **nonparametric** classifier: The K-NN classifier. This classifier is actually very similar to the K-NN regressor, since it merely replaces the average nearest neighbour value by a mayority voting scheme. \n",
        "\n",
        "Given a new point $\\mathbf{x}^*$, the predicted label is obtained as\n",
        "\n",
        "\\begin{align}\n",
        "\\hat{y}^* =  {\\arg \\max}_{y\\in\\{0,1,\\ldots, M-1\\}} h(\\mathbf{x}^*,y)\n",
        "\\end{align}\n",
        "\n",
        "where the hipothesis function, $h(\\mathbf{x},y)$, is given by:\n",
        "\n",
        "\\begin{align}\n",
        "h(\\mathbf{x}^*,y) = \\frac{1}{K}\\sum_{k\\in \\mathcal{S}_{K}(\\mathbf{x}^*)} \\mathbb{I}[y^{(k)}==y],\n",
        "\\end{align}\n",
        "\n",
        "where $\\mathbb{I}[\\cdot]$ is the indicator function that takes value $1$ if the condition within the brakets is met, and zero otherwise. Also, $\\mathcal{S}_{K}$ is the set of the **$K$ training points that are closest to $\\mathbf{x}^*$ according to a given distance metric $d(\\mathbf{x}^{(i)},\\mathbf{x}^*)$**. For instance, in real spaces the most common choice is the **euclidean distance**:\n",
        "\n",
        "\\begin{align}\n",
        "d(\\mathbf{x}^{(i)},\\mathbf{x}^*) = \\left|\\left|\\mathbf{x}^{(i)}-\\mathbf{x}^*\\right|\\right|^2\n",
        "\\end{align}\n",
        "\n",
        "As the regression version, we can find many variants of the above expression; for instance, we can **weight differently the indicator terms according to the distance**. \n",
        "\n",
        "Note that $h(\\mathbf{x},y)$ is approximating the posterior probability of each class, $P(Y=y|\\mathbf{x}^*)$, by averaging the number of neigbours that are belonging to each class. So the final classification criteria is a MAP decisor:\n",
        "\\begin{align}\n",
        "y* = {\\arg \\max}_{y\\in\\{0,1,\\ldots, M-1\\}} P(Y=y|\\mathbf{x}^*) \n",
        "\\end{align}\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FouSud8PPKYG",
        "colab_type": "text"
      },
      "source": [
        "### Exercise 1\n",
        "\n",
        "Analize the K-NN performance:\n",
        "1. For a range of K values from 1 to 20 analyze the train and test performance (accuracy$^1$). \n",
        "2. Select the optimum value of K using a 10-fold CV.\n",
        "3. Compute the test accuracy for the optimum value of K.\n",
        "4. Plot the classification boundary.\n",
        "\n",
        "\n",
        "(1) In classification problems, the performance is ussually evaluated either in terms of the classfication error (CE) rate or the **accuracy** (average number of hits):\n",
        "\n",
        "$$ CE = \\frac{1}{N}\\sum_{i=1}^N \\mathbb{I}[y^{(i)}\\neq \\hat{y}^{(i)}] $$\n",
        "\n",
        "$$ Acc =  1- CE = \\frac{1}{N}\\sum_{i=1}^N \\mathbb{I}[y^{(i)}= \\hat{y}^{(i)}] $$\n",
        "\n",
        "You can use the [*sklearn K-NN classifier implementation*](http://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsClassifier.html) toghether to its score() method (which computes the classifier accuracy), as well as the [GridSearchCV( )](http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.GridSearchCV.html) function to complete this exercise. Besides, next cell provides you the function plot_boundary(), which can help you to plot the classification boundary."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0wzBpmKnRNgo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot the decision boundary\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_boundary(clf, X, Y, plt):\n",
        "    \"\"\"Plot the classification regions for a given classifier.\n",
        "\n",
        "    Args:\n",
        "        clf: scikit-learn classifier object.\n",
        "        X (numpy dnarray): training or test data to be plotted (number data x number dimensions). Only frist two \n",
        "                            dimensions are ploted\n",
        "        Y (numpy dnarray): labels of the training or test data to be plotted (number data x 1).\n",
        "        plt: graphic object where you wish to plot                                             \n",
        "   \n",
        "    \"\"\"\n",
        "\n",
        "    plot_colors = \"brymc\"\n",
        "    plot_step = 0.02\n",
        "    n_classes = np.unique(Y).shape[0]\n",
        "    # Plot the decision regions\n",
        "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n",
        "                        np.arange(y_min, y_max, plot_step))\n",
        "\n",
        "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    cs = plt.contourf(xx, yy, Z, cmap=plt.cm.Paired)\n",
        "\n",
        "    plt.xlabel('Feature 1')\n",
        "    plt.ylabel('Feature 2')\n",
        "    plt.axis(\"tight\")\n",
        "\n",
        "    # Plot the training points\n",
        "    for i, color in zip(range(n_classes), plot_colors):\n",
        "        idx = np.where(Y == i)\n",
        "        plt.scatter(X[idx, 0], X[idx, 1], c=color, cmap=plt.cm.Paired)\n",
        "\n",
        "    plt.axis(\"tight\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tHPmpowb5QhC",
        "colab_type": "text"
      },
      "source": [
        "#### SOLUTION 1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GHbhrh3jLQyB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# <SOL>\n",
        "# </SOL>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wapb4lyCPY3u",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# <SOL>\n",
        "# </SOL>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EZA12Ulp5pvD",
        "colab_type": "text"
      },
      "source": [
        "#### SOLUTION 2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4J56Je0nPf-g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# <SOL>\n",
        "# </SOL>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WLjlAVdnP1Zu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# <SOL>\n",
        "# </SOL>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q3-rC2fy5sIu",
        "colab_type": "text"
      },
      "source": [
        "#### SOLUTION 3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k9rYdUwBQKr9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# <SOL>\n",
        "# </SOL>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yoVBQiTL5yui",
        "colab_type": "text"
      },
      "source": [
        "#### SOLUTION 4"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N2ONuPs4Qvj6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# <SOL>\n",
        "# </SOL>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hjHi9iO4RH9d",
        "colab_type": "text"
      },
      "source": [
        "## 2. Logistic Regression (LR)\n",
        "\n",
        "2.1. **The LR model** \n",
        "\n",
        "The **logistic regression (LR)** model assumes that the posterior probabilities are given by the following expressions (for the binary case$^1$):\n",
        "\n",
        "$$P(Y=1| {\\bf x}) =\\displaystyle \\frac{\\exp({\\bf w}^T {\\bf x})}{1+\\exp({\\bf w}^T {\\bf x})} =  \\frac{1}{1+\\exp(-{\\bf w}^T {\\bf x})}$$\n",
        "$$ P(Y=0| {\\bf x}) = 1- P(Y=1| {\\bf x}) = \\frac{1}{1+\\exp({\\bf w}^T {\\bf x})}$$\n",
        "\n",
        "where the weight vector $\\bf w$ will be the classifier parameters to be learnt. \n",
        "\n",
        "The definition of these posterior probabilities makes possible to obtain a  **linear classifier**, since the classification boundary of the MAP classifier is linear (this boundary is defined by the points where $P(Y=1| {\\bf x})=P(Y=0| {\\bf x})$): \n",
        "$$\\log \\frac{P(Y=1| {\\bf x})}{P(Y=0| {\\bf x})} = {\\bf w}^T {\\bf x} = 0$$\n",
        "\n",
        "(1) In the multiclass case, M posterior distributions are defined using $M-1$ weigth vectors and the class $0$ distribution ensures that the sum of all of them is 1. Besides, applying the MAP criterium $M-1$ linear boundaries are obtained.\n",
        "\n",
        "2.2. **Model Inference**\n",
        "\n",
        "To learn the value of $\\bf w$ (**inference learning**), the LR model maximizes the likelihood of $\\bf w$ over the training data, i.e,\n",
        "\n",
        "$$\\bf w^* =  \\displaystyle \\underset{{\\bf w}}{\\operatorname{max}} \\prod_{i=1}^N p(y^{(i)}|\\mathbf{x}^{(i)},\\bf w) = \\displaystyle \\underset{{\\bf w}}{\\operatorname{max}}  L({\\bf w})$$\n",
        "\n",
        "Taking into account that the probability likelihood of $\\bf w$ over the data ${\\bf x}$ is $P(Y=1| {\\bf x})=\\frac{1}{1+\\exp(-{\\bf w}^T {\\bf x})}$ if its label is 1, whereas it is $1- P(Y=1| {\\bf x}) $ if it belongs to class $0$; so, the  likelihood over all training data is$^2$:\n",
        "\n",
        "$$L({\\bf w}) = \\prod_{i=1}^N P(Y=1| {\\bf x}^{(i)})^{y^{(i)}}\\left(1- P(Y=1| {\\bf x}^{(i)}) \\right)^{1-y^{(i)}} $$\n",
        "\n",
        "and, then, the log-likelihood for N observations is:\n",
        "\n",
        "$$l({\\bf w}) =\\log{L({\\bf w})} = \\sum_{i=1}^N \\left\\lbrace  y^{(i)}\\log \\left(  P(Y=1| {\\bf x}^{(i)})\\right)  + (1-y^{(i)}) \\log\\left(  1- P(Y=1| {\\bf x}^{(i)}) \\right)  \\right\\rbrace   $$\n",
        "$$l({\\bf w}) = \\sum_{i=1}^N \\left\\lbrace  y^{(i)} ({\\bf w}^T {\\bf x}^{(i)}) - \\log \\left( 1+ \\exp({\\bf w}^T {\\bf x}^{(i)})\\right) \\right\\rbrace   $$\n",
        "\n",
        "So, the optimal value of $\\bf w$ can be found as the solution of the following optimization problem:\n",
        "$$\\bf w^* =\\displaystyle \\underset{{\\bf w}}{\\operatorname{min}}\\sum_{i=1}^N \\left\\lbrace   \\log \\left( 1+ \\exp({\\bf w}^T {\\bf x}^{(i)})\\right) \\right\\rbrace  -y^{(i)} ({\\bf w}^T {\\bf x}^{(i)}) $$  \n",
        "\n",
        "\n",
        "As this cost function has not a closed form, gradient descend algorithms or the Newton method are usually used to find its minimum.\n",
        "\n",
        "(2) In the multiclass case, the overall likelihood is defined using the multinomial distribution instead of the binomial one.\n",
        "\n",
        "**References**\n",
        "\n",
        "T. Hastie, R. Tibshirani and J. Friedman. “Elements of Statistical Learning”. Chapter 4: ``Linear Methods for Classification''. Springer, 2009.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2c4kK-K79vyO",
        "colab_type": "text"
      },
      "source": [
        "**Cost funcion analysis**\n",
        "\n",
        "This cost function,\n",
        "$$\\bf w^* =\\displaystyle \\underset{{\\bf w}}{\\operatorname{min}}\\sum_{i=1}^N \\left\\lbrace   \\log \\left( 1+ \\exp({\\bf w}^T {\\bf x}^{(i)})\\right) \\right\\rbrace  -y^{(i)} ({\\bf w}^T {\\bf x}^{(i)}) $$  \n",
        "known a binomial deviance, is un upper bound of the classification error."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "umzSY5MjRAaG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot the logistic regression loss function (un upper bound of the classfication error)\n",
        "f = np.arange(-4,4,0.01)\n",
        "y = 1\n",
        "l_w = np.log(1+ np.exp(f))-y*f\n",
        "plt.figure()\n",
        "plt.plot(y*f,l_w, label='Binomial deviance')\n",
        "\n",
        "\n",
        "# Classification error\n",
        "e_class = np.zeros(f.shape)\n",
        "e_class[y*f<0] =1\n",
        "plt.plot(y*f,e_class, label='Classification error')\n",
        "\n",
        "plt.legend()\n",
        "plt.xlabel('y*f')\n",
        "plt.ylabel('Loss function')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DPDC2g_V9M3M",
        "colab_type": "text"
      },
      "source": [
        "## Extensions of the LR model\n",
        "\n",
        "2.3. **Regularized versions**\n",
        "\n",
        "**Regularized Logistic Regression**: we can add a prior over ${\\bf w}$ to the inference process and maximize the posterior distribution of $\\bf w$ (instead of its likelihood). In this way, we can arrive to an equivalent loss function but with a regularization term:\n",
        "\n",
        "* L2 regularization:\n",
        "\n",
        "$$ \\bf w^* = \\displaystyle \\underset{{\\bf w}}{\\operatorname{min}} \\sum_{i=1}^N \\left\\lbrace   \\log \\left( 1+ \\exp({\\bf w}^T {\\bf x}^{(i)})\\right) \\right\\rbrace  -y^{(i)} ({\\bf w}^T {\\bf x}^{(i)}) + C \\Vert {\\bf w} \\Vert_2^2$$\n",
        "\n",
        "* L1 regularization:\n",
        "\n",
        "$$ \\bf w^* = \\displaystyle \\underset{{\\bf w}}{\\operatorname{min}} \\sum_{i=1}^N \\left\\lbrace  \\log \\left( 1+ \\exp({\\bf w}^T {\\bf x}^{(i)})\\right) \\right\\rbrace  -y^{(i)} ({\\bf w}^T {\\bf x}^{(i)}) + C \\Vert {\\bf w} \\Vert_1$$\n",
        "\n",
        "In general, most LR implementations are using the L2 regularized version. Only in case a feature selection is desired, the L1 regularization is prefered.\n",
        "\n",
        "2.4 **No -linear RL**: as in linear regression models, we can obtain a non-linear version of the LR model by applying a polynomial expansion of the input data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FrMVzpdkPEkX",
        "colab_type": "text"
      },
      "source": [
        "### Exercise 2\n",
        "\n",
        "Analize the LR performance:\n",
        "1. Compute the test accuracy of the L2-LR model. Use a 10-fold CV process to adjust the value of C (you can explore 10 values of C logarithmically spaced from 0.001 to 1000)\n",
        "2. Plot the classification boundary.\n",
        "3. Can we improve its performance with a 2-degree polynomical expansion?\n",
        "\n",
        "To complete this exercise, you can use the [*sklearn LR implementation*](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.LogisticRegression.html),  the [GridSearchCV( )](http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.GridSearchCV.html) function. Besides, next cell includes an extension of previous function plot_boundary(), called plot_boundary_poly(), which adapts the previous code to be used with ploynomial data extensions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bwinBEwZZ12k",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot the decision boundary\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_boundary_poly(clf, X, Y, plt, transformer, polynomial_features):\n",
        "    \"\"\"Plot the classification regions for a given classifier.\n",
        "\n",
        "    Args:\n",
        "        clf: scikit-learn classifier object.\n",
        "        X (numpy dnarray): training or test data to be plotted (number data x number dimensions). Only frist two \n",
        "                            dimensions are ploted\n",
        "        Y (numpy dnarray): labels of the training or test data to be plotted (number data x 1).\n",
        "        plt: graphic object where you wish to plot     \n",
        "        transformer: transformer class of sklearn used to normalized the data (the object has already been fitted)\n",
        "        polynomial_features: polynomial_features class of sklearn used to create a polynomial extension of the data (the object has already been fitted)\n",
        "   \n",
        "    \"\"\"\n",
        "\n",
        "    plot_colors = \"brymc\"\n",
        "    plot_step = 0.02\n",
        "    n_classes = np.unique(Y).shape[0]\n",
        "    # Plot the decision regions\n",
        "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n",
        "                        np.arange(y_min, y_max, plot_step))\n",
        "    \n",
        "    X_plot = np.c_[xx.ravel(), yy.ravel()]\n",
        "    X_plot_pol = polynomial_features.transform(X_plot)\n",
        "    X_plot_pol =  transformer.transform(X_plot_pol)\n",
        "    \n",
        "    Z = clf.predict(X_plot_pol)\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    cs = plt.contourf(xx, yy, Z, cmap=plt.cm.Paired)\n",
        "\n",
        "    plt.xlabel('Feature 1')\n",
        "    plt.ylabel('Feature 2')\n",
        "    plt.axis(\"tight\")\n",
        "\n",
        "    # Plot the training points\n",
        "    for i, color in zip(range(n_classes), plot_colors):\n",
        "        idx = np.where(Y == i)\n",
        "        plt.scatter(X[idx, 0], X[idx, 1], c=color, cmap=plt.cm.Paired)\n",
        "\n",
        "    plt.axis(\"tight\")\n",
        "    \n",
        "    "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6qA3AyGRDd4E",
        "colab_type": "text"
      },
      "source": [
        "#### SOLUTION 2.1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ih7r1QSmRARy",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# <SOL>\n",
        "# </SOL>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ot-O8T2dDf09",
        "colab_type": "text"
      },
      "source": [
        "#### SOLUTION 2.2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r3tIgAUkQxtE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# <SOL>\n",
        "# </SOL>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TP8lfLxrDhrj",
        "colab_type": "text"
      },
      "source": [
        "#### SOLUTION 2.3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kfwIjPZ2Zdj1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# <SOL>\n",
        "# </SOL>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p_uABqT0olGk",
        "colab_type": "text"
      },
      "source": [
        "# 3. Decision Trees\n",
        "\n",
        "1. **An example**\n",
        "\n",
        "<img src=\"http://www.tsc.uc3m.es/~vanessa/Figs_notebooks/ML/Classification/TREE_1.jpg\" width=\"48%\" > <img src=\"http://www.tsc.uc3m.es/~vanessa/Figs_notebooks/ML/Classification/TREE_2.jpg\" width=\"48%\"> \n",
        "\n",
        "\n",
        "2. **Training a decision tree** \n",
        "\n",
        "* Inputs: $ \\left\\lbrace {\\bf x}^{(i)} \\right\\rbrace_{i=1}^N \\in \\Re^D$ and $ \\left\\lbrace y^{(i)} \\right\\rbrace_{i=1}^N \\in \\{0,1, \\ldots, M-1\\} $.\n",
        "\n",
        "* for $d=1:D$                                                             \\# For each feature\n",
        "  * for $u_{d,i} \\in$ {all values of $x_d$}            \\# Explore threshold values\n",
        "    * The node splits the data: \n",
        "    \n",
        "        $X_{\\rm left}=\\left\\lbrace {\\bf x}^{(i)}, y^{(i)} \\right\\rbrace_{i \\in S_L}$, where $S_L$ is the set of data with $x_d<u_{d,i}$\n",
        "        \n",
        "        $ X_{\\rm right}=\\left\\lbrace {\\bf x}^{(i)}, y^{(i)} \\right\\rbrace_{i \\in S_R}$, where $S_R$ is the set of data with $x_d>u_{d,i}$\n",
        "    \n",
        "    * Evaluate the impurity of this split as\n",
        "    \n",
        "        $ G(u_{d,i}) = \\displaystyle \\frac{n_{\\rm left}}{N} g(X_{\\rm left}) + \\frac{n_{\\rm right}}{N} g(X_{\\rm right}) $\n",
        "    \n",
        "         where $ g(X) $ is the index Gini defined as :\n",
        "    \n",
        "       $ g(X) = \\displaystyle \\sum_{m=0}^{M-1} P_m \\left( 1- P_m\\right) $\n",
        "        \n",
        "\t\t being $P_m$ the fraction of items classified in the class $J$.\n",
        "\t\t\n",
        "* Select threshold ($u_{d,i}$) and feature ($x_d$) minimizing $G(u_{d,i})$\n",
        "* Split the data according to $x_d$ and threshold $u_{d,i}$\n",
        "* Apply these steps recursively  to generate the following nodes.\n",
        "\n",
        "3. **Some comments**\n",
        "* Note that the tree training does not minimize any cost function (any bound over the classification error); however, after the training, each tree leave has computed the posterior class probabilities and the Gini index tries that these probabilities are close to 0 or 1. That is,  the  minimization of Gini index aims at getting leaves ``pure enough''.\n",
        "* This model can be easily extended to regression problems using as $ g(X) $ function the mean square (or absolute) error.\n",
        "* The resulting classifier is easy to understand and to interpret \n",
        "*  We do not need data preprocessing and can easily handle both numerical and categorical data.\n",
        "\n",
        "**References**\n",
        "\n",
        "- L. Breiman, J. Friedman, R. Olshen, and C. Stone, “Classification and Regression Trees”, Wadsworth, Belmont, CA, 1984.\n",
        "\n",
        "- T. Hastie, R. Tibshirani and J. Friedman. “Elements of Statistical Learning”. Chapter 9: ''Additive Models, Trees, and Related Methods''. Springer, 2009.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2OcoW_x5RByg",
        "colab_type": "text"
      },
      "source": [
        "### Exercise 3.1\n",
        "**Tree performance analysis**\n",
        "\n",
        "Compute the test accuracy of a tree. For now, do not cross validate any parameter and use the default values.\n",
        "\n",
        "To complete this exercise, you can use the [*sklearn Decision Tree Classifier*](https://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier). Besides, the following cell code provides you the code to show the structure of the trained decision tree. To use it, you only have to create an object called *clf_tree* with the DecisionTreeClassifier of sklearn, fit it over the training data and run the provided code. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7RY8gAhqltjC",
        "colab_type": "text"
      },
      "source": [
        "#### SOLUTION 3.1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ux2i_t8BosR7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# <SOL>\n",
        "# </SOL>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ATLg1oo_osOa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Code to plot the tree structure\n",
        "\n",
        "import pydotplus \n",
        "from IPython.display import Image  \n",
        "dot_data = tree.export_graphviz(clf_tree, out_file=None, \n",
        "                         feature_names=[iris.feature_names[i] for i in sel_feat],  \n",
        "                         class_names=iris.target_names,  \n",
        "                         filled=True, rounded=True,  \n",
        "                         special_characters=True)  \n",
        "graph = pydotplus.graph_from_dot_data(dot_data)  \n",
        "Image(graph.create_png())  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "syrqwVa4uYia",
        "colab_type": "text"
      },
      "source": [
        "### Exercise 3.2\n",
        "\n",
        "**Plot the decision boundary**\n",
        "\n",
        "Use the function plot_boundary(), used in Section 1, to plot the classification boundary of the decision tree. Then, try to answer the following questions:\n",
        "1. Why do you think that the classification boundary is a combination of vertical and horizontal lines?\n",
        "2. Do you think the classification boundary is smooth or does it change abruptly? What do you think might be the reason?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "14Ru4gdmmFFH",
        "colab_type": "text"
      },
      "source": [
        "#### SOLUTION 3.2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PHmMKQG4TRgi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# <SOL>\n",
        "# </SOL>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DWSgA0zWThkn",
        "colab_type": "text"
      },
      "source": [
        "### Exercise 3.3\n",
        "**Preventing overfitting problems**\n",
        "\n",
        "Decision trees can create very complex structures that do not generalize, causing overadjustment problems. This is mainly due to the fact that, as the depth of the tree increases, we divide the samples that arrive at each node and the nodes of the deepest branches have very few samples to learn. To avoid this effect, we can control two parameters:\n",
        "* **The depth of the tree**. This can be controlled with the parameter *max_depth*. Note that if we have a tree depth of $P$ and in each node the training data are divided in two equal sets, the last nodes would have $N/2^P$ samples. So, according to the number of samples we can set a maximum depth.\n",
        "* **The minimum number of samples required to divide a node**. This can be set with the parameters *min_samples_split* (the minimum number of samples required to split an internal node) or *min_samples_leaf* (the minimum number of samples required to be at a leaf node).\n",
        "\n",
        "Train again the previous decision tree and plot its boundary for different values of these parameters."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3UMiXmx8mfKq",
        "colab_type": "text"
      },
      "source": [
        "#### SOLUTION 3.3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IVO-PdjTWRU8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# <SOL>\n",
        "# </SOL>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBeHIw-cWR_s",
        "colab_type": "text"
      },
      "source": [
        "### Other overfitting problems\n",
        "\n",
        "Decission trees also tend to present overfitting problems when we work in spaces with a high dimensionality and there is a few number of data ($D>>N$), as the tree can easily find features leading to pure nodes over the training data and causing a bad generalization. The only way to alliviate this problem is to increase the number of training data, which is not always possible. \n",
        "\n",
        "However, this effect can be mitigated by building sets of trees where each tree uses a subset of input featues, as we will see in the next section. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZZASZQhdorFx",
        "colab_type": "text"
      },
      "source": [
        "## 4. Random Forest\n",
        "\n",
        "1. **Introduction**\n",
        "\n",
        "A Random Forest (RF) trains several decision tree classifiers, where each one is trained with different samples and features of the training data, and averages their outputs to improve the final accuracy. The key point of this kind of arquitecture relies in getting a  low correlation among trees (**high diversity**)\n",
        "\n",
        "\t\n",
        "2. **Training a Random Forest**\n",
        "\n",
        "* Inputs: $ \\left\\lbrace {\\bf x}^{(i)} \\right\\rbrace_{i=1}^N\\in \\Re^D$ and $ \\left\\lbrace y^{(i)} \\right\\rbrace_{i=1}^N \\in   \\left\\lbrace 0, 1, ...M-1 \\right\\rbrace$, number of trees $T$.\n",
        "* for $t=1:T$                                                             \\# For each tree \n",
        "  * Sample with replacement from the original data set (boostrap sampling) : $L' (<L)$ data\n",
        "\t* Randomly select $D'(<D)$ features\n",
        "\t* Train a tree optimizing the index Gini with the sub data matrix ($L'\\times D'$). \n",
        "  \n",
        "\n",
        "3. **Test data classification**\n",
        "* Each test data (${\\bf x}^*$) is classified by all the tree and we obtain a set of posterior class probabilities. Once the forest is trained, each tree can obtain the posterior class probabilities for a new sample data ($P_t(Y=y|{\\bf x^*})$). \n",
        "* The forest classification rule is given by:\n",
        "$$ y^* = \\underset{y}{\\operatorname{argmax}} {\\frac{1}{T} \\sum_{t=1}^T P_t(Y=y|{\\bf x}^*)}$$\n",
        "where   $P_t(Y=y|{\\bf x}^*)$ is the output class probabilities.\n",
        "\n",
        "**References**\n",
        "- L. Breiman, “Random Forests”, Machine Learning, 45(1), 5-32, 2001.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hYC3qAHtf_C5",
        "colab_type": "text"
      },
      "source": [
        "### Exercise 4.1\n",
        "\n",
        "**RF performance analysis**\n",
        "\n",
        "Compute the test accuracy of a RF built with 100 decission trees an plot its classification boundary.  \n",
        "\n",
        "Use the [RandomForestClassifier( )](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html) function to train this RF classifier. Set the *max_depth* parameter to 2 and the left remaining parameters with their default values. \n",
        "\n",
        "Note that we are working with a two dimensional dataset, so subsampling features does not make sense in the particular example, but at least we can analyze the effect of data subsampling.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qe_Ge8-1n7jR",
        "colab_type": "text"
      },
      "source": [
        "#### SOLUTION 4.1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hspwmwEko1C7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# <SOL>\n",
        "# </SOL>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W0DVfK7-gD_G",
        "colab_type": "text"
      },
      "source": [
        "## Going deeper into the RF\n",
        "\n",
        "**1. The out-of-bag (OOB) error**\n",
        "\n",
        "As we know, for the training of each RF tree we only use a subset of samples, so in practice each training sample only participates in the training of some trees. We can use this to obtain an estimate of the error of this sample by classifying it with the set of trees over which it has not participated. If we do this with all the samples and average these errors, we obtain what is known as   The out-of-bag (OOB) error. This allows the RandomForestClassifier to be fit and validated whilst being trained. You can find an example of this validation procedure in this [link](https://scikit-learn.org/stable/auto_examples/ensemble/plot_ensemble_oob.html#sphx-glr-auto-examples-ensemble-plot-ensemble-oob-py).\n",
        "\n",
        "\n",
        "\n",
        "**2. Feature selection with Random Forest**\n",
        "\n",
        "We can use of forests of trees to evaluate the importance of features on a classification task. Note that the relative rank (or depth) of a feature used by a decision node provides us a relative importance of that feature. Analyzing this information over all the trees of a RF, we can obtain a good estimation of predictive ability of each fature and use it for feature selection. \n",
        "\n",
        "The example \"[Pixel importances with a parallel forest of trees](https://scikit-learn.org/stable/auto_examples/ensemble/plot_forest_importances_faces.html#sphx-glr-auto-examples-ensemble-plot-forest-importances-faces-py)\" shows the relative importances of each pixel for a face recognition task. You can check as sklearn RF classifier generates, on the fitted model, a variable 'feature_importances_' which stores these feature importances. This variable is an array with shape (n_features,) whose values are positive and sum to 1.0. The higher the value, the more important is the contribution of the matching feature to the prediction function.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hhg1wQoXo9eq",
        "colab_type": "text"
      },
      "source": [
        "# 5. Performance evaluation in multiclass problems"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vv7vnYcFNDzk",
        "colab_type": "text"
      },
      "source": [
        "### Exercise 5.1\n",
        "\n",
        "We normally use the .score( ) method of the object classifier to compute the accuracy of a classifier. Use the [metrics.accuracy\\_score( )](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.accuracy_score.html) function to calculate the classifier performance and check that you obtain the same value. Note that you need to obtain the classifier output for each test data to able to run the metrics.accuracy\\_score() function; review the method .predict() of the classifier object to compute this output.\n",
        "\n",
        "For the sake of simplicity, you can use a L2-LR classifier setting its C parameter to 100.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FqawhtCpt1dF",
        "colab_type": "text"
      },
      "source": [
        "#### SOLUTION 5.1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fPkxV_x0pOGh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# <SOL>\n",
        "# </SOL>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OEM8wqE6qFDb",
        "colab_type": "text"
      },
      "source": [
        "### Exercise 5.2\n",
        "\n",
        "\n",
        "**Confusion matrix**\n",
        "\n",
        "In multiclass problems, the confusion matrix provides the percentage of well classified data over each class. \n",
        "\n",
        "\n",
        "<img src=\"http://www.tsc.uc3m.es/~vanessa/Figs_notebooks/ML/Classification/confusion_matrix.jpg\" width=\"50%\"> \n",
        "\n",
        "\n",
        "Use the method [metrics.confusion_matrix()](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.confusion_matrix.html) to compute the confusion matrix over the performance of above LR classifier. Besides, you can use the following function (plot_confusion_matrix) to depict the object CM returned by the confusion_matrix() method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ThWT12ykuweL",
        "colab_type": "text"
      },
      "source": [
        "#### SOLUTION 5.2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P_P3_RIdPGb3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_confusion_matrix(cm, title='Confusion matrix', cmap=plt.cm.Blues):\n",
        "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
        "    plt.title(title)\n",
        "    plt.colorbar()\n",
        "    tick_marks = np.arange(cm.shape[0])\n",
        "    plt.xticks(tick_marks, np.arange(cm.shape[0]))\n",
        "    plt.yticks(tick_marks, np.arange(cm.shape[0]))\n",
        "    plt.tight_layout()\n",
        "    plt.ylabel('True label')\n",
        "    plt.xlabel('Predicted label')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yB2pZ37ZqLyn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# <SOL>\n",
        "# </SOL>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "icHrxVgTsOxk",
        "colab_type": "text"
      },
      "source": [
        "# 6. Perfomance evaluation in binary problems\n",
        "So far, we have focused on measuring the performance of multiclass problems; in the next  cells we will present some specific assessment methods for this kind of problems.\n",
        "\n",
        "For this purpose, let's start redefining our multiclass problem as a binary one, just trying to detect the class 'versicolor' from the other two."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zJqKn2PwNuIg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Class 0 and 2  ('setosa' and  'virginica') -> Class 0 ('no versicolor')\n",
        "# Class 1 ('versicolor') -> Class 1 ('versicolor')\n",
        "Y_train [Y_train==2] =0\n",
        "Y_test [Y_test==2] =0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GVIVkuJ-NvMF",
        "colab_type": "text"
      },
      "source": [
        "From this new problem definition, we have two possible hypothesis:\n",
        "* $H=1$ ($Y=1$), the data observations (the length and the width of the sepals and petals)  belong to a 'versicolor' iris.\n",
        "* $H=0$ ($Y=0$), the data observations (the length and the width of the sepals and petals) do not belong to a 'versicolor' iris.\n",
        "\n",
        "\n",
        "And our classifier can provide two possible outputs:\n",
        "* $D=1$ (${\\hat Y}=1$), the data is a 'versicolor' iris.\n",
        "* $D=0$ (${\\hat Y}=0$), the data is not a 'versicolor' iris.\n",
        "\n",
        "\n",
        "So, when a classifiers provides the output for a new observation, we can find four possible events:\n",
        "\n",
        "\n",
        "| Hypothesis (Y) /Decisions ($\\hat Y$) | $D=1$ | $D=0$ |\n",
        "| --- | --- | --- |\n",
        "| $H=1$  | True Positive (TP) | False Negative or Missing (FN)|\n",
        "| $H=0$  |  False Positive or False Alarm (FP)  | True Negative (TN) |\n",
        "\n",
        "From these events, we can define:\n",
        "* **The false positive ratio (or false alarm ratio)** is the ratio between the number of negative samples wrongly categorized as positive (false positives) and the total number of actual negative samples:\n",
        "\n",
        "$$ FPR = \\frac{\\# FP}{ \\#  (H=0)} = \\frac{\\# (Y=0 ~\\& ~{\\hat Y}=1) }{\\#(Y=0)} $$\n",
        "\n",
        "* **The false negative ratio (or missing ratio)** is the ratio between the number of positive samples wrongly categorized as negative (false negatives) and the total number of actual positive samples:\n",
        "\n",
        "$$ FNR = \\frac{\\# FN}{\\#  (H=1)} = \\frac{\\# (Y=1 ~\\& ~{\\hat Y}=0) }{\\#(Y=1)} $$\n",
        "\n",
        "\n",
        "* **The true positive or detection ratio**, also known as **recall**, is the ratio between the number of positive samples correctly categorized as positive (true positives) and the total number of actual positive samples:\n",
        "\n",
        "$$ TPR = 1- FNR = \\frac{\\# TP}{\\#  (H=1)} = \\frac{\\#(Y=1 ~\\& ~{\\hat Y}=1) }{\\# (Y=1)} $$\n",
        "\n",
        "* **Precision**, is the number of the number of positive samples correctly categorized as positive (true positives) divided by the total number of samples detected as positives.\n",
        "$$ Precision = \\frac{\\# TP}{\\#  (D=1)} = \\frac{\\#(Y=1 ~\\& ~{\\hat Y}=1) }{\\# (~{\\hat Y}=1)} $$\n",
        "\n",
        "Precision and recall parameters can be summarized with the following plot from [Wikipedia](https://en.wikipedia.org/wiki/Precision_and_recall)\n",
        "\n",
        "<img src=\"https://upload.wikimedia.org/wikipedia/commons/thumb/2/26/Precisionrecall.svg/700px-Precisionrecall.svg.png\" width=\"30%\"> \n",
        "\n",
        "\n",
        "\n",
        "\n",
        "Precision (P) and Recall (R) are usually combined into a single score, called **F-score**:\n",
        "\n",
        "$$F-score = 2 \\frac{P\\cdot R}{P+R}$$\n",
        "\n",
        "Sklearn has the method [precision_recall_fscore_support()](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.precision_recall_fscore_support.html) to obtain these parameters.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OIeHF7PEMgA6",
        "colab_type": "text"
      },
      "source": [
        "### Exercise 6.1\n",
        "\n",
        "With the binary problem ('no versicolor' v.s 'versicolor') train a L2-Logistic Regression classifier able to detect the versicolor Iris and evaluate its performance with:\n",
        "* False Alarm Ratio ($R_{FA}$) \n",
        "* Detection Ratio ($R_{D}$)\n",
        "* Precision, recall and F-score. Compare yours results with those provided by the precision_recall_fscore_support method of sklearn.\n",
        "\n",
        "It is not needed to adjust the L2-LR  C parameter and you can use a value of 100.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mBQHHCx3Dj_P",
        "colab_type": "text"
      },
      "source": [
        "#### SOLUTION 6.1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7dQx0C0hNGOM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# <SOL>\n",
        "# </SOL>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x1MFfWkPNsub",
        "colab_type": "text"
      },
      "source": [
        "## Modifying the classifier threshold\n",
        "\n",
        "As we have seen, in multiclass problems, we estimate the most likely class of a new point $\\mathbf{x}^*$ as:\n",
        "\n",
        "\\begin{align}\n",
        "\\hat{y}^* =  {\\arg \\max}_{y\\in\\{0,1,\\ldots, M-1\\}} h(\\mathbf{x}^*,y)\n",
        "\\end{align}\n",
        "\n",
        "\n",
        "In binary problems, this  decision can be expressed as:\n",
        "\n",
        "\\begin{align}\n",
        "h(\\mathbf{x}^*,1) \\begin{array}{c} \\hat{y}^* = 1 \\\\ \\gtrless \\\\ \\hat{y}^* = 0 \\end{array} h(\\mathbf{x}^*,0)\n",
        "\\end{align}\n",
        "\n",
        "\n",
        "\\begin{align}\n",
        "\\ln \\frac{h(\\mathbf{x}^*,1)}{h(\\mathbf{x}^*,0)} \\begin{array}{c} \\hat{y}^* = 1 \\\\ \\gtrless \\\\ \\hat{y}^* = 0 \\end{array} 0\n",
        "\\end{align}\n",
        "\n",
        "with this criterion, if $h(\\mathbf{x},y)$ is approximating the posterior probability of each class, $P(Y=y|\\mathbf{x}^*)$, we are miniziming the probability of error. However, in some cases, we may want to modify the decision threshold $\\eta$ by a value greater or lower than zero, \n",
        "\n",
        "\\begin{align}\n",
        "\\ln \\frac{h(\\mathbf{x}^*,1)}{h(\\mathbf{x}^*,0)} \\begin{array}{c} \\hat{y}^* = 1 \\\\ \\gtrless \\\\ \\hat{y}^* = 0 \\end{array} \\eta\n",
        "\\end{align}\n",
        "\n",
        "In this way, we will penalize with different costs or weigths the different errors (false negatives and /or false positives). \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MEX44hZSNJjz",
        "colab_type": "text"
      },
      "source": [
        "### Exercise 6.2\n",
        "\n",
        "So far, to evaluate a classifier, we have always used the output of the sklearn methods by means of the  method .predict() or, directly, the performance given by the method.score(). However, we can use the soft-output of our classfifiers and apply different thresholds over this output to give more weight to either false negatives or false positives.\n",
        "\n",
        "Use the classifier that you have just trained in Exercise 6.1 and analyze its soft-output by means of the method .decision_function(). Then,\n",
        "\n",
        "* Apply a threshold of 0 over this output to obtain the classifier output and compute its $R_{FA}$ and $R_{D}$.\n",
        "* Analyze the behavior of $R_{FA}$ and $R_{D}$ when this threshold is:  $-1, -0.5, 0, 0.5, 1$. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RpPy5C6MDm-x",
        "colab_type": "text"
      },
      "source": [
        "#### SOLUTION 6.2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xiyJN4BMVc5S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# <SOL>\n",
        "# </SOL>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5r5NFL1Wedy5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# <SOL>\n",
        "# </SOL>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "50aVp7fTcdr3",
        "colab_type": "text"
      },
      "source": [
        "## A receiver operating characteristic (ROC)\n",
        "\n",
        "The ROC curve and its area (AUC, Area Under the Curve) are typically used in binary problems to study the output of a classifier independently of the classification threshold. Wikipedia defines the ROC as:\n",
        "\n",
        "    “A receiver operating characteristic (ROC), or simply ROC curve, is a graphical plot which illustrates the performance of a binary classifier system as its discrimination threshold is varied.\"\n",
        "  \n",
        "To plot this curve we have plot the Detection Ratio vs. the False Alarm ration at various threshold settings. The AUC computes the area under the ROC, in this way, the curve information is summarized in one number. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKfnbWM2qM0H",
        "colab_type": "text"
      },
      "source": [
        "### Exercise 6.3\n",
        "\n",
        "Use the [roc_curve( )](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.roc_curve.html) and [auc( )](http://scikit-learn.org/stable/modules/generated/sklearn.metrics.auc.html) methods to obtain the ROC curve (expressed in terms of the False Positive Rate (FPR) and the True Positive Rate (TPR)) and the AUC value for the classifier designed in Exercise 6.1.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fBEJ55DYDqaY",
        "colab_type": "text"
      },
      "source": [
        "#### SOLUTION 6.3"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ho7UXL_qqaa2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# <SOL>\n",
        "# </SOL>"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}