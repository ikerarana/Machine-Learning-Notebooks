{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Homework1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7-jkWR1LhNMh",
        "colab_type": "text"
      },
      "source": [
        "# Week 4: Homework 1 \n",
        "\n",
        "----------------------------------------------------\n",
        "Machine Learning                      \n",
        "\n",
        "Year 2019/2020\n",
        "\n",
        "*Vanessa GÃ³mez Verdejo vanessa@tsc.uc3m.es* \n",
        "\n",
        "----------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xfXCi3qshchx",
        "colab_type": "text"
      },
      "source": [
        "The goal of this practice is to analyze the performance of different estimators on the Diabetes problem and we will analyze, by means of different approximations, which input features are more relevant to solve this problem. \n",
        "\n",
        "Note that previous week we already worked on Diabetes database, but we only used one of the input variable (BMI) to construct the regression model; in this practice we will use all the input features jointly.\n",
        "\n",
        "To solve these notebook, complete the following sections implementing the solution that you consider most appropriate and showing the results that you find most interesting. For the evaluation of this notebook,  we will take into account the methodology used, the solution adopted, the presentation of the results and the conclusions obtained at the light of the results.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nCXU4m2YmB2y",
        "colab_type": "text"
      },
      "source": [
        "## 1. Data loading and preprocessing\n",
        "\n",
        "Following the ML pipeline, start loading the data, creating the partitions that you consider necessary and carrying out the preprocessing that yu need.\n",
        "\n",
        "Keep in mind that there is no single valid solution, and different reasons can lead you to make different data partitions or apply different normalizations. So **please justify the steps you are taking**."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-7fqumVhhJAl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## 1. Data loading and preprocessing\n",
        "\n",
        "# Include your code here (create as many cells as you need)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fe7_QAXDmatm",
        "colab_type": "text"
      },
      "source": [
        "## 2. Performance evaluation \n",
        "\n",
        "Now, analyze the performance of different estimators to predict the diabetes progression from all the available features. \n",
        "\n",
        "As possible estimators to be included in this study, we will consider those studied so far: K-NN, linear regressor, polynomial regressor and their regularized versions. Please, in case these methods have any free hiperparameter, **clearly justify** the selection of their optimal values.\n",
        "\n",
        "As you know, for the performance evaluation, we have seen several metrics. So, here, you can use one or several of them. But, regardless of the used metric, don't forget to include a final analysis comparing the performance of different methods and trying to justify the obtained results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EnOY0K4Fo62S",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## 2. Performance evaluation \n",
        "\n",
        "# Include your code here (create as many cells as you need)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VHWjw1eAo_9V",
        "colab_type": "text"
      },
      "source": [
        "## 3. Study of feature relevance and feature selection\n",
        "\n",
        "In this last section, using different criteria, you have to analyze the relevance of the input features. Thus, you will have to find a subset with the $D'$ most relevant features and, using this subset of features, analyze the final performance of a regressor (for the sake of simplicity, consider a linear ridge regressor as final regressor).\n",
        "\n",
        "To analyze the feature importances or relevances, you can use the following criteria:\n",
        "\n",
        "1. **Relevance ranking based on the validation error**: if there were $D$ input features, we could try to train $D$ regressors where each regressor uses one (and only one) different input feature. According to the final perfomance of each regressor (evaluated on a validation set or with a CV proccess), we could rank the features (the most relevant feature is the one providing the lowest error). Using this ranking, we can select the $D'$ most relevant features as the $D'$ top-ranked ones. Note that this scheme only analyzes the isolated relevance of each feature to predict the output; so, it is said that this approach is *univariate*.\n",
        "\n",
        " <img src=\"http://www.tsc.uc3m.es/~vanessa/Figs_notebooks/ML/Regression/FeatureRanking.png\" width=\"90%\" > \n",
        "\n",
        "\n",
        "2. **Greedy search based on the validation error**: approach (1) has the disadvantage of not taking into account relationships between features. For instance, method (1) would not realize that two features can be rendundant or that a feature, that is useless by itself, can be very useful combined with another feature. To overcome this drawback, we should have to analyze subsets of features; however, exploring all possible subsets  is usually computationally unflexible (there are $2^D$ combinations!!!!); so a greedy search (fordward or backward) is usually prefered:\n",
        "\n",
        "  2.1 *Fordward search*: It starts with an empty set and, iteratively, adds new features according to a relevance criterion (in this case, minimum validation MSE).\n",
        "\n",
        "  <img src=\"http://www.tsc.uc3m.es/~vanessa/Figs_notebooks/ML/Regression/Fig_Forward_search.png\" width=\"48%\" > \n",
        "\n",
        "  2.2 *Backward search*: It starts considering all the features and, iteratively, removes features according to a relevance criterion (in this case, minimum validation MSE).\n",
        "\n",
        "  <img src=\"http://www.tsc.uc3m.es/~vanessa/Figs_notebooks/ML/Regression/Fig_Backward_search.png\" width=\"48%\" > \n",
        "\n",
        "3. **Ridge linear regression with a prunning**: We know that the L2 regularization limits the magnitude of the weight vector to avoid overfitting problems, but these weigths do not become to null. However, in a linear model, *the weight magnitude can be an indicative of the feature relevance* and, unlike approach (1), all features are analyzed at the same time (*multivariate approach*). Use the weigth magnitude to generate a ranking of features and, later, use this ranking to select the $D'$ most relevant features.\n",
        "\n",
        "4. **Lasso linear regression**: In this case, the L1 penalty allows us to directly eliminate some of the input features. Explore different values of the regularization parameter $\\lambda$ to get a sequence of selected feature sets (from a single feature to all features).\n",
        "\n",
        "\n",
        "5. **Elastic-net linear regression**: This last approach combines L1 and L2 regularizations, thus including the advantages of both methods. Varying adequately their regularization parameters, create a sequence of feature selection subsets. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fe2uvnVP0-AZ",
        "colab_type": "text"
      },
      "source": [
        "Final comments:\n",
        "\n",
        "* Due to part of the feature selection process involves selecting the optimum number of features, to avoid additional complexity (having to validate this number), you can analyze the different methods by exploring the curves of MSE vs. number of selected features ($D'$).\n",
        "\n",
        "* It is not necessary to apply all these methods to complete this notebook (you can choose, at least, three of them). In fact, the implementation of greedy search approaches require an advanced knowledge of Python; so take this into account when you design your notebook solution.\n",
        "\n",
        "* **Please, analyze in detail the different results, pointing out the advantages/disadvantages of each feature selection scheme**. Think about the behaviour of the different criteria in cases where a feature is irrelevant or there are redundant features. Additional experiments helping you to support any of your conclusions will be welcome."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jtDSa1iq-G9V",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "## 3. Feature selection \n",
        "\n",
        "# Include your code here (create as many cells as you need)"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}