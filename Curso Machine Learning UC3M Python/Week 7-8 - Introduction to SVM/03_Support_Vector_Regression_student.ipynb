{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "celltoolbar": "Slideshow",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "name": "Support_Vector_Regression.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "s-fSro6vLOif",
        "colab_type": "text"
      },
      "source": [
        "# Week 9: Support Vector Machines for Regression\n",
        "------------------------------------------------------\n",
        "Machine Learning     Year 2019/2020\n",
        "\n",
        "*Vanessa GÃ³mez Verdejo vanessa@tsc.uc3m.es* and *Pablo M. Olmos olmos@tsc.uc3m.es*\n",
        "\n",
        "------------------------------------------------------\n",
        "\n",
        "We now extend support vector machines to regression problems while at the same time preserving the property of sparseness.\n",
        "\n",
        "A good tutorial about Regression Vector Machines can be found in this [link](https://www.svms.org/regression/SmSc98.pdf)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "w3t1q7uuLOio",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import time\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "from sklearn.svm import SVR\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn.model_selection import learning_curve\n",
        "from sklearn.kernel_ridge import KernelRidge\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "plt.rcParams[\"figure.figsize\"] = [6,6]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zl7j6LsULOin",
        "colab_type": "text"
      },
      "source": [
        "## Kernel Ridge Regression: a review\n",
        "\n",
        "Given a training set in the feature space $\\mathcal{D}=(\\phi(\\mathbf{x})^{(i)},y^{(i)})$, $i=1,\\ldots,N$, where $\\mathbf{x}\\in\\mathbb{R}^D$ and $y^{(i)}\\in\\mathbb{R}$, Ridge Regression minimizes a regularized version of the mean squared error:\n",
        "\n",
        "\\begin{align}\n",
        "\\sum_{i=1}^{N} \\left(y^{(i)}-f(\\mathbf{x}^{(i)})\\right)^2 + \\lambda \\sum_{j=1}^{D+1} w_j^2\n",
        "\\end{align}\n",
        "\n",
        "where $f(\\mathbf{x})={\\bf w}^T\\phi(\\mathbf{x}^{(i)})$.\n",
        "\n",
        "In the last notebook, analyzing its solution, we saw that:\n",
        "* We could obtain no-linear solutions \n",
        "* But the regression model was **no sparse**. All training data were used as support of the solution.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l5XCbT2VLOiw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "rng = np.random.RandomState(0)\n",
        "\n",
        "# #############################################################################\n",
        "# Generate sample data\n",
        "\n",
        "N_train = 1000\n",
        "\n",
        "X = 5 * rng.rand(N_train, 1)\n",
        "y = np.sin(X).ravel()\n",
        "\n",
        "# Add noise to every 5 targets\n",
        "y[::5] += 3 * (0.5 - rng.rand(X.shape[0] // 5))\n",
        "\n",
        "X_plot = np.linspace(0, 5, 100000)[:, None]\n",
        "\n",
        "plt.plot(X,y,'*',markersize=12)\n",
        "plt.legend(['Training Data'])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h1rOv54mLOi4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Train a Kernel Ridge Regression model\n",
        "\n",
        "KR_model = KernelRidge(kernel='rbf',alpha=1,gamma=1)\n",
        "\n",
        "t0 = time.time()\n",
        "\n",
        "KR_model.fit(X,y)\n",
        "\n",
        "train_time = time.time()-t0\n",
        "\n",
        "t0 = time.time()\n",
        "\n",
        "y_plot = KR_model.predict(X_plot)\n",
        "\n",
        "test_time=time.time()-t0\n",
        "\n",
        "# Analize the solution and the model performance\n",
        "\n",
        "print('Train Kernel Ridge: %f seconds' %(train_time))\n",
        "print('Test Kernel Ridge: %f seconds' %(test_time))\n",
        "\n",
        "a_coef = KR_model.dual_coef_\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(X,y,'+',markersize=12)\n",
        "plt.plot(X[np.abs(a_coef)>0],y[np.abs(a_coef)>0],'o',markersize=10)\n",
        "plt.plot(X_plot,y_plot,'r')\n",
        "\n",
        "plt.legend(['Training Data','Training points with $a_i>0$','Kernel Ridge Estimate',])\n",
        "print('The fraction of training points that participate in the test decision is %f' %(100*np.sum(np.abs(a_coef)>0)/N_train))\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbbtCTMVJuO9",
        "colab_type": "text"
      },
      "source": [
        "All training data have a no-zero dual variable ($a^{(i)}\\neq 0$) and this increases considerablily the testing time!!!! Besides, it can create overfitting problems."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aUPD4O0ALOi_",
        "colab_type": "text"
      },
      "source": [
        "## Regression Vector Machines: Sparse Linear Regression\n",
        "\n",
        "To obtain sparse solutions, Regression Vector Machines replace the quadratic error function by an **$\\epsilon$-insensitive error function** defined as:\n",
        "\n",
        "$$E(f(\\mathbf{x})-y)=\\left\\{\\begin{array}[cc] &0& |f(\\mathbf{x})-y|<\\epsilon;\\\\|f(\\mathbf{x})-y|-\\epsilon & \\text{otherwise}\\end{array}\\right.$$\n",
        "\n",
        "<img src=\"http://www.tsc.uc3m.es/~vanessa/Figs_notebooks/ML/SVMs/E_loss.png\" width=\"60%\" >\n",
        "\n",
        "Figure source: Bishop's book\n",
        "\n",
        "Then, they solve the following optimization problem:\n",
        "\n",
        "\n",
        "\\begin{align}\n",
        "&\\min_{\\mathbf{w},w_0}  ~~~ \\frac{1}{2} ||\\mathbf{w}||^2\\\\\n",
        "\\text{s. t.}~~~ &y^{(i)} - f(\\mathbf{x}^{(i)})\\leq \\epsilon, ~ i=1,\\ldots,N \\\\\n",
        "& f(\\mathbf{x}^{(i)}) - y^{(i)} \\leq \\epsilon, ~ i=1,\\ldots,N\n",
        "\\end{align}\n",
        "\n",
        "where $f(\\mathbf{x})={\\bf w}^T\\phi(\\mathbf{x}^{(i)})$, and $\\epsilon$ is an hyperparameter to be tuned using Cross-Validation.\n",
        "\n",
        "The solution of this problem exists  (the optimization problem is *feasible*) if the function $f(\\mathbf{x})$ is able to approximate all training points with a precision $\\epsilon$. However, this is not always the case, or we may want to allow some errors. In this case, we can reformulate the problem by addind some slack variables."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HZoWolYlLOjC",
        "colab_type": "text"
      },
      "source": [
        "### Reformulate the problem adding Slack Variables\n",
        "\n",
        "In the same way that SVMs (for classification) introduce the idea of soft-margin for non-linearly separable problems, we can introduce here a set of slack variables to cope with no-feasible problems or to get a smoother solution allowing some data to be outside the $[-\\epsilon, \\epsilon]$ tube.  So, we can re-express the problem using slack variables:\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\\begin{align}\n",
        "&\\min_{\\mathbf{w},w_0}  ~~~ \\frac{1}{2} ||\\mathbf{w}||^2 + C\\sum_{i=1}^{N}(\\xi_i+\\xi^*_i)\\\\\n",
        "\\text{s. t.}~~~ &y^{(i)} - f(\\mathbf{x}^{(i)})\\leq \\epsilon + \\xi_i, ~ i=1,\\ldots,N \\\\\n",
        "& f(\\mathbf{x}^{(i)}) - y^{(i)} \\leq \\epsilon - \\xi^*_i, ~ i=1,\\ldots,N \\\\\n",
        "& \\xi_i,\\xi^*_i \\geq 0, \\quad ~ i=1,\\ldots,N \n",
        "\\end{align}\n",
        "\n",
        "The constant $C$ determines a trade-off between the smoothness of the regression function and the number of data with an error larger than $\\epsilon$. Note that now for each point $\\mathbf{x}^{(i)}$ we need two slack variables:\n",
        "\n",
        "- $\\xi_i\\geq 0$, where $\\xi_i>0$ if $y^{(i)}- f(\\mathbf{x}^{(i)})>\\epsilon$ \n",
        "- $\\xi^*_i\\geq 0$, where $\\xi^*_i>0$ if $y^{(i)}- f(\\mathbf{x}^{(i)})< - \\epsilon$\n",
        "\n",
        "\n",
        "<img src=\"http://www.tsc.uc3m.es/~vanessa/Figs_notebooks/ML/SVMs/slack_SVR2.png\" width=\"80%\" >\n",
        "\n",
        "\n",
        "Figure from [SVR tutorial](https://www.svms.org/regression/SmSc98.pdf)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwkY5EsgLOjH",
        "colab_type": "text"
      },
      "source": [
        "### Lagrangian function (Primal Space)\n",
        "\n",
        "We can introduce a set of dual variables (Lagrange multipliers) to introduce the constrains into the objetive function (Lagrange function):\n",
        "\n",
        "\\begin{align}\n",
        "L=&C\\sum_{i=1}^{N}(\\xi_i+\\xi^*_i)+||{\\bf w}||^2-\\sum_{i=1}^{N} (\\mu_i\\xi_i+\\mu^*_i\\xi^*_i)\n",
        "-\\sum_{i=1}^{N}a_i(\\epsilon+\\xi_i+f(\\mathbf{x}^{(i)})-y^{(i)})\n",
        "-\\sum_{i=1}^{N}a^*_i(\\epsilon+\\xi^*_i-f(\\mathbf{x}^{(i)})+y^{(i)})\n",
        "\\end{align}\n",
        "\n",
        "\n",
        "\n",
        "where $a_i\\geq0, a^*_i\\geq0,\\mu_i\\geq0, \\mu^*_i\\geq0$ are the *Lagrangre multipliers*. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gEVp7wzmLOjI",
        "colab_type": "text"
      },
      "source": [
        "If we compute gradients regrading to the primal variables ($\\bf{w}$, $w_0$ and $\\xi_i, \\xi^*_i$) and equalize to $0$, we get \n",
        "\n",
        "\\begin{align}\n",
        "\\frac{\\partial L }{\\partial \\bf{w}} = 0 ~~~~\\Rightarrow ~~~~& \\mathbf{w}=\\sum_{i=0}^{N}(a_i-{a}^*_i)\\phi(\\mathbf{x}^{(i)}) \\\\\n",
        "\\frac{\\partial L }{\\partial w_0} = 0 ~~~~\\Rightarrow ~~~~& 0=\\sum_{i=0}^{N}(a_i-a^*_i)\\\\\n",
        "\\frac{\\partial L }{\\partial \\xi^{(*)}_i} = 0 ~~~~\\Rightarrow ~~~~& C=a_i+\\mu_i=a^*_i+\\mu^*_i\\\\\n",
        "\\end{align}\n",
        "\n",
        "The first condition corroborates that this model fits the Representer Theorem and let's express the regression function as\n",
        "\n",
        "$$  f(\\mathbf{x})=\\sum_{i=1}^{N}(a_i-{a}^*_i)k(\\mathbf{x},\\mathbf{x}^{(i)})$$\n",
        "where $k(\\mathbf{x},\\mathbf{x}^{(i)})$ is the **kernel function** between $\\mathbf{x}$ and $\\mathbf{x}^{(i)}$. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZ4HWukQLOjJ",
        "colab_type": "text"
      },
      "source": [
        "### Dual Problem\n",
        "\n",
        "Using the above results, we can simplify the expression to \n",
        "\n",
        "\\begin{align}\n",
        "L(\\mathbf{a},\\mathbf{a}^*)=-\\frac{1}{2}\\sum_{i=1}^N\\sum_{j=1}^{N}(a_i-a^*_i)(a_j-a^*_j)k(\\mathbf{x}^{(i)},\\mathbf{x}^{(j)})-\\epsilon\\sum_{i=1}^N(a_i+a^*_i)+\\sum_{i=1}^N(a_i-a_i^*)y^{(i)}\n",
        "\\end{align}\n",
        "\n",
        "The corresponding Karush-Kuhn-Tucker (KKT) conditions are\n",
        "\n",
        "\\begin{align}\n",
        "a_i(\\epsilon+\\xi_i+f(\\mathbf{x}^{(i)})-y^{(i)})=0\\\\\n",
        "a^*_i(\\epsilon+{\\xi}^*_i-f(\\mathbf{x}^{(i)})+y^{(i)})=0\\\\\n",
        "(C-a_i)\\xi_i=0\\\\\n",
        "(C-a^*_i)\\xi^*_i=0\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OGiM1UBHLOjL",
        "colab_type": "text"
      },
      "source": [
        "And thus\n",
        "\n",
        "- $a_i$ can only be different from zero if $\\epsilon+\\xi_i+f(\\mathbf{x}^{(i)})-y^{(i)}=0$, i.e. a data point that either lies on the boundary of the $\\epsilon$-tube ($\\xi_i=0$) or lies above it ($\\xi_i>0$). \n",
        "\n",
        "- $a^*_i$ can only be different from zero if $\\epsilon+\\xi^*_i-f(\\mathbf{x}^{(i)})+y^{(i)}=0$, i.e. a data point that either lies on the boundary of the $\\epsilon$-tube ($\\xi^*_i=0$) or lies above it ($\\xi^*_i>0$). \n",
        "\n",
        "\n",
        "- $a_i$ and $a^*_i$ cannot be different from zero at the same time.\n",
        "\n",
        "- The support vectors are those for which either $a_i\\neq 0$ or $a^*_i\\neq0$:\n",
        "\n",
        "$$f(\\mathbf{x})=\\sum_{i=1}^{N}(a_i-a^*_i)k(\\mathbf{x},\\mathbf{x}^{(i)})$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TzACgiWvLOjM",
        "colab_type": "text"
      },
      "source": [
        "### Exercise 1\n",
        "\n",
        "Check how [SVR](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVR.html) performs in the above data set. Set $C=1$  and $\\epsilon=0.1$, and use a Gaussian RBF Kernel with $\\gamma=1$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_KSluFmZY67",
        "colab_type": "text"
      },
      "source": [
        "### Solution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fgSB3A4JLOjM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#<SOL>\n",

        "#</SOL>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gRCrPDTFLOjS",
        "colab_type": "text"
      },
      "source": [
        "### Exercise 2\n",
        "Try to vary the $\\epsilon$ and $C$ values and answer the following questions:\n",
        "\n",
        "* How can we control the tradeoff between accuracy and sparsity?\n",
        "* How can we control the overfitting?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DitO4XwSZ7ts",
        "colab_type": "text"
      },
      "source": [
        "### Solution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P0kgefHaLOjT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#<SOL>\n",

        "#</SOL>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkda4JKbLOjZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}