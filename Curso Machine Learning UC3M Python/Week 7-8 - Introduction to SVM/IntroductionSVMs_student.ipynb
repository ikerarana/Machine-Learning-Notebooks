{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.5"
    },
    "colab": {
      "name": "Introduction to SVMs.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "j9iRSeXH6avN",
        "colab_type": "text"
      },
      "source": [
        "# Week 8: Introduction to Support Vector Machines\n",
        "\n",
        "----------------------------------------------------\n",
        "Machine Learning     Year 2019/2020\n",
        "\n",
        "*Vanessa GÃ³mez Verdejo vanessa@tsc.uc3m.es* and *Pablo M. Olmos olmos@tsc.uc3m.es*\n",
        "\n",
        "----------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXwSxNl86avR",
        "colab_type": "text"
      },
      "source": [
        "[Support vector machines (SVMs)](http://scikit-learn.org/stable/modules/svm.html) are a particularly powerful and flexible class of supervised algorithms for both classification and regression.\n",
        "In this section, we will develop the intuition behind support vector machines and their use in classification problems.\n",
        "\n",
        "Recommended Bibliography:\n",
        "\n",
        "- Chapter 7 of [Pattern Recognition and Machine Learning](https://cds.cern.ch/record/998831/files/9780387310732_TOC.pdf), Christopher Bishop, 2006\n",
        "- Chapter 17 of [Bayesian Reasoning and Machine Learning](http://web4.cs.ucl.ac.uk/staff/D.Barber/textbook/090310.pdf), David Barber, 2010\n",
        "- Chapter 14 of Machine Learning: a probabilistic perspective, Kevin Murphy 2012\n",
        "- This excellent [post](http://efavdb.com/svm-classification/) by Andrew Ng. \n",
        "\n",
        "We begin with the standard imports:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0fl8AOUj6avT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats\n",
        "\n",
        "\n",
        "# use seaborn plotting defaults\n",
        "import seaborn as sns; sns.set()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BFFMhczx6avc",
        "colab_type": "text"
      },
      "source": [
        "Consider the simple case of a classification task, in which the two classes of points are well separated. \n",
        "\n",
        "Notes:\n",
        "* You can check out first what the [make blobs](http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_blobs.html) function does.\n",
        "\n",
        "* And you want to learn about color maps in matplotlib check [here](https://matplotlib.org/users/colormaps.html)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6td56m6F6ave",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets.samples_generator import make_blobs\n",
        "X, y = make_blobs(n_samples=500, centers=2,\n",
        "                  random_state=10, cluster_std=3)    #With random_state we fix the random seed\n",
        "\n",
        "# Define the labels as -1, +1\n",
        "y = 2*y-1\n",
        "\n",
        "#We separate away some data for test\n",
        "X_test = X[100:,:] \n",
        "y_test = y[100:]\n",
        "\n",
        "X = X[:100,:]\n",
        "y = y[:100]\n",
        "\n",
        "print(\"The shape of X is \",X.shape)\n",
        "\n",
        "print(\"y is a label vector. The first 10 labels are:\", y[:10])\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='RdBu')\n",
        "\n",
        "plt.rcParams[\"figure.figsize\"] = [8,8]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GiHyc6Bq6avm",
        "colab_type": "text"
      },
      "source": [
        "A linear discriminative classifier would attempt to draw a straight line separating the two sets of data, and thereby create a model for classification. **However, note there are many possible solutions!!**\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ic5YqVM6avn",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xfit = np.linspace(-5, 15)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='RdBu')\n",
        "\n",
        "m,b = (1, -7.5)\n",
        "plt.plot(xfit, m * xfit + b, '-k')\n",
        "\n",
        "m,b = (0.08, -2.1)\n",
        "plt.plot(xfit, m * xfit + b, '-m')\n",
        "\n",
        "m,b = (-0.2,-1.2)\n",
        "plt.plot(xfit, m * xfit + b, '-g')\n",
        "\n",
        "plt.rcParams[\"figure.figsize\"] = [8,8]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3RGL5tq6avs",
        "colab_type": "text"
      },
      "source": [
        "**Which one do you think separates best the data?**\n",
        "\n",
        "Let's plot some *test data* with the right category."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xohWZAwU6avv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xfit = np.linspace(-5, 15)\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='RdBu',label='Train')\n",
        "\n",
        "plt.scatter(X_test[:, 0], X_test[:, 1], c=y_test, s=50, cmap='RdBu',marker='x',label='Test')\n",
        "\n",
        "plt.legend()\n",
        "\n",
        "m,b = (1, -7.5)\n",
        "plt.plot(xfit, m * xfit + b, '-k')\n",
        "\n",
        "m,b = (0.08, -2.1)\n",
        "plt.plot(xfit, m * xfit + b, '-m')\n",
        "\n",
        "m,b = (-0.2,-1.2)\n",
        "plt.plot(xfit, m * xfit + b, '-g')\n",
        "\n",
        "\n",
        "plt.rcParams[\"figure.figsize\"] = [8,8]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p6Nmjf9x6av1",
        "colab_type": "text"
      },
      "source": [
        "## Maximizing the *Margin*\n",
        "\n",
        "SVMs offer one way to improve on this.\n",
        "The intuition is this: rather than simply drawing a zero-width line between the classes, we can draw around each line a *margin* of some width, up to the nearest point. The largest the margin is, the more robust the model is and generalizes better.\n",
        "\n",
        "Here is an example of how this might look:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "T9ZdmvJo6av3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "xfit = np.linspace(-5, 15)\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='RdBu')\n",
        "\n",
        "\n",
        "m,b,d = (1, -7.5,0.01)\n",
        "yfit = m * xfit + b\n",
        "plt.plot(xfit,yfit, '-k')\n",
        "plt.fill_between(xfit, yfit - d, yfit + d, edgecolor='none',color='#AAAAAA', alpha=0.4)\n",
        "\n",
        "m,b,d = (0.08, -2.1, 1)\n",
        "yfit = m * xfit + b\n",
        "plt.plot(xfit,yfit, '-m')\n",
        "plt.fill_between(xfit, yfit - d, yfit + d, edgecolor='none',color='#AAAAAA', alpha=0.4)\n",
        "\n",
        "m,b,d = (-0.2,-1.2,0.05)\n",
        "yfit = m * xfit + b\n",
        "plt.plot(xfit,yfit, '-g')\n",
        "plt.fill_between(xfit, yfit - d, yfit + d, edgecolor='none',color='#AAAAAA', alpha=0.4)\n",
        "\n",
        "plt.rcParams[\"figure.figsize\"] = [8,8]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lyeFZV6N6av9",
        "colab_type": "text"
      },
      "source": [
        "In SVMs, the line that maximizes this margin is the one we will choose as the optimal model.\n",
        "SVMs are an example of such a *maximum margin* estimator."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I9x6wumP6av-",
        "colab_type": "text"
      },
      "source": [
        "## A little bit of geometry: hyperplanes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uYPtAVCQ6awA",
        "colab_type": "text"
      },
      "source": [
        "### Hyperplanes\n",
        "\n",
        "First of all, for $\\mathbf{x}\\in\\mathbb{R}^D$, consider the function\n",
        "\\begin{align}\n",
        "f(\\mathbf{x}) = \\mathbf{w}^T\\mathbf{x}+w_0\n",
        "\\end{align}\n",
        "\n",
        "Then, the set of points $\\mathbf{x}\\in\\mathbb{R}^D$ such that $f(\\mathbf{x})=0$ is called an [**hyperplane**](http://mathworld.wolfram.com/Hyperplane.html), a subspace of dimension $D-1$. E.g., for $D=2$, then the hyperplane is a line. For $D=3$ is a plane.\n",
        "\n",
        "#### Some useful results with hyperplanes\n",
        "\n",
        "  <img align=\"right\" src=\"http://www.tsc.uc3m.es/~vanessa/Figs_notebooks/ML/SVMs/Fig1.png\" width=\"40%\" >\n",
        "  <br>\n",
        "\n",
        "1. If $\\mathbf{x}_A$ and $\\mathbf{x}_B$ are two points in the hyperplane, then the vector $\\mathbf{v}=\\alpha(\\mathbf{x}_A-\\mathbf{x}_B)$ is orthogonal to $\\mathbf{w}$ for $\\alpha\\in\\mathbb{R}$. Thus, $\\mathbf{w}$ is orthogonal to any vector contained in the hyperplane and it determines the orientation of the hyperplane.\n",
        "<br><br>\n",
        "\n",
        "  *Proof*: If $\\mathbf{x}_A$ and $\\mathbf{x}_B$ are two points in the hyperplane, then $\\mathbf{w}^T\\mathbf{x}_A+w_0=\\mathbf{w}^T\\mathbf{x}_B+w_0=0$. Thus, $\\mathbf{w}^T\\left(\\mathbf{x}_A-\\mathbf{x}_B\\right)=0$. \n",
        "\n",
        "<br><br><br><br><br>\n",
        "\n",
        " <img align=\"right\" src=\"http://www.tsc.uc3m.es/~vanessa/Figs_notebooks/ML/SVMs/Fig2.png\" width=\"40%\" >\n",
        " <br>\n",
        "\n",
        "2. The normal distance between the origin $\\mathbf{0}$ and the closest point in the hyperplane is given by $-\\frac{w_0}{||\\mathbf{w}||}$, where a negative distance denotes that the hyperplane lies below the origin, and $||\\mathbf{w}||=\\sqrt{\\mathbf{w}^T\\mathbf{w}}$.\n",
        "<br><br>\n",
        "\n",
        "  *Proof*: Note that, for any $\\mathbf{x}$ (not necesarily in the hyperplane), the projection of $\\mathbf{x}$ into $\\mathbf{w}$ is the dot product between $\\mathbf{x}$ and a normalized vector of $\\mathbf{w}$, thus $\\mathbf{x}_{\\perp}=\\frac{ \\mathbf{w}^T\\mathbf{x}}{||\\mathbf{w}||}$.  Note that this projection is the distance from the origin to the hyperplane orthogonal to $ ||\\mathbf{w}||$ that passes through $\\mathbf{x}$. If $\\mathbf{x}$ is a point of the hyperplane, $f(\\mathbf{x}) = \\mathbf{w}^T\\mathbf{x}+w_0 =$, so \n",
        "\n",
        "$$\n",
        "\\mathbf{x}_{\\perp}=\\frac{ -w_0}{||\\mathbf{w}||}\n",
        "$$\n",
        "<br><br><br>\n",
        "\n",
        "  <img align=\"right\" src=\"http://www.tsc.uc3m.es/~vanessa/Figs_notebooks/ML/SVMs/Fig3.png\" width=\"40%\" >\n",
        "<br>\n",
        "\n",
        "3. The normal distance to any point $\\mathbf{x}^*\\in\\mathbb{R}^D$ to the hyperplane is $ \\frac{f(\\mathbf{x}^*)}{||\\mathbf{w}||}$. This distance is positive for points above the hyperplane, and negative for points below the hyperplane.\n",
        "<br><br>\n",
        "\n",
        "  *Proof*: Given any point $\\mathbf{x}$ in the hyperplane $f(\\mathbf{x}) = \\mathbf{w}^T\\mathbf{x}+w_0=0$, the normal distance of $\\mathbf{x}^*$ to the hyperplane is the proyection of $(\\mathbf{x}^*-\\mathbf{x})$ into $\\mathbf{w}$ (since $\\mathbf{w}$ is perpedincular to the plane). Hence\n",
        "\n",
        "$$\n",
        "\\text{distance} = \\frac{\\mathbf{w}^T(\\mathbf{x}^*-\\mathbf{x})}{||\\mathbf{w}||}=\\frac{\\mathbf{w}^T\\mathbf{x}^*-\\mathbf{w}^T\\mathbf{x}}{||\\mathbf{w}||}=\\frac{\\mathbf{w}^T\\mathbf{x}^*+w_0}{||\\mathbf{w}||}=\\frac{f(\\mathbf{x}^*)}{||\\mathbf{w}||}\n",
        "$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "YSA_8inw6awB",
        "colab_type": "text"
      },
      "source": [
        "# Support Vector Machines: The linearly separable case\n",
        "\n",
        "Assume there exists an hyperplane that separates our data $\\mathcal{D}=(\\mathbf{x}^{(i)},y^{(i)})$, $i=1,\\ldots,N$, where $\\mathbf{x}^{(i)}\\in\\mathbb{R}^D$ and $y^{(i)}\\in\\{-1,+1\\}$. Then, there must exist an hyperplane $f(\\mathbf{x}) = \\mathbf{w}^T\\mathbf{x}+w_0=0$ that verifies\n",
        "\\begin{align}\n",
        "y^{(i)}f(\\mathbf{x}^{(i)})\\geq 0, ~~Â \\forall (\\mathbf{x}^{(i)},y^{(i)})\n",
        "\\end{align}\n",
        "\n",
        "## The SVM: maximizing the margin\n",
        "\n",
        "If we want to maximize the classification margin, we have to maximize the distance of the closest point to the hyperplane, so the SVM optimization problem reads as follows\n",
        "\n",
        "\\begin{align}\n",
        "\\arg\\max_{\\mathbf{w},w_0} \\left\\{ \\min_{i} \\frac{y^{(i)}f(\\mathbf{x}^{(i)})}{||\\mathbf{w}||}\\right\\}\n",
        "\\end{align}\n",
        "\n",
        "However, trying to solve this optimization problem would be too complex, but it can be written in a simpler way. Since the distance of any point $\\mathbf{x}^{(i)}$ to the hyperplane is invariant to a scale of the form $\\mathbf{w}\\leftarrow \\eta\\mathbf{w}$, $w_0\\leftarrow\\eta w_0$, **then we can freely set the minimum distance to the hyperplane to $||\\mathbf{w}||$**, rescaling the whole problem. Thus, any point in the training set must verify\n",
        "\n",
        "\\begin{align}\n",
        "y^{(i)}f(\\mathbf{x}^{(i)})\\geq 1, ~~ \\forall (\\mathbf{x}^{(i)},y^{(i)})\n",
        "\\end{align}\n",
        "\n",
        "\n",
        "<center> <img src=\"http://www.tsc.uc3m.es/~vanessa/Figs_notebooks/ML/SVMs/Fig4.png\" width=\"40%\"> </center>\n",
        "\n",
        "And the equivalent problem can be written follows:\n",
        "\n",
        "\\begin{align}\n",
        "&\\min_{\\mathbf{w},w_0}  ~~~ \\frac{1}{2} ||\\mathbf{w}||^2\\\\\n",
        "\\text{s. t.}~~~ &y^{(i)}f(\\mathbf{x}^{(i)})\\geq 1, ~ i=1,\\ldots,N\n",
        "\\end{align}\n",
        "\n",
        "where we have modified the minimization of $||\\mathbf{w}||^{-1}$ by the maximization of $||\\mathbf{w}||^2$ and we have introduced the factor $\\frac{1}{2}$ for later convenience. This optimization problem is an example of a [**Quadratic Programming (QP)**](https://sites.math.washington.edu/~burke/crs/408f/notes/qp/qp1.pdf) optimization problem. Very efficient solvers are known for these kind of problems. Complexity scales cubic in the input dimension, i.e., $\\mathcal{O}(D^3)$.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vIA6Txfu6PIT",
        "colab_type": "text"
      },
      "source": [
        "Let's run this model and visualize the solution for our running example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "am_5JOvwQFW5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.svm import LinearSVC # \"Support vector classifier\"\n",
        "model = LinearSVC(C=1E20)   # We explain later the role of C\n",
        "model.fit(X, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mwJZjeP5kglW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(model.coef_)\n",
        "print(model.intercept_)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7ZzyYgUS6awI",
        "colab_type": "text"
      },
      "source": [
        "The following function plots the SVM decision boundaries for us:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wfExRgxo6awJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_svc_decision_function(model, ax=None):\n",
        "    \"\"\"Plot the decision function for a 2D SVC\"\"\"\n",
        "    if ax is None:            #If no figure handle is provided, it opens the current figure\n",
        "        ax = plt.gca()\n",
        "        \n",
        "    xlim = ax.get_xlim()\n",
        "    ylim = ax.get_ylim()\n",
        "    \n",
        "    # create grid to evaluate model\n",
        "    x = np.linspace(xlim[0], xlim[1], 30)    #30 points in the grid axis\n",
        "    y = np.linspace(ylim[0], ylim[1], 30)\n",
        "    Y, X = np.meshgrid(y, x)                 # We create a grid with the x,y coordinates defined above\n",
        "    \n",
        "    # From the grid to a list of (x,y) values. \n",
        "    # Check Numpy help for ravel()\n",
        "    \n",
        "    xy = np.vstack([X.ravel(), Y.ravel()]).T \n",
        "    P = model.decision_function(xy).reshape(X.shape)\n",
        "    \n",
        "    # plot decision boundary and margins\n",
        "    # In levels we provide a list of floating point numbers indicating \n",
        "    #the level curves to draw, in increasing order; e.g., to draw just the zero contour pass\n",
        "    ax.contour(X, Y, P, colors='k',\n",
        "               levels=[-1, 0, 1], alpha=0.5,\n",
        "               linestyles=['--', '-', '--'])\n",
        "    \n",
        "    ax.set_xlim(xlim)\n",
        "    ax.set_ylim(ylim)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZSGAelVe6awP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
        "plot_svc_decision_function(model)\n",
        "\n",
        "plt.rcParams[\"figure.figsize\"] = [8,8]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tIIJU-236awU",
        "colab_type": "text"
      },
      "source": [
        "# Going depeer into the SVM solution: the dual formulation\n",
        "\n",
        "There's lot more that we can say about how SVM perform. To this end, we have to go deeper into the optimization problem itself:\n",
        "\n",
        "\\begin{align}\n",
        "&\\min_{\\mathbf{w},w_0}  ~~~ \\frac{1}{2} ||\\mathbf{w}||^2\\\\\n",
        "\\text{s.t.}~~~ & y^{(i)}f(\\mathbf{x}^{(i)})\\geq 1, ~ i=1,\\ldots,N\n",
        "\\end{align}\n",
        "\n",
        "## Introducing Lagrange Multipliers\n",
        "\n",
        "Given the constraints of the problem, the Lagrange function that has to be optimized is of the form\n",
        "\n",
        "\\begin{align}\n",
        "\\mathcal{L}(\\mathbf{w},w_0,\\mathbf{a}) = \\frac{1}{2}||\\mathbf{w}||^2 -\\sum_{i=1}^N a^{(i)} (y^{(i)}f(\\mathbf{x}^{(i)})-1),\n",
        "\\end{align}\n",
        "where $a^{(i)}\\geq 0$, $i=1,\\ldots,N$ are the Lagrange multipliers (we have introduced a multiplier for each constraint). If we compute the gradient of $\\mathcal{L}(\\mathbf{w},w_0,\\mathbf{a})$ w.r.t. $\\mathbf{w}$ and $w_0$ and equalize to zero, we get the following conditions:\n",
        "1. $$\\mathbf{w} =\\sum_{i=1}^N a^{(i)} y^{(i)} \\mathbf{x}^{(i)}$$ \n",
        "2. $$ 0 = \\sum_{i=1}^{N} a^{(i)}  y^{(i)}$$\n",
        "\n",
        "## The Dual Problem\n",
        "If we introduce the above expressions in the the Lagrange function, our optimization problem reads\n",
        "\n",
        "\\begin{align}\n",
        "&\\max_{\\mathbf{a}}  ~~~~ \\sum_{i=1}^{N}a^{(i)} -\\frac{1}{2}\\sum_{i=1}^N \\sum_{j=1}^N a^{(i)} a^{(j)} {\\mathbf{x}^{(i)}}^\\top\\mathbf{x}^{(j)}\\\\\n",
        "\\text{s.t.} ~~~~ & a^{(i)} \\geq 0, ~ i=1,\\ldots,N\\\\\n",
        "& 0  = \\sum_{i=1}^{N} a^{(i)}  y^{(i)}\n",
        "\\end{align}\n",
        "\n",
        "This problem is another instance of **Quadratic Programming** and its resolution is also $\\mathcal{O}(N^3)$ complex. Note that it is solved for dual variables $\\mathbf{a}$.\n",
        "\n",
        "However, given the solutionfor $\\mathbf{a}$, we can recover the weight vector using the condition (1):\n",
        "\n",
        "\\begin{align}\n",
        "\\mathbf{w} =\\sum_{i=1}^N a^{(i)} y^{(i)} \\mathbf{x}^{(i)}\n",
        "\\end{align}\n",
        "\n",
        "Note that the weigth vector is a linear combination of the training data!!!!. And we can classify a new point $\\mathbf{x}^*$ according to the sign of\n",
        "\\begin{align}\n",
        "f(\\mathbf{x}^*) = \\sum_{i=1}^{N} a^{(i)} y^{(i)} {\\mathbf{x}^{(i)}}^\\top\\mathbf{x}^*+w_0\n",
        "\\end{align}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BJvr2dfvmnUb",
        "colab_type": "text"
      },
      "source": [
        "In which scenarios do you think it is preferable to use the primal formulation (solving for $\\mathbf{w}$ and $w_0$) or in the dual (solving for $\\mathbf{a}$)?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HRYXGhky6awa",
        "colab_type": "text"
      },
      "source": [
        "## SVMs are sparse! \n",
        "\n",
        "There is even more we can say about the SVM solution. In fact, we will see that when we get the  dual variables $a_1, \\ldots, a_N$ most of their values are zero. So the SVM solution is determined only by a **subset** of training points, which are known as **support vectors**.\n",
        "\n",
        "Despite we do not prove it, given the problem\n",
        "\\begin{align}\n",
        "&\\max_{\\mathbf{a}}  ~~~~ \\sum_{i=1}^{N}a^{(i)} -\\frac{1}{2}\\sum_{i=1}^N \\sum_{j=1}^N a^{(i)} a^{(j)} y^{(i)} y^{(j)} {\\mathbf{x}^{(i)}}^\\top\\mathbf{x}^{(j)}\\\\\n",
        "\\text{s.t.} ~~~~ & a^{(i)} \\geq 0, ~ i=1,\\ldots,N\\\\\n",
        "& 0  = \\sum_{i=1}^{N} a^{(i)}  y^{(i)}\n",
        "\\end{align}\n",
        "\n",
        "the [**Karush-Kuhn-Tucker (KKT)**](http://www.onmyphd.com/?p=kkt.karush.kuhn.tucker) conditions require that the solution of the problem must verify, for $i=1,\\ldots,N$, the following:\n",
        "\n",
        "  1. $a^{(i)}\\geq 0$\n",
        "  2. $y^{(i)}f(\\mathbf{x}^{(i)})-1\\geq 0$\n",
        "  3. $a^{(i)}\\left(y^{(i)}(f(\\mathbf{x}^{(i)})-1)\\right)=0$\n",
        "\n",
        "If you want to understand how to prove these results, check out Appendix E in Bishop's book.\n",
        "\n",
        "## Support Vectors\n",
        "\n",
        "Contidion (3) implies that, for any point in our training set, either $a^{(i)}=0$ or $f(\\mathbf{x}^{(i)})-1 = 0$. This means that either the point **do not participate in the SVM solution** or that the point lies **exactly in the margin**. Points for which $a^{(i)}>0$ are called **support vectors (SVs)** and are the only ones defining the separation hyperplane:\n",
        "\n",
        "\\begin{align}\n",
        "\\mathbf{w} &=\\sum_{i: a^{(i)}>0} a^{(i)} y^{(i)} \\mathbf{x}^{(i)}\n",
        "\\end{align}\n",
        "and the prediction for future values:\n",
        "\\begin{align}\n",
        "f(\\mathbf{x}^*) &= \\sum_{i: a^{(i)}>0} a^{(i)} y^{(i)} {\\mathbf{x}^{(i)}}^\\top\\mathbf{x}^*+w_0\n",
        "\\end{align}\n",
        "\n",
        "The fraction of SVs w.r.t. to the total number of training points must be read as a measure of the complexity of the model and how much it is exposed to **overfitting**. The more we have, the poorest generalization we can expect.\n",
        "\n",
        "\n",
        "Besides, SVs can help us to find the value $w_0$, since we know that any support vector satisfies $y^{(i)}f( \\mathbf{x}^{(i)}) = 1$, so:\n",
        "$$y^{(i)} \\left( \\sum_{i: a^{(i)}>0} a^{(i)} y^{(i)} {\\mathbf{x}^{(i)}}^\\top\\mathbf{x}^{(i)} +w_0 \\right) = 1$$\n",
        "and (averaging over all SVs)\n",
        "$$w_0 = \\frac{1}{N_S}\\sum_{i: a^{(i)}>0} \\left(y^{(i)}-\\sum_{j: a^{(j)}>0} a^{(j)} y^{(j)}{\\mathbf{x}^{(i)}}^\\top\\mathbf{x}^{(j)}\\right)$$\n",
        "\n",
        "\n",
        "Let's plot the support vectors in our example."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kJ_nv3yDlZT7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.svm import SVC # \"Support vector classifier using the dual formulation\"\n",
        "model = SVC(kernel='linear', C=1E10)   # We use a linear kernel (no transformation), we will explain during next session.\n",
        "                                       # Also, we explain below the role of C\n",
        "model.fit(X, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZflSq8vTlElc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_svc_decision_function(model, ax=None, plot_support=True):\n",
        "    \"\"\"Plot the decision function for a 2D SVC\"\"\"\n",
        "    if ax is None:            #If no figure handle is provided, it opens the current figure\n",
        "        ax = plt.gca()\n",
        "        \n",
        "    xlim = ax.get_xlim()\n",
        "    ylim = ax.get_ylim()\n",
        "    \n",
        "    # create grid to evaluate model\n",
        "    x = np.linspace(xlim[0], xlim[1], 30)    #30 points in the grid axis\n",
        "    y = np.linspace(ylim[0], ylim[1], 30)\n",
        "    Y, X = np.meshgrid(y, x)                 # We create a grid with the x,y coordinates defined above\n",
        "    \n",
        "    # From the grid to a list of (x,y) values. \n",
        "    # Check Numpy help for ravel()\n",
        "    \n",
        "    xy = np.vstack([X.ravel(), Y.ravel()]).T \n",
        "    P = model.decision_function(xy).reshape(X.shape)\n",
        "    \n",
        "    # plot decision boundary and margins\n",
        "    # In levels we provide a list of floating point numbers indicating \n",
        "    #the level curves to draw, in increasing order; e.g., to draw just the zero contour pass\n",
        "    ax.contour(X, Y, P, colors='k',\n",
        "               levels=[-1, 0, 1], alpha=0.5,\n",
        "               linestyles=['--', '-', '--'])\n",
        "    \n",
        "    # plot support vectors\n",
        "    if plot_support:\n",
        "        ax.scatter(model.support_vectors_[:, 0],\n",
        "                   model.support_vectors_[:, 1],\n",
        "                   s=300, linewidth=1, marker='+')\n",
        "    ax.set_xlim(xlim)\n",
        "    ax.set_ylim(ylim)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RyPdzbKy6awc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure()\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='autumn')\n",
        "plot_svc_decision_function(model, plot_support=True)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_Ae_wzgD2FP6",
        "colab_type": "text"
      },
      "source": [
        "## Exercise 1\n",
        "\n",
        "## Exercise 1.1\n",
        "\n",
        "Analyze the model that we have just trained. Check the following parameters:\n",
        "* model.n_support_\n",
        "* model.support_\n",
        "* model.support_vectors_\n",
        "* model.dual_coef_"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Yi3pc6KflZGU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# <SOL>\n",

        "# </SOL>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "huJe4LoI4hpW",
        "colab_type": "text"
      },
      "source": [
        "## Exercise 1.2\n",
        "\n",
        "Use these parameters to compute the weigth vector ($\\mathbf{w}$) and the intercept term ($w_0$). Check that the obtained solution matches that saved in the model parameters coef_ and intercept_. Besides, you can also check this solution with that provided by the primal problem resolution. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_gElnxnYmdns",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# <SOL>\n",

        "# </SOL>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mXlptoX-6awg",
        "colab_type": "text"
      },
      "source": [
        "# Support Vector Machines: Dealing with non-separable datasets\n",
        "\n",
        "So far, the whole SVM formulation builds up over the assumption that the data is separable by an hyperplane. I.e., that there exists an hyperplane $f(\\mathbf{x}) = \\mathbf{w}^T\\mathbf{x}+w_0=0$ that verifies\n",
        "\\begin{align}\n",
        "y^{(i)}f(\\mathbf{x}^{(i)})\\geq 1, ~~ \\forall (\\mathbf{x}^{(i)},y^{(i)}),\n",
        "\\end{align}\n",
        "where points holding the equality are the support vectors. \n",
        "\n",
        "In order to **relax** this assumption and prevent **overfitting**, we could allow certain **training** points to be missclassified. We introduce the so-called **slack** variables:\n",
        "\n",
        "\\begin{align}\n",
        "y^{(i)}f(\\mathbf{x}^{(i)})\\geq 1-\\xi^{(i)}, ~~ \\forall (\\mathbf{x}^{(i)},y^{(i)})\n",
        "\\end{align}\n",
        "where $\\xi^{(i)}\\geq 0$:\n",
        "- Training points for which $\\xi^{(i)} = 0 $ are correctly classified and outside the SVM margin:  $y^{(i)}f(\\mathbf{x}^{(i)}) > 1$.\n",
        "- Training points for which $\\xi^{(i)}\\leq 1$ are correctly classified but inside the margin:  $0<y^{(i)}f(\\mathbf{x}^{(i)}) < 1$.\n",
        "- Training points for which $\\xi^{(i)} > 1$ are in the wrong side of the decision boundary: $y^{(i)}f(\\mathbf{x}^{(i)}) < 0$.\n",
        "\n",
        "<center> <img src=\"http://www.tsc.uc3m.es/~vanessa/Figs_notebooks/ML/SVMs/Fig5.png\" width=\"40%\" ></center>\n",
        "\n",
        "## Optimization problem with slack variables\n",
        "\n",
        "The optimization problem can now be written as follows:\n",
        "\\begin{align}\n",
        "&\\min_{\\mathbf{w},w_0} = \\frac{1}{2} ||\\mathbf{w}||^2 + C \\sum_{i=1}^{N}\\xi^{(i)}\\\\\n",
        "\\text{s. t.}~~~~ & y^{(i)}f(\\mathbf{x}^{(i)})\\geq 1-\\xi^{(i)}, ~ i=1,\\ldots,N\\\\\n",
        "&\\xi^{(i)}\\geq 0, ~ i=1,\\ldots,N\n",
        "\\end{align}\n",
        "\n",
        "where the $C$ parameter is introduced to control the regularization. For $C\\rightarrow\\infty$ we recover the original SVM formulation, as the  solution tends to $\\xi^{(i)}=0$ for $i=1,\\ldots,N$.\n",
        "\n",
        "Now, the Lagrange function to be optimized is\n",
        "\n",
        "\\begin{align}\n",
        "\\mathcal{L}(\\mathbf{w},w_0,\\mathbf{a},\\mathbf{b}) = \\frac{1}{2} ||\\mathbf{w}||^2+ C \\sum_{i=1}^{N}\\xi^{(i)} -\\sum_{i=1}^N a^{(i)} (y^{(i)}f(\\mathbf{x}^{(i)})-1+\\xi_i)-\\sum_{i=1}^{N}b^{(i)}\\xi^{(i)}\n",
        "\\end{align}\n",
        "\n",
        "### KKT conditions\n",
        "\n",
        "The KKT conditions associated to the new optimization problem are, for $i=1,\\ldots,N$,  as follows:\n",
        "\n",
        "1. $\\xi^{(i)}\\geq 0$\n",
        "2. $a^{(i)}\\geq 0$\n",
        "3. $b^{(i)}\\geq 0$\n",
        "4. $b^{(i)} \\xi^{(i)} = 0$\n",
        "5. $y^{(i)}f(\\mathbf{x}^{(i)})-1 + \\xi^{(i)} \\geq 0$\n",
        "6. $ a^{(i)}\\left(y^{(i)}f(\\mathbf{x}^{(i)}-1 +\\xi^{(i)} \\right) = 0$\n",
        "\n",
        "### Support Vectors\n",
        "\n",
        "As before, the last condition implies that, for any point in our training set, either $a^{(i)}=0$ or $f(\\mathbf{x}^{(i)})-1+\\xi^{(i)} = 0$. This means that either:\n",
        "* the point **do not participate in the SVM solution** ($a^{(i)}=0$), \n",
        "* the point lies **exactly in the margin** ($y^{(i)}f(\\mathbf{x}^{(i)})=1$ and $\\xi^{(i)} = 0$), \n",
        "* or the point is **inside the margin or wrongly classified** ($\\xi^{(i)} = 1 - y^{(i)}f(\\mathbf{x}^{(i)})>0$).\n",
        "\n",
        "Points for which $a^{(i)}>0$ are called **support vectors** and are the only ones defining the separation hyperplane and the prediction for future values!\n",
        "\n",
        "### Dual problem\n",
        "\n",
        "If we compute the gradient Lagrange function w.r.t. $\\mathbf{w}$, $w_0$ and $\\xi_i$ and equalize to zero we get the following conditions\n",
        "\n",
        "\\begin{align}\n",
        "\\mathbf{w} &=\\sum_{i=1}^N a^{(i)} y^{(i)} \\mathbf{x}^{(i)}, ~~~~~~\n",
        "0 = \\sum_{i=1}^{N} a^{(i)}  y^{(i)}, ~~~~~~~ a^{(i)} = C-b^{(i)}\n",
        "\\end{align}\n",
        "\n",
        "If we substitute them in the Lagrange function, we derive the folowing dual optimization problem, \n",
        "\\begin{align}\n",
        "&\\max_{\\mathbf{a}}  ~~~~ \\sum_{i=1}^{N}a^{(i)} -\\frac{1}{2}\\sum_{i=1}^N \\sum_{j=1}^N a^{(i)} a^{(j)} y^{(i)} y^{(j)}{\\mathbf{x}^{(i)}}^\\top\\mathbf{x}^{(j)}\\\\\n",
        "\\text{s.t.} ~~~~ & 0\\leq a^{(i)} \\leq C, ~ i=1,\\ldots,N\\\\\n",
        "& 0  = \\sum_{i=1}^{N} a^{(i)}  y^{(i)}\n",
        "\\end{align}\n",
        "\n",
        "which is also a quadratic problem with complexity $\\mathcal{O}(N^3)$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dw3EUdLl6RDM",
        "colab_type": "text"
      },
      "source": [
        "###  The hinge error function\n",
        "\n",
        "We have seen that data lying on the correct side of the margin ($y^{(i)}f(\\mathbf{x}^{(i)})>1$) have $\\xi^{(i)}= 0$, whereas remaining points have $\\xi^{(i)}= 1- y^{(i)}f(\\mathbf{x}^{(i)})$; so, we can write the SVM formulation as:\n",
        "\n",
        "$$\\mathbf{w}^* =\\displaystyle \\underset{{\\bf w}}{\\operatorname{min}}\\sum_{i=1}^N  \\left[ 1 - f(\\mathbf{x}^{(i)}) y^{(i)}\\right]_+ + \\frac{1}{2C} ||\\mathbf{w}||^2 $$  \n",
        "where $[\\cdot]_+$ denotes the positive part. This cost function, known as **hinge loss**, is un upper bound of the classification error."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SzVzQs-b5GTm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot the hinge loss function (un upper bound of the classfication error)\n",
        "f = np.arange(-4,4,0.01)\n",
        "y = 1\n",
        "l_w = 1-y*f\n",
        "l_w[l_w<0]=0\n",
        "plt.figure()\n",
        "plt.plot(y*f,l_w, label='Hinge loss')\n",
        "\n",
        "\n",
        "# Classification error\n",
        "e_class = np.zeros(f.shape)\n",
        "e_class[y*f<0] =1\n",
        "plt.plot(y*f,e_class, label='Classification error')\n",
        "\n",
        "plt.legend()\n",
        "plt.xlabel('y*f')\n",
        "plt.ylabel('Loss function')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "abkoLxA96awh",
        "colab_type": "text"
      },
      "source": [
        "Now, let's play with a non-linearly separable data set. So, let's generate a new dataset..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zAFNtEgNdse7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets.samples_generator import make_blobs\n",
        "X, y = make_blobs(n_samples=500, centers=2,\n",
        "                  random_state=10, cluster_std=5)    #With random_state we fix the random seed\n",
        "\n",
        "# Define the labels as -1, +1\n",
        "y = 2*y-1\n",
        "\n",
        "#We separate away some data for test\n",
        "X_test = X[100:,:] \n",
        "y_test = y[100:]\n",
        "\n",
        "X = X[:100,:]\n",
        "y = y[:100]\n",
        "\n",
        "print(\"The shape of X is \",X.shape)\n",
        "\n",
        "plt.scatter(X[:, 0], X[:, 1], c=y, s=50, cmap='RdBu')\n",
        "\n",
        "plt.rcParams[\"figure.figsize\"] = [8,8]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H_jX8HSieLEc",
        "colab_type": "text"
      },
      "source": [
        "## Exercise 2\n",
        "\n",
        "Train a linear SVM to solve the classification problem that we have just defined. Analyze its solution for different values of the C parameter (0.01, 1, 100, 1e4, 1e6):\n",
        "* Plot the classification problem and the given solution.\n",
        "* Analyze the number of SVs and the margin width.\n",
        "* Plot the SVs. Which SVs are on the margin, inside the margin or wrongly classified? \n",
        "\n",
        "You could train the linear SVM using either its primal or dual SVM implementation. However, to be able to analyze the SVs, use the dual one: [SVC()](https://scikit-learn.org/stable/modules/generated/sklearn.svm.SVC.html)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fMG8gdCUeKcb",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# <SOL>\n",

        "# </SOL>\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4eL3OJbPq-z2",
        "colab_type": "text"
      },
      "source": [
        "# SVMs in multiclass problems\n",
        "\n",
        "When we have to face a multiclass problems, some classifiers are inherently multiclass (for instance, K-Nearest Neighbours); however,  other ones, such as the SVM, have been designed for the binary case. So, when we have to deal with multipclass problems, we have to convert our multiclass problem in a set of binary ones. There are two main strategies for this:\n",
        "\n",
        "### One vs. one\n",
        "* It trains a binary classifier per pair of classes. \n",
        "* At prediction time, the class which receives the most votes is selected. \n",
        "* It requires to train $\\frac{M (M - 1)}{2}$ classifiers.\n",
        "* Each individual learning problem involves a small subset of the data.\n",
        "\n",
        "### One vs. all\n",
        "* It defines binary problems fitting each class against the remaining classes. \n",
        "* Only $M$ classifiers are trained (computational efficiency).\n",
        "* A gain of interpretability (each class is represented by one classifier).\n",
        "* This is the most commonly used strategy.\n",
        "\n",
        "By default, Scikit-learn implements all classifiers in a one vs. all fashion. However, to let the user select the multiclass implementation to be used, it also includes two wrapper functions that work over the classifier to force the multiclass strategy to be applied: [OneVsRestClassifier( )](http://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsRestClassifier.html#sklearn.multiclass.OneVsRestClassifier) and [OneVsOneClassifier( )](http://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsOneClassifier.html#sklearn.multiclass.OneVsOneClassifier). \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OMlSp7x0tIte",
        "colab_type": "text"
      },
      "source": [
        "## Exercise 3\n",
        "\n",
        "The following code builds a bidimensional problem with four classes and 500 samples, it creates the training and testing partitions with the 60% and 40% of the original data and, it normalizes the data to zero mean and unitary standard deviation."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sh84wwS_tHgL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Initialize the random generator seed to compare results\n",
        "np.random.seed(0)\n",
        "\n",
        "# Generating artifial data\n",
        "X, Y = datasets.make_classification(n_samples=500, n_features=2, n_classes = 4, n_clusters_per_class=1,\n",
        "                                    class_sep=2, n_redundant=0, flip_y =0.01)\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=.4)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qjjPxLRDtbVT",
        "colab_type": "text"
      },
      "source": [
        "Considering as default classifier a linear SVM ( with C=100), obtain two different multiclass configurations: one vs. all and one vs. one. For this purpose, you can use the functions: [OneVsRestClassifier( )](http://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsRestClassifier.html#sklearn.multiclass.OneVsRestClassifier) and [OneVsOneClassifier( )](http://scikit-learn.org/stable/modules/generated/sklearn.multiclass.OneVsOneClassifier.html#sklearn.multiclass.OneVsOneClassifier). \n",
        "\n",
        "Check the number of classifiers that are built by each configuration (you can use access to the .estimators\\_ parameter of the resulting classifier object). Finally, compute their classification accuracies over the test data.\n",
        "\n",
        "When you complete your code, you can use next function to plot the classification boundaries.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "px5GbIe4utjS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot the decision boundary\n",
        "def plot_multiclass_boundary(clf, X, Y):\n",
        "    \"\"\"Plot the classification regions for a given classifier.\n",
        "\n",
        "    Args:\n",
        "        clf: scikit-learn classifier object.\n",
        "        X (numpy dnarray): training or test data to be plotted (number data x number dimensions). Only frist two \n",
        "                            dimensions are ploted\n",
        "        Y (numpy dnarray): labels of the training or test data to be plotted (number data x 1).\n",
        "        plt: graphic object where you wish to plot                                             \n",
        "   \n",
        "    \"\"\"\n",
        "\n",
        "    plot_colors = \"brymc\"\n",
        "    plot_step = 0.02\n",
        "    plt.figure()\n",
        "        \n",
        "\n",
        "    n_classes = np.unique(Y).shape[0]\n",
        "    # Plot the decision regions\n",
        "    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1\n",
        "    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1\n",
        "    xx, yy = np.meshgrid(np.arange(x_min, x_max, plot_step),\n",
        "                        np.arange(y_min, y_max, plot_step))\n",
        "\n",
        "    Z = clf.predict(np.c_[xx.ravel(), yy.ravel()])\n",
        "    Z = Z.reshape(xx.shape)\n",
        "    cs = plt.contourf(xx, yy, Z, cmap=plt.cm.Paired)\n",
        "\n",
        "    plt.xlabel('Feature 1')\n",
        "    plt.ylabel('Feature 2')\n",
        "    plt.axis(\"tight\")\n",
        "\n",
        "    # Plot the training points\n",
        "    for i, color in zip(range(n_classes), plot_colors):\n",
        "        idx = np.where(Y == i)\n",
        "        plt.scatter(X[idx, 0], X[idx, 1], c=color, cmap=plt.cm.Paired)\n",
        "\n",
        "    plt.axis(\"tight\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1oy8aIbGtAGG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# <SOL>\n",

        "# </SOL>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KGa0IAjduRFF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# <SOL>\n",

        "# </SOL>"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}