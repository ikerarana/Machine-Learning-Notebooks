{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "colab": {
      "name": "KernelMethods.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RqvtCBbR0BH6",
        "colab_type": "text"
      },
      "source": [
        "# Week 8-9: Kernel methods \n",
        "\n",
        "----------------------------------------------------\n",
        "Machine Learning     Year 2019/2020\n",
        "\n",
        "*Vanessa GÃ³mez Verdejo vanessa@tsc.uc3m.es* and *Pablo M. Olmos olmos@tsc.uc3m.es*\n",
        "\n",
        "----------------------------------------------------"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ar1Fa4GNUecz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "# use seaborn plotting defaults\n",
        "import seaborn as sns; sns.set()\n",
        "plt.rcParams[\"figure.figsize\"] = [6,6]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0NxNijN0juL",
        "colab_type": "text"
      },
      "source": [
        "# 1. Introduction to kernel methods\n",
        "\n",
        "## Review of linear models\n",
        "\n",
        "Many of the algorithms presented in previous lessons are intended to infer a target variable $y$ (either a label or regressor) from a set of features $\\mathbf{x}\\in\\mathbb{R}^D$, for this purpose, they define a linear model. For instance: \n",
        "* **Linear regression** estimates the target variable for a new sample $\\mathbf{x}^*$ as a linear combination of the input features:\n",
        "\n",
        "$$\\hat{y}^* = f(\\mathbf{x}^*) = \\mathbf{w}^T\\mathbf{x}^*+w_0$$\n",
        "\n",
        "* Both **logistic regression** and **support vector machines**, to separate the data of one class from the other, define a linear classification boundary given by\n",
        "$$ f(\\mathbf{x}) = \\mathbf{w}^T\\mathbf{x}+w_0=0$$\n",
        "\n",
        "\n",
        "\n",
        "However, in many situations, the relationships between the observations and the target variables are rather nonlinear and we need models able to capture these relationships.\n",
        "\n",
        "For instance, consider the following binary classification problem:\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EsV--X2tUQVI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Initialize the random generator seed to compare results\n",
        "np.random.seed(0)\n",
        "\n",
        "# Generating artifial data\n",
        "X0 = 0.5*np.random.randn(40,1)\n",
        "Y0 = np.ones((40,))\n",
        "X1 = 0.5*np.random.randn(20,1)-2\n",
        "X2 = 0.5*np.random.randn(20,1)+2\n",
        "Y12 = -np.ones((40,))\n",
        "\n",
        "X = np.concatenate((X0,X1,X2))\n",
        "y = np.concatenate((Y0,Y12))\n",
        "\n",
        "plt.figure()\n",
        "plt.scatter(X, np.zeros((80,)), c=y, s=50, cmap='autumn')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TqafQMVCZHuM",
        "colab_type": "text"
      },
      "source": [
        "Let's plot different possibles solutions of a linear classifier:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eg-jq97EYEHa",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure()\n",
        "\n",
        "xfit = np.linspace(-0.02, 0.02)\n",
        "\n",
        "plt.scatter(X, np.zeros((80,)), c=y, s=50, cmap='autumn')\n",
        "\n",
        "\n",
        "m,b = (0, -1)\n",
        "plt.plot( m * xfit + b,  xfit,'-k')\n",
        "\n",
        "m,b = (0, 0)\n",
        "plt.plot( m * xfit + b,  xfit,'-m')\n",
        "\n",
        "m,b = (0, 1)\n",
        "plt.plot( m * xfit + b,  xfit,'-g')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oLGJfirL9k7a",
        "colab_type": "text"
      },
      "source": [
        "we can check that any of these solutions is suboptimal. However, we know that we can find a no-linear solution with a polynomial extension of the input features."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L8Ua4tX3W_tl",
        "colab_type": "text"
      },
      "source": [
        "## Polyniomial extension\n",
        "\n",
        "We have also seen that we can use a non-linear transformation of the input features to generate a new set of features. For example, if we consider the above classification problem (with a single input feature), we can create a new set of features with polynomial extension of degree $2$ as follows\n",
        "\n",
        "$${\\bf \\phi}(\\mathbf{x}) = [x, x^2]$$ \n",
        "where $\\phi()$ is the transformation function. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hyztDVMvZTFc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Let's create the polynomial extension\n",
        "phiX = np.hstack((X, X**2))\n",
        "\n",
        "plt.figure()\n",
        "plt.scatter(phiX[:, 0], phiX[:, 1],  c=np.squeeze(y), s=50, cmap='autumn')\n",
        "plt.show() \n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kdm579rfdvm2",
        "colab_type": "text"
      },
      "source": [
        "Now, we can use these new features to learn our linear model:\n",
        "\n",
        "$$f(x) = w_0 +w_1 \\phi_1(\\mathbf{x}) + w_2 \\phi_2(\\mathbf{x}) = w_0 +w_1 x + w_2 x^2 $$\n",
        "\n",
        "For example, we can train a linear SVM with these new features.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BbBHVk_veaOA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.svm import SVC # \"Support vector classifier using the dual formulation\"\n",
        "model = SVC(kernel='linear', C=1)   # We use a linear kernel (no transformation), we will explain during next session.\n",
        "                                       # Also, we explain below the role of C\n",
        "model.fit(phiX, y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FFPf6R2Eevce",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plt.figure(figsize=(12,6))\n",
        "\n",
        "plt.subplot(1,2,1)\n",
        "\n",
        "plot_step = 0.02\n",
        "\n",
        "x_min, x_max = phiX[:, 0].min() - 1, phiX[:, 0].max() + 1\n",
        "y_min, y_max = phiX[:, 1].min() - 1, phiX[:, 1].max() + 1\n",
        "XX, YY = np.meshgrid(np.arange(x_min, x_max, plot_step),\n",
        "                        np.arange(y_min, y_max, plot_step))\n",
        "\n",
        "\n",
        "xy = np.vstack([XX.ravel(), YY.ravel()]).T \n",
        "P = model.predict(xy).reshape(XX.shape)\n",
        "\n",
        "plt.contourf(XX, YY, P, cmap=plt.cm.Paired) \n",
        "\n",
        "plt.scatter(phiX[:, 0], phiX[:, 1], c=y, s=50, cmap='autumn')\n",
        "\n",
        "plt.title('High dimensional space')\n",
        "\n",
        "plt.subplot(1,2,2)\n",
        "\n",
        "\n",
        "x_min, x_max = X[:, 0].min() - 1, phiX[:, 0].max() + 1\n",
        "y_min, y_max = -0.2, 0.2\n",
        "XX, YY = np.meshgrid(np.arange(x_min, x_max, plot_step),\n",
        "                        np.arange(y_min, y_max, plot_step))\n",
        "\n",
        "\n",
        "xy = np.vstack([XX.ravel(), (XX**2).ravel()]).T \n",
        "P = model.predict(xy).reshape(XX.shape)\n",
        "\n",
        "plt.contourf(XX, YY, P, cmap=plt.cm.Paired) \n",
        "\n",
        "plt.scatter(X, np.zeros((80,)), c=y, s=50, cmap='autumn')\n",
        "plt.title('Original space')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cukhMLX4XvCV",
        "colab_type": "text"
      },
      "source": [
        "Note that $\\phi()$ representes the transformation function able to project the data from a space of dimension $D=1$ to a higher dimension space (in this case, $D'=2$). So, our model is linear in this high dimensional space, but the provided solution is nonlinear in the original space. That is, **a nonlinear model can be constructed by a nonlinear transformation to a space of higher dimension**.\n",
        "\n",
        "But, can we generalize the transformation function $\\phi()$ to any (no-)linear transformation of our observations? The answer to this question is **yes** and we can do it by means of the **kernel functions**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jSHNcIt2DHaT",
        "colab_type": "text"
      },
      "source": [
        "## Kernel functions\n",
        "\n",
        "Let's consider a mapping function ${\\bf \\phi}(\\mathbf{x})$ able to find a new representation of our data from the input space to a high dimesional space, called **feature space**\n",
        "\n",
        "\\begin{align}\n",
        "\\phi(\\mathbf{x}): \\mathbb{R}^D \\rightarrow \\mathbb{R}^{D'},\n",
        "\\end{align}\n",
        "\n",
        "where $D'\\geq D$.\n",
        "\n",
        "The kernel function, $K(\\cdot,\\cdot)$, is just the inner product of two data points in this new feature space:\n",
        "\n",
        " $$K(\\mathbf{x},\\mathbf{x}') = <\\phi(\\mathbf{x}),\\phi(\\mathbf{x}')> = \\phi(\\mathbf{x})^T\\phi(\\mathbf{x}')$$\n",
        "\n",
        "We can see that the kernel is a symmetric function of its arguments since $K(\\mathbf{x},\\mathbf{x}') = K(\\mathbf{x}',\\mathbf{x})$.\n",
        "\n",
        "\n",
        "The concept of a kernel formulated as an inner product in a feature space allows us to build interesting extensions of many well-known algorithms by making use of the  **kernel trick**, also known as kernel substitution. \n",
        "\n",
        "The general idea is that, if we have an algorithm formulated in such a way that the input vector $\\mathbf{x}$ enters only in the form of scalar products, then we can firstly map the data to the feature space and we can later replace that scalar product in the feature space with the associated kernel funtion.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JZsj3hAfPbZi",
        "colab_type": "text"
      },
      "source": [
        "# 2. Kernel formulation of SVMs\n",
        "\n",
        "**Dual formulation of the SVMs**\n",
        "\n",
        "As we know, the dual SVM formulation is given by the following optimization problem:\n",
        "\n",
        "\\begin{align}\n",
        "&\\max_{\\mathbf{a}}  ~~~~ \\sum_{i=1}^{N}a^{(i)} -\\frac{1}{2}\\sum_{i=1}^N \\sum_{j=1}^N a^{(i)} a^{(j)} y^{(i)} y^{(j)}\\color{red}{{\\mathbf{x}^{(i)}}^\\top\\mathbf{x}^{(j)}}\\\\\n",
        "\\text{s.t.} ~~~~ & 0\\leq a^{(i)} \\leq C, ~ i=1,\\ldots,N\\\\\n",
        "& 0  = \\sum_{i=1}^{N} a^{(i)}  y^{(i)}\n",
        "\\end{align}\n",
        "\n",
        "that can be solved by QP solver to find the optimum value of the dual parameters $\\mathbf{a}$ and, then, we can predict future values as:\n",
        "\\begin{align}\n",
        "f(\\mathbf{x}^*) &= \\sum_{i=1}^{N} a^{(i)} y^{(i)} \\color{red}{{\\mathbf{x}^{(i)}}^\\top\\mathbf{x}^*}+w_0\n",
        "\\end{align}\n",
        "note that $f(\\mathbf{x}^*) = \\mathbf{w}^T\\mathbf{x}^*+w_0$ and $\\mathbf{w} =\\sum_{i=1}^N a^{(i)} y^{(i)} \\mathbf{x}^{(i)}$.\n",
        "\n",
        "Both the optimization problem to find dual parameters and the estimation function are expressed in terms of inner products of the input data, so we can apply the **kernel trick** to obtain a no-linear SVM formulation.\n",
        "\n",
        "**Formulation of the SVMs in the feature space**\n",
        "\n",
        "If we project our data into a feature space by means of a mapping function ${\\bf \\phi}(\\cdot)$, we can rewrite the SVM formulation as:\n",
        "\n",
        "\\begin{align}\n",
        "&\\max_{\\mathbf{a}}  ~~~~ \\sum_{i=1}^{N}a^{(i)} -\\frac{1}{2}\\sum_{i=1}^N \\sum_{j=1}^N a^{(i)} a^{(j)} y^{(i)} y^{(j)}\\color{red}{{{\\bf \\phi}(\\mathbf{x}^{(i)})}^\\top {\\bf \\phi}(\\mathbf{x}^{(j)})}\\\\\n",
        "\\text{s.t.} ~~~~ & 0\\leq a^{(i)} \\leq C, ~ i=1,\\ldots,N\\\\\n",
        "& 0  = \\sum_{i=1}^{N} a^{(i)}  y^{(i)}\n",
        "\\end{align}\n",
        "\n",
        "Now, applying the **kernel trick**, we can newly rewrite this formulation as:\n",
        "\n",
        "\\begin{align}\n",
        "&\\max_{\\mathbf{a}}  ~~~~ \\sum_{i=1}^{N}a^{(i)} -\\frac{1}{2}\\sum_{i=1}^N \\sum_{j=1}^N a^{(i)} a^{(j)} y^{(i)} y^{(j)}\\color{red}{K(\\mathbf{x}^{(i)},\\mathbf{x}^{(j)})}\\\\\n",
        "\\text{s.t.} ~~~~ & 0\\leq a^{(i)} \\leq C, ~ i=1,\\ldots,N\\\\\n",
        "& 0  = \\sum_{i=1}^{N} a^{(i)}  y^{(i)}\n",
        "\\end{align}\n",
        "\n",
        "Once dual parameters are obtained solving this new optimization problem, we can also apply the kernel trick over the prediction function to be able to classify new samples:\n",
        "$$ \n",
        "f(\\mathbf{x}^*) = \\sum_{i=1}^{N} a^{(i)} y^{(i)} \\color{red}{{{\\bf \\phi}(\\mathbf{x}^{(i)})}^\\top {\\bf \\phi}(\\mathbf{x}^*)} +w_0 = \\sum_{i=1}^{N} a^{(i)} y^{(i)} \\color{red}{K(\\mathbf{x}^{(i)},\\mathbf{x}^*)}+w_0\n",
        "$$\n",
        "\n",
        "\n",
        "Note that this kernelized formulation has several advantages:\n",
        "\n",
        "* A nonlinear SVM can be constructed by a nonlinear transformation to a higher dimension space (feature space). Then, we can find the maximum margin hiperplane that separates our training data in this feature space and, at the same time, finding a no-linear solution in the input space.\n",
        "\n",
        "* The formulation is expressed in terms of kernel function. We don't even need to know the mapping function  ${\\bf \\phi}(\\mathbf{x})$. It's enough to know all paired kernel values over the training data.\n",
        "\n",
        "* If we wanted to recover the primal parameters ($\\mathbf{w}$), the mapping function  ${\\bf \\phi}(\\mathbf{x})$ should be known (which is not usually the case):\n",
        "$$\\mathbf{w} =\\sum_{i=1}^N a^{(i)} y^{(i)} {\\bf \\phi}(\\mathbf{x}^{(i)})$$\n",
        "However, as we are using the formulation in terms of dual variables and kernel functions, this is not needed anymore.\n",
        "\n",
        "* The output of the SVM can also be computed in terms of kernel functions. In this case we only have to compute the kernel between the new sample $\\mathbf{x}^*$ and the training data. And, due to most $a^{(i)}$ values of the SVM solution are zero, it's enough to compute the kernel between the new sample $\\mathbf{x}^*$ and the support vectors. Reducing the computational burden during the classification stage.\n",
        "\n",
        "* The kernel can be understood as a similarity metric. So, vectors closest to $\\mathbf{x}^*$ in the training set (or, among suppport vectors) are those that weight more in the prediction! \n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QAeOtjeAPZIl",
        "colab_type": "text"
      },
      "source": [
        "## Examples of kernel functions \n",
        "\n",
        "In case we need to construct a kernel function, we could directly  choose  a feature space mapping $\\phi(\\mathbf{x})$. For instance, if ${x}\\in\\mathbb{R}$ we can choose the following transformation\n",
        "\\begin{align}\n",
        "\\phi(x) =[1, x, ~ x^2] \n",
        "\\end{align}\n",
        "\n",
        "Then, the kernel function between two samples ${x}^{(1)}$ and ${x}^{(2)}$ is given by:\n",
        "\\begin{align}\n",
        "k({x}^{(1)},{x}^{(2)}) = \\phi(x)^{\\top} \\phi(x)= 1+ (x^{(1)}x^{(2)}) + \\left(x^{(1)}x^{(2)}\\right)^2\n",
        "\\end{align}\n",
        "\n",
        "However, the most common approach to construct kernel functions is to **directly choose the kernel**. In this case, we must ensure that the function we choose **corresponds to a scalar product in some (perhaps infinite dimensional) feature space** (technicaly, that [Mercer's theorem](http://people.cs.uchicago.edu/~niyogi/papersps/MinNiyYao06.pdf) has to satisfied).\n",
        "\n",
        "A very common kernel, that actually maps over an *infinite dimensional feature* space, is the **radial basis function (RBF) or Gaussian kernel**:\n",
        "\n",
        "\\begin{align}\n",
        "k(\\mathbf{x},\\mathbf{x}') = \\exp \\left( -\\frac{||\\mathbf{x}-\\mathbf{x}'||^2}{2\\sigma^2} \\right) = \\exp \\left( -\\gamma ||\\mathbf{x}-\\mathbf{x}'||^2 \\right)\n",
        "\\end{align}\n",
        "\n",
        "where either $\\sigma$ is called the bandwith and it is a hiperparameter that typically has to be adjusted by *cross validation*. Some implementions use an equivalent parameter $\\gamma=\\frac{1}{2\\sigma^2}$.\n",
        "\n",
        "Another common kernel is the **linear one**, defined as \n",
        "\\begin{align}\n",
        "k(\\mathbf{x},\\mathbf{x}') = {\\mathbf{x}}^{\\top}\\mathbf{x}'\n",
        "\\end{align}\n",
        "and it recovers the original linear formulation.\n",
        "\n",
        "Check this [**link**](http://scikit-learn.org/stable/modules/metrics.html) to see possibles examples of kernels that can be used within sklearn.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "E4CQpSPASJff",
        "colab_type": "text"
      },
      "source": [
        "### Exercise 1\n",
        "\n",
        "To analyze the advantages of this formultion, next cell generates a no-linear classification problem."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IBZ8pp3YSITi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import datasets\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Initialize the random generator seed to compare results\n",
        "np.random.seed(0)\n",
        "\n",
        "# Generating artifial data\n",
        "X, Y = datasets.make_classification(n_samples=500, n_features=2, n_classes = 2, n_clusters_per_class=2,\n",
        "                                    class_sep=1.5, n_redundant=0, flip_y =0.01)\n",
        "\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=.8)\n",
        "\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "plt.figure()\n",
        "plt.scatter(X_train[:, 0], X_train[:, 1], c=Y_train, cmap='autumn')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HxdDQc2fUNaM",
        "colab_type": "text"
      },
      "source": [
        "### Exercise 1.1 Linear SVM\n",
        "\n",
        "Analyze the performance of a linear SVM over the above problem. For the sake of simplicity, you can use a default value of C = 1. Once the classfier is trained:\n",
        "* Compute the accuracy over the test data\n",
        "* Use the function plot_svc_decision_function() to plot the classification problem and the SVM solution (togheter to the support vectors)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8xxVyqYmSIPc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_svc_decision_function(model, ax=None, plot_support=True):\n",
        "    \"\"\"Plot the decision function for a 2D SVC\"\"\"\n",
        "    if ax is None:            #If no figure handle is provided, it opens the current figure\n",
        "        ax = plt.gca()\n",
        "        \n",
        "    xlim = ax.get_xlim()\n",
        "    ylim = ax.get_ylim()\n",
        "    \n",
        "    # create grid to evaluate model\n",
        "    x = np.linspace(xlim[0], xlim[1], 30)    #30 points in the grid axis\n",
        "    y = np.linspace(ylim[0], ylim[1], 30)\n",
        "    Y, X = np.meshgrid(y, x)                 # We create a grid with the x,y coordinates defined above\n",
        "    \n",
        "    # From the grid to a list of (x,y) values. \n",
        "    # Check Numpy help for ravel()\n",
        "    \n",
        "    xy = np.vstack([X.ravel(), Y.ravel()]).T \n",
        "    P = model.decision_function(xy).reshape(X.shape)\n",
        "    \n",
        "    # plot decision boundary and margins\n",
        "    # In levels we provide a list of floating point numbers indicating \n",
        "    #the level curves to draw, in increasing order; e.g., to draw just the zero contour pass\n",
        "    ax.contour(X, Y, P, colors='k',\n",
        "               levels=[-1, 0, 1], alpha=0.5,\n",
        "               linestyles=['--', '-', '--'])\n",
        "    \n",
        "    # plot support vectors\n",
        "    if plot_support:\n",
        "        ax.scatter(model.support_vectors_[:, 0],\n",
        "                   model.support_vectors_[:, 1],\n",
        "                   s=300, linewidth=1, marker='+')\n",
        "    ax.set_xlim(xlim)\n",
        "    ax.set_ylim(ylim)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ho3VknH6TpH3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#<SOL>\n",

        "#</SOL>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qX_dpUQfUUFT",
        "colab_type": "text"
      },
      "source": [
        "### Exercise 1.2 RBF SVM\n",
        "\n",
        "Train a no-linear SVM with a RBF kernel, set C=1 and explore different values of the gamma parameter (0.01, 0.1, 1, 10, 100). Compute the test error and plot the classification boundary (and its margin) with plot_svc_decision_function() function.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KxXL_CiASIF_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#<SOL>\n",

        "#</SOL>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rQA_5_R7SIDE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#<SOL>\n",

        "#</SOL>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KHhy1PIwSH_J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#<SOL>\n",

        "#</SOL>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4z8tnMMT-NOc",
        "colab_type": "text"
      },
      "source": [
        "**Analyzing these results**\n",
        "\n",
        "In the light of the results try to answer the following questions:\n",
        "* Why do small values of $\\gamma$ (or large values of the RBF bandwidth) tend to provide linear solutions?.\n",
        "* Why do large values of $\\gamma$ (or small values of the RBF bandwidth) tend overfit?.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iB9sktqRZDls",
        "colab_type": "text"
      },
      "source": [
        "#### Answer\n",
        "\n",
        "#<SOL>\n",

        "#</SOL>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CZoPB3AIYW8w",
        "colab_type": "text"
      },
      "source": [
        "### Exercise 1.3 RBF SVM: parameter selection\n",
        "\n",
        "Use a 5-fold CV process to adjust both $C$ and $\\gamma$ parameters of a RBF SVM. Explore 10 values of $C$ from $0.001$ to $1000$, logarithmic equispaced, and define the range of $\\gamma$ as: $[0.125, 0.25, 0.5, 1, 2, 4, 8])/\\sqrt(D)$,\n",
        "being $D$ the data dimension. Note that this definition of the $\\gamma$ values is used alleviate the influence of the data dimension in the definition of the RBF kernel.\n",
        "\n",
        "Again, you can plot the final classification boundary and compute the test error to analyze the results.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vHsy8LJV53Zc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#<SOL>\n",

        "#</SOL>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IrnTwOY79aXJ",
        "colab_type": "text"
      },
      "source": [
        "## Representer Theorem\n",
        "\n",
        "**Can we extend the kernel formulation to other linear methods?**\n",
        "\n",
        "\n",
        "The answer to this question is given by the [**Representer Theorem**](https://en.wikipedia.org/wiki/Representer_theorem) which, in simple terms, says that the solution of any algorithm obtained by the minimization of a regularized empirical cost can be represented as a weigthed linear combination of the training data. \n",
        "\n",
        "\n",
        "So, if we are working with a linear algorithm \n",
        "\n",
        "$$ \n",
        "f(\\mathbf{x}) = \\mathbf{w}^T\\mathbf{x}\n",
        "$$\n",
        "\n",
        "and we learn the weights $\\mathbf{w}$ of this model by minimizing any regularized empirical cost function:\n",
        "\n",
        "$$\\bf w^* =\\displaystyle \\underset{\\bf{w}}{\\operatorname{min}} \\sum_{i=1}^N    L \\left( f( {\\bf x}^{(i)}) , y^{(i)} \\right) + \\lambda \\Vert \\bf{w} \\Vert^2 $$  \n",
        "\n",
        "where $L \\left( \\cdot, \\cdot \\right)$ is the cost function (for instance, the binomial deviance in logistic regression or the MSE in linear regression); then,the Representer Theorem let's assure that the parameters of the lineal model are a linear combination of the training data\n",
        "$$\\mathbf{w} =\\sum_{i=1}^N a^{(i)} \\mathbf{x}^{(i)}$$\n",
        "and\n",
        "$$ \n",
        "f(\\mathbf{x}^*) = \\sum_{i=1}^{N} a^{(i)} {\\mathbf{x}^{(i)}}^{\\top}\\mathbf{x}^*\n",
        "$$\n",
        "\n",
        "Even, in case that we are working over the feature space generated by a mapping function $\\phi(\\mathbf{x})$, we can say that the algorithm output is given by are a linear combination of kernel functions (inner products in this feature space)\n",
        "$$ \n",
        "f(\\mathbf{x}^*) = \\sum_{i=1}^{N} a^{(i)}  K(\\mathbf{x}^{(i)},\\mathbf{x}^*)\n",
        "$$\n",
        "and now the goal of the model training (or inference) is learning these dual variables $a^{(1)}, \\ldots, a^{(N)}$.\n",
        "\n",
        "So, **if an algorithm fits the Representer Theorem, a dual expression\n",
        "can be constructed as function of dot products (or kernel functions) between data**.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NwL7mUZEzFuX",
        "colab_type": "text"
      },
      "source": [
        "## Kernel Ridge Regression\n",
        "\n",
        "\n",
        "### Ridge linear regression model in the feature space\n",
        "\n",
        "Consider we have a **training** database of $N$ entries of the form $(\\mathbf{x}^{(i)},y^{(i)})$, where $\\mathbf{x}^{(i)}\\in\\mathbb{R}^D$ and $y^{(i)}\\in\\mathbb{R}$. Now, imagine that each of the input vectors is transformed into a **feature vector** as\n",
        "\n",
        "\\begin{align}\n",
        "\\phi(\\mathbf{x}): \\mathbb{R}^D \\rightarrow \\mathbb{R}^{D'},\n",
        "\\end{align}\n",
        "\n",
        "where $D'\\geq D$. We will use this training set to fit a model (in the feature space) of the form\n",
        "\n",
        "$$f(\\mathbf{x}) = \\theta_0 + \\theta_1 \\phi(\\mathbf{x})_1 + \\theta_2 \\phi(\\mathbf{x})_2 + \\ldots + \\theta_{D'} \\phi(\\mathbf{x})_{D'} $$\n",
        "note that the first element  of $\\phi(\\mathbf{x})$ is $1$, i.e. $\\phi(\\mathbf{x})_0=1$, so that it accomodates the intercept. And, the optimal values of $\\boldsymbol{\\theta}$ have to be found minimizing the **ridge loss function**:\n",
        "$$ L(\\boldsymbol{\\theta},\\lambda) = \\frac{1}{N} \\left[\\sum_{i=1}^{N} (y^{(i)}-\\boldsymbol{\\theta}^T\\phi(\\mathbf{x}^{(i)}))^2 + \\lambda \\sum_{j=1}^{D'} \\theta_j^2\\right]\n",
        "$$\n",
        "\n",
        "\n",
        "### The dual problem representation\n",
        "\n",
        "As this model satifies the Representer Theorem (it is minimizing a regularized empirical cost), we know that the optimal solution ($\\boldsymbol{\\theta}^*)$ can be expressed as a **linear combination of the mapped training data**\n",
        "\n",
        "\\begin{align}\n",
        "\\boldsymbol{\\theta}^* = \\sum_{i=1}^{N} a^{(i)} \\phi(\\mathbf{x}^{(i)}) = \\mathbf{\\Phi}^T\\mathbf{a},\n",
        "\\end{align}\n",
        "\n",
        "Thus, given $\\mathbf{a}$, the regression estimate for a new vector $\\mathbf{x}^{*}$ is:\n",
        "\n",
        "\\begin{align}\n",
        "f(\\mathbf{x}^*) =  \\sum_{i=1}^{N} a^{(i)} k(\\mathbf{x}^{(i)},\\mathbf{x}^*)\n",
        "\\end{align}\n",
        "\n",
        " \n",
        "### Finding the vector of coefficients $\\mathbf{a}$\n",
        "\n",
        "If we substitute $\\mathbf{\\theta} = \\mathbf{\\Phi}^T\\mathbf{a}$ in the Ridge Loss function we get the dual optimization problem. After some manipulation, the loss function is expressed in the following way:\n",
        "\n",
        "\\begin{align}\n",
        "L(\\boldsymbol{a},\\lambda) = \\frac{1}{N}\\left[\\mathbf{a}^T \\mathbf{K}^T\\mathbf{K}\\mathbf{a}-2\\mathbf{a}^T\\mathbf{K}\\mathbf{y}+\\lambda\\mathbf{a}^T\\mathbf{K}\\mathbf{a}\\right],\n",
        "\\end{align}\n",
        "where $\\mathbf{K}$ is the $(N\\times N)$ symmetric **kernel matrix** such that\n",
        "\n",
        "\\begin{align}\n",
        "K_{nm} = k(\\mathbf{x}^{(m)},\\mathbf{x}^{(n)}).\n",
        "\\end{align}\n",
        "\n",
        "$L(\\boldsymbol{a},\\lambda)$ is a convex function whose minimum is attained at\n",
        "\n",
        "\\begin{align}\n",
        "\\boldsymbol{a} = \\left(\\boldsymbol{K}+\\lambda \\boldsymbol{I}\\right)^{-1} \\boldsymbol{y}.\n",
        "\\end{align}\n",
        "\n",
        "Note that:\n",
        "* The complexity (to obtain the dual variables) now increases as $\\mathcal{O}(N^3)$ instead of the $\\mathcal{O}(D^3)$ complexity that we had in the feature primal space. \n",
        "* The obtained solution is **not sparse** (in general, $\\boldsymbol{a}$ values are not zero), unlike SVM solution; so, the estimation function \n",
        "\\begin{align}\n",
        "f(\\mathbf{x}^*) =  \\sum_{i=1}^{N} a^i k(\\mathbf{x}^{(i)},\\mathbf{x}^*)\n",
        "\\end{align}\n",
        "wil need to compute all the kenels between all training data and the test sample to estimate its ouput.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "collapsed": true,
        "id": "qsIb_yPMzFuc",
        "colab_type": "text"
      },
      "source": [
        "### Exercise 2\n",
        "\n",
        "Consider the following regression problem where a sinc() function has to be modeled."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_FFGmyAnzFuG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "np.random.seed(0)\n",
        "\n",
        "N_train = 25\n",
        "R = 20\n",
        "Xtrain = np.sort(np.random.uniform(-R,R,N_train), axis=0)       #Training points\n",
        "\n",
        "Xtest = np.linspace(-R,R,500)      #Test points\n",
        "\n",
        "def noise_sinc(X):\n",
        "    Y = np.sin(X)/np.pi/(X+1e-6) + np.sqrt(1e-04) * np.random.randn(X.shape[0])\n",
        "    return Y\n",
        "\n",
        "Ytrain = noise_sinc(Xtrain)\n",
        "Ytest = noise_sinc(Xtest)\n",
        "\n",
        "Xtrain = Xtrain[:, np.newaxis] #Define 2-dimensional data (.fit does not work with 1-dimensional vectors)\n",
        "Xtest = Xtest[:, np.newaxis]\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(Xtrain,Ytrain,'go',ms=15,label='train')\n",
        "plt.plot(Xtest,Ytest,'b-',label='test (true)')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6O-YOY_n8QZR",
        "colab_type": "text"
      },
      "source": [
        "Now, train a kernel ridge linear regression (KRR) with a RBF kernel to estimate the sinc() function (you can start with its default parameters). Once the model has been trained:\n",
        "* Analyze how does the kernel regressor behave as you vary either the number of training points, the alpha ($\\lambda$) parameter, the bandwith in the RBF kernel ($\\gamma$). \n",
        "* How do you identify that the model is overfitting? or if the model is too biased? how do you regularize?\n",
        "* Investigate also the effect of varying the kernel (linear, polynomial, ...)\n",
        "\n",
        "You can use [**sklearn implementation of KRR**](http://scikit-learn.org/stable/modules/generated/sklearn.kernel_ridge.KernelRidge.html#sklearn.kernel_ridge.KernelRidge)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oPXAgn96zFuP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#<SOL>\n",

        "#</SOL>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXwzgg2XHFql",
        "colab_type": "text"
      },
      "source": [
        "## Kernel Logistic Regression (KLR)\n",
        "\n",
        "We can also kernelize the formulation of the Logistic Regression classifier. As this algorithm is minimizing the regularized binomial deviance:\n",
        "\n",
        "$$ L(\\bf{w}) = \\sum_{i=1}^N \\left\\lbrace   \\log \\left( 1+ \\exp({\\bf w}^T {\\bf x}^{(i)})\\right) \\right\\rbrace  -y^{(i)} ({\\bf w}^T {\\bf x}^{(i)}) + \\lambda \\Vert {\\bf w} \\Vert_2^2$$\n",
        "\n",
        "it also satisfies the Representer Theorem, then we can claim that\n",
        "\n",
        "$$\\mathbf{w} =\\sum_{i=1}^N a^{(i)} \\mathbf{x}^{(i)}$$\n",
        "\n",
        "So, we can find a dual representation of this cost function in a feature space as:\n",
        "$$ L(\\bf{a}) = \\sum_{i=1}^N \\left\\lbrace   \\log \\left( 1+ \\exp(\\mathbf{a}^T \\bf{k}(\\mathbf{x}^{(i)})\\right) \\right\\rbrace  -y^{(i)} \\mathbf{a}^T \\bf{k}(\\mathbf{x}^{(i)}) + \\lambda\\mathbf{a}^T\\mathbf{K}\\mathbf{a}$$\n",
        "\n",
        "where $\\mathbf{K}$ is the $(N\\times N)$ symmetric training kernel matrix and $\\bf{k}(\\mathbf{x}^{(i)})$ is a column vector $(N\\times 1)$ with all the kernels between the training data and the data $\\mathbf{x}^{(i)}$.\n",
        "\n",
        "The KLR optimization problem is usually solved by gradient descend. However, its implementation is not included in sklearn as, for kernel methods in classification, sparse kernel machines like Support Vector Machines are preferred.\n",
        "\n",
        "\n"
      ]
    }
  ]
}