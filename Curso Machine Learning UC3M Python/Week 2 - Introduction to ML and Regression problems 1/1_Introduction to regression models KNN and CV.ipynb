{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "KNN_student.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8rh_Zt6M8h4W",
        "colab_type": "text"
      },
      "source": [
        "# Week 2: Introduction to regression problems \n",
        "\n",
        "----------------------------------------------------\n",
        "Machine Learning     Year 2019/2020\n",
        "\n",
        "*Vanessa GÃ³mez Verdejo vanessa@tsc.uc3m.es* and *Pablo M. Olmos olmos@tsc.uc3m.es*\n",
        "\n",
        "----------------------------------------------------\n",
        "\n",
        "To begin to understand what is a method or algorithm of machine learning, let's start working with a real problem: house sale-value prediction. For this purpose, we will start by loading **Boston housing dataset** and analyzing what it is made of.... "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4gMLwGPw8q_E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nDlA2VI56WDI",
        "colab_type": "text"
      },
      "source": [
        "### Boston housing database\n",
        "\n",
        "To goal of this problem or dataset is to predict the value of houses in the suburbs of Boston during the 80's using different features. The data was drawn from the Boston Standard Metropolitan Statistical Area (SMSA) in 1970.\n",
        "\n",
        "We can load this dataset from the sklearn repository as follows:\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WcutpdFI41ks",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.datasets import load_boston\n",
        "boston = load_boston()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G7GtzX5rtJQ3",
        "colab_type": "text"
      },
      "source": [
        "Variable 'boston' is a dictionary from where we can extract the data, the labels or targets, and some information about the feature meaning....  "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FocO0vomtJ77",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X = boston.data\n",
        "Y = boston.target\n",
        "feature_names = boston.feature_names"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2hdbftIJsExu",
        "colab_type": "text"
      },
      "source": [
        "Now, for instance, from the input data or observations, we can check their dimensions..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nKx630yusCrB",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(X.shape)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nwGpd-HFshl8",
        "colab_type": "text"
      },
      "source": [
        "Each record (a single each datum) is usually placed in a row; whereas columns are used for features or variables. In this dataset, we have 506 records or observations, and each one has 13 features.\n",
        "\n",
        "Here, each record in the database describes a Boston suburb (or town) and it is characterized by a set of features. If we want to know what the are representing, we can check variable 'feature_names' toghether to the sklearn [User Guide](https://scikit-learn.org/stable/datasets/index.html#boston-dataset).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yw3JrFHfuutJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(feature_names)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cBrqILoCv_4y",
        "colab_type": "text"
      },
      "source": [
        "Besides, we can acces to the target information. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iA2EZOk641ki",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print(Y.shape)\n",
        "print(Y[:10])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZBUJoVR7xmrG",
        "colab_type": "text"
      },
      "source": [
        "It's a numpy array with a single column with as many elements as records in X. Each element of Y is associated to a row of X."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GmU-lClFx6E3",
        "colab_type": "text"
      },
      "source": [
        "Finally, we can try to analyze the relationship between each input feature and the target variable by plotting Y vs. each feature...."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YiyHcvoC_5FG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "#%matplotlib inline\n",
        "\n",
        "D = X.shape[1]\n",
        "\n",
        "plt.figure( figsize=(20,5))\n",
        "for d in range (D):\n",
        "  plt.subplot(2,7,d+1)\n",
        "  plt.plot(X[:,d],Y,'.',)\n",
        "  plt.title(feature_names[d])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aRxHMNzo85r6",
        "colab_type": "text"
      },
      "source": [
        "Which variables are most useful to predict the house sale prize?\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZCk2X5ypCZL3",
        "colab_type": "text"
      },
      "source": [
        "# Machine Learning Pipeline\n",
        "\n",
        "When we have to design a regressor (or classifier) to solve a data analysis problem, we usually follow these steps:\n",
        "\n",
        "<img src=\"http://www.tsc.uc3m.es/~vanessa/Figs_notebooks/ML/Regression/MLPipeline.png\" width=\"100%\" > \n",
        "\n",
        "\n",
        "In the **preprocessing** step we can find several tasks:\n",
        "* Imputation of missing values\n",
        "* Dimensionality reduction: feature extraction or feature selection\n",
        "* Data normalization\n",
        "\n",
        "Along this course, we will review all these techniques. But for now, let's start with the data normalization."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "u9nBKjvaGudY",
        "colab_type": "text"
      },
      "source": [
        "# Preparing the dataset\n",
        "\n",
        "Folowing the ML pipeline, before desinging any ML regressor to predict the house sale-prize, we need:\n",
        "1. Generate the train and test partitions.  \n",
        "2. Normalize the data variables (preprocessing).\n",
        "3. Any additional preprocessing (in this case, feature selection).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "c__oJXr7ppKu",
        "colab_type": "text"
      },
      "source": [
        "## Generating training/test partitions\n",
        "This task consist in dividing the overall datasets into two independt subsets:\n",
        "* The **training subset**: used to train the ML model. We will later see that this subset can be divided into diferent (validation) subsets if hiperparameters of the ML model have to be adjusted.\n",
        "* The **test subset**: exclusively used for performance evaluation.\n",
        "\n",
        "This division usually depends on the avaliable number of data, but 60/40\\%, 70/30\\% or 80/20\\% train/test divisions are quite common.\n",
        "\n",
        "For this task, we can use some preprocessing tools from sklearn; for instance, we can use the function [train_test_split](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.train_test_split.html).\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H4CHRqkeGuKO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "# Parameter test_size lest you select the percentage of data into the test partition (in this case 20%)\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.4, random_state=0)\n",
        "\n",
        "# Note that we have to divide both observations and labels!!!"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kmoYaX6G6M6E",
        "colab_type": "text"
      },
      "source": [
        "## Normalizing our input data\n",
        "\n",
        "Next step implies data normalization. In general, features are defined in different ranges. I.e., if $x_1$ for instance can take values in the $[-1,1]$ set and $x_2$ in the $[-10^6,10^6]$ range. To improve numerical robustness of ML methods, we usually apply a linear normalization preprocessing stage to use as input to ML model features with (sample) mean 0 and (sample) variance equal to 1. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uhsAMKv12JS2",
        "colab_type": "text"
      },
      "source": [
        "Let's check the mean and standard deviation of our training data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "t3hartcCj_ls",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Analyze mean and std of each feature\n",
        "print('Mean values')\n",
        "print(np.mean(X_train, axis=0))\n",
        "\n",
        "print('Std values')\n",
        "print(np.std(X_train, axis=0))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oNU3O1X1eQUM",
        "colab_type": "text"
      },
      "source": [
        "**Normalization process**\n",
        "\n",
        "Given the unnormalized feature matrix $\\mathbf{X}_{N\\times D}$, we compute the sample mean per feature column , $\\mu_j$, and the sample variance per  feature column, $\\sigma^2_j$ for $j=1,\\ldots,D$. Then, each row of the normalized feature matrix $\\overline{\\mathbf{X}}_{N\\times D}$ is obtained as follows:\n",
        "\n",
        "$$\\overline{\\mathbf{x}}^{(i)}= \\left[\\frac{x_1^{(i)}-\\mu_1}{\\sqrt{\\sigma^2_1}}, ~~ \\frac{x_2^{(i)}-\\mu_2}{\\sqrt{\\sigma^2_2}}, \\ldots, \\frac{x_D^{(D)}-\\mu_D}{\\sqrt{\\sigma^2_D}}\\right]$$\n",
        "\n",
        "One last thing, in general we will not normalize the output variable $y$. We can do it and but typically it leads to mistakes in the evaluation of the cost function. \n",
        "\n",
        "To carry out this proccess we can also use other of the sklearn preprocessing tools: the object [StandardScalerStandardScaler](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.StandardScaler.html#sklearn.preprocessing.StandardScaler)\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3KzhM5GZ6pDR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Data normalization\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Define the object\n",
        "transformer = StandardScaler()\n",
        "transformer.fit(X_train)  # fit does nothing, just learns mean and std from data\n",
        "X_train_norm = transformer.transform(X_train)\n",
        "X_test_norm =  transformer.transform(X_test)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FzoV0CIvfBoW",
        "colab_type": "text"
      },
      "source": [
        "One important thing, we normalize the test data using the sample means and variances computed from the **training** set. This is important to **make sure that all datapoints are equally normalized**."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2o5dTpFpe8_5",
        "colab_type": "text"
      },
      "source": [
        "Check mean and std of the normalized data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PpQqypxB6pGR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "print('Mean values')\n",
        "print(np.mean(X_train_norm, axis=0))\n",
        "\n",
        "print('Std values')\n",
        "print(np.std(X_train_norm, axis=0))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B8dvbBcDk8ty",
        "colab_type": "text"
      },
      "source": [
        "### Exercise 1\n",
        "\n",
        "Desing your own normalization proccess"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fz7PrTtHk7iT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# <SOL>\n",
        "# </SOL>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IjthBS3xReKP",
        "colab_type": "text"
      },
      "source": [
        "## Other preprocesing tools: Feature selection\n",
        "Other possible preprocessing step can be a feature selection. This consists in selecting a subset of the original features which seem to be more usefull to solve the predicition task. \n",
        "\n",
        "There are many sophisticated methods to carry out this task (we wiil review some of them along this course), but now, simply from the above display (variables vs. target values), let's select the variables:\n",
        "* Feature 0: CRIM\n",
        "* Feature 5: RM\n",
        "* Feature 12: LSTAT \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0cVnw68ISmfS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sel_feat = [0, 5, 12]\n",
        "X_train_sel = X_train_norm[:,sel_feat]\n",
        "X_test_sel = X_test_norm[:,sel_feat]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qhGMAbbWSwYf",
        "colab_type": "text"
      },
      "source": [
        "Note that the feature selection is not affecting to the target vectors."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DdjDEnGcfYEs",
        "colab_type": "text"
      },
      "source": [
        "For the sake of simplicity, from now on, let's consider that train and test data are the preprocessed versions."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_YsFz6JtFci",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train = X_train_sel\n",
        "X_test = X_test_sel"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jX35YyX-lNZd",
        "colab_type": "text"
      },
      "source": [
        "# The ML model\n",
        "\n",
        "Consider we have a **training** database of $N$ entries of the form $(\\mathbf{x}^{(i)},y^{(i)})$, where $\\mathbf{x}\\in\\mathbb{R}^{D}$ is an observation  and $y \\in\\mathbb{R}$  is its target value. Our goal is to design a  function $f(\\mathbf{x}^*)$  able to estimate the associated target value for a new point $\\mathbf{x}^*$ as follows\n",
        "\n",
        "\\begin{align}\n",
        "y^* \\approx f(\\mathbf{x}^*)\n",
        "\\end{align}\n",
        "\n",
        "Note that in the above equation we are using the $\\approx$ because we consider that the estimation is not goint to be perfect, i.e.,  the designed function will have an estimation error. As we will see later, a good design will try to minimize this error."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2QWJnRQ_lSaT",
        "colab_type": "text"
      },
      "source": [
        "## My first estimation model:  K-NN (Nearest Neighbours) \n",
        "\n",
        "K-NN is a *non-parametric* method, since there are not parameters to be learned. To estimate the output a new sample $\\mathbf{x}^*$, you only have to:\n",
        "* Select the value of K.\n",
        "* Search, among the training data, the K nearest neighbours of $\\mathbf{x}^*$\n",
        "* Compute the estimated target  $\\mathbf{x}^*$ as the average value of the targets associated to the K-nearest neighbours of $x^*$. \n",
        "That is,\n",
        "\n",
        "\\begin{align}\n",
        "f(\\mathbf{x}^*) =\\frac{1}{K}\\sum_{k\\in \\mathcal{S}_{K}(\\mathbf{x}^*)} y^{(k)},\n",
        "\\end{align}\n",
        "where $\\mathcal{S}_{K}$ is the set of the **$K$ training points that are closest to $\\mathbf{x}^*$ according to a given distance metric $d(\\mathbf{x}^{(i)},\\mathbf{x}^*)$**. For instance, in real spaces the most common choice is the **euclidean distance**:\n",
        "\n",
        "\\begin{align}\n",
        "d(\\mathbf{x}^{(i)},\\mathbf{x}^*) = \\left|\\left|\\mathbf{x}^{(i)}-\\mathbf{x}^*\\right|\\right|^2\n",
        "\\end{align}\n",
        "\n",
        "\n",
        "**Some comments on K-NN**\n",
        "\n",
        "1. There are many variants of the above expression. For instance, we can **weight differently the regression values according to the distance**. In this way, the closest points has a bigger influence in the regression value:\n",
        "\n",
        "\\begin{align}\n",
        "f(\\mathbf{x}^*) =\\frac{1}{K}\\sum_{k\\in \\mathcal{S}_{K}(\\mathbf{x}^*)}  \\frac{d(\\mathbf{x}^{(k)},\\mathbf{x}^*)}{\\sum_{q\\in \\mathcal{S}_{K}} d(\\mathbf{x}^{(q)},\\mathbf{x}^*) } y^{(k)}\n",
        "\\end{align}\n",
        "\n",
        "\n",
        "2. The K-NN complexity is determined by the [**search of the set of closest neighbors**](https://en.wikipedia.org/wiki/Nearest_neighbor_search). This complexity grows as $\\mathcal{O}(DN)$, where $D$ is the dimension and $N$ is the number of points. Researches in computer science have developed many approximate lower-complexity algorithms to perform this task in high dimension spaces. The most common ones are [**k-d trees**](https://en.wikipedia.org/wiki/K-d_tree) and [**Local Sensitive Hashing**](https://en.wikipedia.org/wiki/Locality-sensitive_hashing).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5WDIslnMB7Z",
        "colab_type": "text"
      },
      "source": [
        "## Model evaluation\n",
        "\n",
        "In regression problems, it is quit common using the Mean Square Error (MSE) as performance measurement of our model. In fact, we can consider the following two metrics to diagnose how well our model is performing:\n",
        "\n",
        "- **Training MSE**: Considering $f({\\bf x}^*)$ is the regressor output for the data ${\\bf x}^*$, the MSE over the training data is defined as:\n",
        "$$MSE_{train} = \\frac{1}{N} \\sum_{i=1}^{N} \\left(y^{(i)}-f({\\bf x}^{(i)})\\right)^2$$\n",
        "\n",
        "- **Test MSE**: evaluating the same measurement over the test data, we have that\n",
        "\\begin{align}\n",
        "MSE_{test} =  \\frac{1}{N_{test}}\\sum_{i=1}^{N_{test}} \\left(y^{(i)}-f({\\bf x}^{(i)})\\right)^2\n",
        "\\end{align}\n",
        "\n",
        "Note that we are interested in evaluating how well our data **generalizes to data we have never seen!** This is the real point of Machine Learning! Learn from data to adapt! To emulate this effect, we have a test-database **which is never used for model training!** and is only used to evaluate generalization. So, the test error informs about the generalization capability of our regressor.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nMg9hJZFlmLd",
        "colab_type": "text"
      },
      "source": [
        "### Exercise 2 \n",
        "Implement your own K-NN regressor (with $K=5$) to predict the house sale-prize. Obtain the predictions over the test data set and evaluate the regression performance computing the MSE over the test data.\n",
        "\n",
        "You can use the function [euclidean_distances](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.pairwise.euclidean_distances.html) to compute the euclidean distances of each test point to the training data."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EItEZlu1qdaK",
        "colab_type": "text"
      },
      "source": [
        "#### SOLUTION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "19K5SWtXk7fI",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# <SOL>\n",
        "# </SOL>\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aKzg4gkkLHfv",
        "colab_type": "text"
      },
      "source": [
        "### Exercise 3\n",
        "Use the sklearn [K-NN regression](https://scikit-learn.org/stable/modules/generated/sklearn.neighbors.KNeighborsRegressor.html) function to train and obtain the test predictions and check that you obtain the same MSE. Of couse, set $K=5$."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzkNugFm_AjK",
        "colab_type": "text"
      },
      "source": [
        "#### SOLUTION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QCGBLDjTk7c6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# <SOL>\n",
        "# </SOL>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AEpQORM-7eh_",
        "colab_type": "text"
      },
      "source": [
        "## Analyzing prediction curve\n",
        "\n",
        "The K-NN model is learning a function to map from an observation to a predicted value (an estimation of the house sale-prize). In case, we are working with unidimensional observations, we can plot this prediction function. \n",
        "\n",
        "Next cell analizes the prediction function ${y^*}=f({\\bf x}^*)$ when we select a single input variable (feature 12: LSTAT, after the previous feature selection, this is feature #2). "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HJ1MSVlkrSZA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Analize the regresion function\n",
        "\n",
        "# We select a single feature \n",
        "\n",
        "Id_feat = 2 #LSTAT\n",
        "\n",
        "# We train and evaluate the model with the unidimensional observation\n",
        "neigh.fit(X_train[:,Id_feat][:,np.newaxis], Y_train) \n",
        "f_test=neigh.predict(X_test[:,Id_feat][:,np.newaxis])\n",
        "MSE_KNN = np.mean((Y_test-f_test)**2)\n",
        "print('MSE using a single input feature:')\n",
        "print(MSE_KNN)\n",
        "\n",
        "# Plot the training data\n",
        "plt.figure()\n",
        "plt.plot(X_train[:,Id_feat][:,np.newaxis], Y_train, '.')\n",
        "# Plot the regression function\n",
        "## Define a range of equispaced values of x\n",
        "X_plot = np.arange (-2,4,0.01)\n",
        "## Obtain the output of the prediction funcion for these values\n",
        "f_plot=neigh.predict(X_plot[:,np.newaxis])\n",
        "## Plot them\n",
        "plt.plot(X_plot,f_plot, 'r')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CaDByUT_pn0A",
        "colab_type": "text"
      },
      "source": [
        "# Analyzing the influece of parameter K\n",
        "\n",
        "For now, we have prefixed a value for the K parameter ($K=5$) and we have analyzed the K-NN for this value. But, can another K value provide a different performance? \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bT0Qqyy7-gq7",
        "colab_type": "text"
      },
      "source": [
        "### Exercise 4\n",
        "\n",
        "Analyze the performance of the K-NN regressor for a range of values of K from 1 to 40. Analyze this performance over the train and test MSE."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LP_ntG-e_CvU",
        "colab_type": "text"
      },
      "source": [
        "#### SOLUTION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChyMlxGTk7ay",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# <SOL>\n",
        "# </SOL>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N6yY9bG_qmSm",
        "colab_type": "text"
      },
      "source": [
        "How can I select the optimum K value?\n",
        "Can I use the training MSE? \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kbJamNJwq3BZ",
        "colab_type": "text"
      },
      "source": [
        "# Parameter selection: Cross Validation\n",
        "\n",
        "In order to fit $K$, we will split the training set once more, to create the **validation set**:\n",
        "\n",
        "<img src=\"http://www.tsc.uc3m.es/~vanessa/Figs_notebooks/ML/Regression/validation_set.png\" width=\"70%\" > \n",
        "\n",
        "\n",
        "The validation set will be used to choose the best $K$ among the same model with different $K$ values, all trained using the training set. This process is known as **cross validation**.\n",
        "\n",
        "-----\n",
        "\n",
        "**Cross Validation Steps**:\n",
        "\n",
        "1) Select a ML model, for instance a K-NN regressor, and construct the normalized feature matrix for both the training, validation and test sets. \n",
        "\n",
        "2) Set a grid of $K$ values to test (i.e. $1, 2,3,4,5,6,7,8,....20$)\n",
        "\n",
        "3) Train the ML model for each $K$ value **using only the training set**. For each, $K$ value we get a model $f_{K}()$.\n",
        "\n",
        "4) Compute the validation MSE (or other performance measurement) for each model:\n",
        "\n",
        "$$MSE_{val}(K) = \\frac{1}{N_{val}} \\sum_{i=1}^{N_{val}} \\left(y^{(i)}-{f_K}({\\bf x}^{(i)})\\right)^2$$\n",
        "\n",
        "Recall, both the validation and test sets are normalized using the train set statistics.\n",
        "\n",
        "5) Select $K^*$ that minimizes the validation error \n",
        "\n",
        "6) Retrain the model for $K^*$ using **both the training and validation sets**. So, you get a new model $f_{K^*}()$\n",
        "\n",
        "7) Compute the final test MSE:\n",
        "\n",
        "$$MSE_{test}(K^*) = \\frac{1}{N_{test}} \\sum_{i=1}^{N_{test}} \\left(y^{(i)}-{f_{K^*}}({\\bf x}^{(i)})\\right)^2$$\n",
        "\n",
        "--- \n",
        "\n",
        "Unless the dataset is large enough (and so does the validation set), the robustnetss of the above procedure is improved by averaging the validation MSE (step 4) over multiple random partitions of the validation/training set (**Repeated random sub-sampling validation**) or splits the training set in K-folds and averages the results using one of the folds as validation set at a time (**K-fold cross validation**). See the [Wikipedia entry](https://en.wikipedia.org/wiki/Cross-validation) about CV for more details. \n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "83v1Gthe2gS3",
        "colab_type": "text"
      },
      "source": [
        "### Exercise 5.1\n",
        "\n",
        "Now, apply a 10 fold cross validation (CV) process to select the optimum value of K from a range of values from 1 to 40. You can use the [GridSearchCV( )](http://scikit-learn.org/stable/modules/generated/sklearn.grid_search.GridSearchCV.html) function to implement it. \n",
        "\n",
        "Note: Define the scoring parameter of GridSerch as 'neg_mean_squared_error' (to minimize the MSE during the CV)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mmRw22Yb9ZdT",
        "colab_type": "text"
      },
      "source": [
        "#### SOLUTION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vMhUUIHp9Xmf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# <SOL>\n",
        "# </SOL>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRMpVjR_2rM1",
        "colab_type": "text"
      },
      "source": [
        "### Analyzing CV output\n",
        "After running the CV process, the estimator object contains the information of the CV process (next cell explore the parameter \".grid\\_scores\\_\" to obtain this information)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9iXui8O0tsLK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Printing results\n",
        "print(\"Cross validation results:\")\n",
        "\n",
        "paramsFolds = clf.cv_results_['params']\n",
        "meanScoreFolds = clf.cv_results_['mean_test_score']\n",
        "stdScoreFolds = clf.cv_results_['std_test_score']\n",
        "\n",
        "for i, K in enumerate(rang_K):\n",
        "    params = paramsFolds[i]\n",
        "    mean_score = meanScoreFolds[i]\n",
        "    std_score = stdScoreFolds[i]\n",
        "    print(\"For K = %d, validation accuracy is %2.2f (+/-%1.3f)%%\" \n",
        "          % (params['n_neighbors'], 100*mean_score, 100*std_score / 2))\n",
        "\n",
        "# Selecting validation error (mean values)\n",
        "vect_val=meanScoreFolds\n",
        "\n",
        "# Ploting results\n",
        "plt.figure()\n",
        "plt.plot(rang_K,vect_val,'g', label='Validation accuracy')\n",
        "plt.legend()\n",
        "plt.xlabel('K value')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Evolution of K-NN accuracy (including validation result)')\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t_35yn_B227F",
        "colab_type": "text"
      },
      "source": [
        "## Exercise 5.2\n",
        "\n",
        "Examine the fields \".best\\_estimator\\_\" and \".best\\_params\\_\" of the estimator generated by the CV process:\n",
        "* \".best\\_estimator\\_\" contains  the final estimator trained with this select value.\n",
        "* \".best\\_params\\_\" is a dictionary with the selected parameters. In our example, \"best\\_params\\_['n\\_neighbors']\" would provide the selected value of K.\n",
        "\n",
        "Save the selected value of K in variable denoted \"K_opt\" and compute the test error of the final estimator."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K81z8ni5UUDx",
        "colab_type": "text"
      },
      "source": [
        "### SOLUTION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HbObZbomtsAv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# <SOL>\n",
        "# </SOL>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0-0nTSS22_x5",
        "colab_type": "text"
      },
      "source": [
        "Note that you can also compute the KNN output directly over the estimator object returned by the CV process:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "do1dv4oLtr1X",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "f_test = clf.predict(X_test)\n",
        "MSE_test = np.mean((Y_test-f_test)**2) \n",
        "print(\"The test accuracy is %2.2f\" %(MSE_test ))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "V_SmeuQK3HRv",
        "colab_type": "text"
      },
      "source": [
        "## Exercise 5.3  Advanced work\n",
        "\n",
        "Complete the following code to implement the CV process without using GridSearchCV().\n",
        "\n",
        "You can use the sklearn [Kfold](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html#sklearn.model_selection.KFold) function to generate the different validation folds."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q1nZ932ZW8o4",
        "colab_type": "text"
      },
      "source": [
        "### SOLUTION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vrXdbuWw3MOr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# <SOL>\n",
        "# </SOL>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MiTUb8ub3YWq",
        "colab_type": "text"
      },
      "source": [
        "# Performance evaluation\n",
        "\n",
        "The last step of the ML pipeline is the performance evaluation. So far, we have used the MSE as performance measurement, but we can find other metrics. In fact, sklearn models include a wide variety of [metrics for regression problems](https://scikit-learn.org/stable/modules/model_evaluation.html#regression-metrics). The most frequent are:\n",
        "* Mean Square Error (MSE)\n",
        "$$MSE = \\frac{1}{N} \\sum_{i=1}^{N} \\left(y^{(i)}-f({\\bf x}^{(i)})\\right)^2$$\n",
        "\n",
        "* Mean Absolute Error (MAE)\n",
        "$$MAE = \\frac{1}{N} \\sum_{i=1}^{N} \\left|y^{(i)}-f({\\bf x}^{(i)})\\right|$$\n",
        "\n",
        "* R2-score (R2)\n",
        "$$R2 =  1- \\frac{ \\sum_{i=1}^{N} \\left(y^{(i)}-f({\\bf x}^{(i)})\\right)^2}{\\sum_{i=1}^{N} \\left(y^{(i)}-\\bar{y}\\right)^2}$$\n",
        "\n",
        "where $\\bar{y} = \\frac{1}{N} \\sum_{i=1}^{N} y^{(i)}$ (mean value of the targets).\n",
        "\n",
        "The R2 score is the default measurement used by sklearn regression methods. It has the advantage of being more interpretable, since the best possible score is 1.0, a constant model that always predicts the expected value of y would get a score of 0.0, and models working even worse that this constant model can provide negative values. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RiddH6vDycMA",
        "colab_type": "text"
      },
      "source": [
        "## Exercise 6\n",
        "\n",
        "Use the [regression metrics](https://scikit-learn.org/stable/modules/classes.html#regression-metrics) of sklearn to obtain the MSE, MAE and R2  scorings of the K-NN regressor. \n",
        "\n",
        "Note that you can also use the .scoring() method of the K-NN model to evaluate the regressor performance, but this method only returns the R2-score. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Bjhp4bEEmOj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# <SOL>\n",
        "# </SOL>"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}