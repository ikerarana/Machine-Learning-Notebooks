{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "7-jkWR1LhNMh"
   },
   "source": [
    "# Week 4: Homework 1 \n",
    "\n",
    "----------------------------------------------------\n",
    "Machine Learning                      \n",
    "\n",
    "Year 2019/2020\n",
    "\n",
    "*Vanessa Gómez Verdejo vanessa@tsc.uc3m.es* \n",
    "\n",
    "----------------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">\n",
    "NOTEBOOK GRADE: 9 <br>\n",
    "EXAM GRADE: 7.5  <br> \n",
    "OVERALL GRADE: 8.25\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xfXCi3qshchx"
   },
   "source": [
    "The goal of this practice is to analyze the performance of different estimators on the Diabetes problem and we will analyze, by means of different approximations, which input features are more relevant to solve this problem. \n",
    "\n",
    "Note that previous week we already worked on Diabetes database, but we only used one of the input variable (BMI) to construct the regression model; in this practice we will use all the input features jointly.\n",
    "\n",
    "To solve these notebook, complete the following sections implementing the solution that you consider most appropriate and showing the results that you find most interesting. For the evaluation of this notebook,  we will take into account the methodology used, the solution adopted, the presentation of the results and the conclusions obtained at the light of the results.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nCXU4m2YmB2y"
   },
   "source": [
    "## 1. Data loading and preprocessing\n",
    "\n",
    "Following the ML pipeline, start loading the data, creating the partitions that you consider necessary and carrying out the preprocessing that yu need.\n",
    "\n",
    "Keep in mind that there is no single valid solution, and different reasons can lead you to make different data partitions or apply different normalizations. So **please justify the steps you are taking**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-7fqumVhhJAl"
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_diabetes\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "## Cogemos en la X las features y en Y las salidas, nos guardamos los feature_names por si en algún momento\n",
    "## queremos plotear algo con los títulos de las columnas de X.\n",
    "diabetes = load_diabetes()\n",
    "X = diabetes.data\n",
    "Y = diabetes.target\n",
    "feature_names = diabetes.feature_names\n",
    "\n",
    "# Hacemos la partición, el test_size tiene que ser mayor que el train para que tengamos datos con los que\n",
    "# testear que con los que entrenar. Así, al tener más para testear será más difícil que se memorice los datos de entrenamiento\n",
    "# porque habrá más para testear.\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.4, random_state=0)\n",
    "\n",
    "\n",
    "# Data normalization: normalizamos tanto train como test para poder trabajar con ellos.\n",
    "transformer = StandardScaler().fit(X_train)  \n",
    "X_train_norm = transformer.transform(X_train) \n",
    "X_test_norm =  transformer.transform(X_test)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fe7_QAXDmatm"
   },
   "source": [
    "## 2. Performance evaluation \n",
    "\n",
    "Now, analyze the performance of different estimators to predict the diabetes progression from all the available features. \n",
    "\n",
    "As possible estimators to be included in this study, we will consider those studied so far: K-NN, linear regressor, polynomial regressor and their regularized versions. Please, in case these methods have any free hiperparameter, **clearly justify** the selection of their optimal values.\n",
    "\n",
    "As you know, for the performance evaluation, we have seen several metrics. So, here, you can use one or several of them. But, regardless of the used metric, don't forget to include a final analysis comparing the performance of different methods and trying to justify the obtained results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "R1212w6mf4Wd"
   },
   "source": [
    "## 2.1 KNN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "EnOY0K4Fo62S",
    "outputId": "0b2054da-123d-436c-d7da-1955f73e3f58"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score of KNN: 0.41\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "#Definimos el KNN\n",
    "knn2 = KNeighborsRegressor()\n",
    "#Para encontrar la K óptima, usamos la función GridSearchCV para que nos compare las posibles K y nos devuelva el mejro parámetro\n",
    "#Vamos de 1 a 40 para asegurarnos de tener una gran variabilidad.\n",
    "rang_K = np.arange(1,40,1)\n",
    "parameters = {\"n_neighbors\": rang_K}\n",
    "#definimos el GSCV con el mean_squared negativo porque nos da una visión (si quisieramos plotear) más entendible de encontrar el valor óptimo puesto que será el más alto\n",
    "#los K-folds realmente hacen referencia a las particiones que vamos a hacer del training y validation para hacer cross-validation.\n",
    "#su valor no puede ser igual al número\n",
    "clf = GridSearchCV(knn2, parameters, cv=10,scoring=\"neg_mean_squared_error\",iid=False)\n",
    "\n",
    "\n",
    "clf.fit(X_train, Y_train)\n",
    "clf.best_estimator_\n",
    "y_pred = clf.predict(X_test)\n",
    "MSE_test = np.mean((Y_test-y_pred)**2)\n",
    "\n",
    "r2_score_KNN = r2_score(Y_test, y_pred)\n",
    "\n",
    "print(\"R2 score of KNN: %.2f\" % r2_score_KNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oSEA2vpIdQCp"
   },
   "source": [
    " We define KNN. To find the OptK we use GridSearchCV to do a cross-validation with a 40 possibles values of K.\n",
    "\n",
    "Then, we use the best K to fit our training data and predic our score, giving us a 0.41 score."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KsfaNjuep2A4"
   },
   "source": [
    "## 2.2 Linear Regression\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "zuBy3hbQqCc8",
    "outputId": "92a6f31a-b15d-4080-dd6b-f37a188990b8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "R2 score of LR without regularization: 0.39\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "lr = LinearRegression().fit(X_train, Y_train)\n",
    "\n",
    "y_pred_LR = lr.predict(X_test)\n",
    "\n",
    "r2_score_LR = r2_score(Y_test, y_pred_LR)\n",
    "print('R2 score of LR without regularization: %.2f' % r2_score_LR)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N0R1O_Rid2gP"
   },
   "source": [
    "On the linear regression method there isn't any value to estimate or select as optimal. So we define a linear regression, fit the trainer and try to predict our final score. As we can see, is worse than the another method (0.39)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Syk4t3T4mTw4"
   },
   "source": [
    "### 2.2.2 Linear Regression with LASSO (L1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 255
    },
    "colab_type": "code",
    "id": "c-Q7B4AqmXFU",
    "outputId": "d7a3ce22-619e-4c3f-dc4f-72089a745ac7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha: \n",
      " 0.07106306306306306\n",
      "R2 score of Lasso: 0.40\n",
      "number of features used: 8\n",
      "number of features erased 2\n",
      "Features used:\n",
      "age\n",
      "sex\n",
      "bmi\n",
      "bp\n",
      "s1\n",
      "s3\n",
      "s5\n",
      "s6\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LassoCV\n",
    "\n",
    "alphas = np.linspace(0.001,10,1000)\n",
    "\n",
    "lasso = LassoCV(cv=5,alphas=alphas, random_state=0)\n",
    "\n",
    "# Train the model using the training sets\n",
    "lasso.fit(X_train, Y_train)\n",
    "\n",
    "# Make predictions using the testing set\n",
    "y_pred_Lasso = lasso.predict(X_test)\n",
    "\n",
    "# The coefficients\n",
    "print('Best alpha: \\n', lasso.alpha_)\n",
    "\n",
    "r2_score_lasso = r2_score(Y_test, y_pred_Lasso) \n",
    "# Explained variance score: 1 is perfect prediction\n",
    "print('R2 score of Lasso: %.2f' % r2_score_lasso)\n",
    "\n",
    "coeff_used = np.sum(lasso.coef_!=0)\n",
    "coeff_no = X_train.shape[1]-coeff_used\n",
    "print (\"number of features used:\", coeff_used)\n",
    "print (\"number of features erased\", coeff_no )\n",
    "\n",
    "print(\"Features used:\")\n",
    "for index,element in enumerate(lasso.coef_):\n",
    "  if element != 0:\n",
    "    print(feature_names[index])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wF0Q2-aEeC56"
   },
   "source": [
    "In this case, we improve the LR adding a regularization method. In this case we use L1 (LASSO) that can erase features not used. On this method we have to decide the value of alpha. In order to do this, we crossvalidate using 5 cv and an array of possible alphas. As it is shown, the Lasso regularization erase 2 features and only uses 8 of them (age, sex, bmi, bp, s1, s3, s5 and s6). Therefore, the CV method gives us the best alpha asociated with the best estimator."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "YUmOz_zpcaTp"
   },
   "source": [
    "### 2.2.3 Linear Regression Ridge (L2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 238
    },
    "colab_type": "code",
    "id": "XbUPLlj0cfTE",
    "outputId": "1494bd21-ea62-4a7e-98f6-db5009951e92"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best alpha: \n",
      " 0.04103603603603604\n",
      "R2 score of Ridge: 0.39\n",
      "Score: 552.6256556415602 name: s5\n",
      "Score: 537.7535013869982 name: bmi\n",
      "Score: 335.2246727323783 name: bp\n",
      "Score: -221.49209594163028 name: s3\n",
      "Score: -211.97264921910033 name: sex\n",
      "Score: -120.45496259967037 name: s1\n",
      "Score: -104.22225624532072 name: s2\n",
      "Score: 68.941035988502 name: s4\n",
      "Score: -60.728034253074384 name: age\n",
      "Score: 56.12123157030079 name: s6\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import RidgeCV\n",
    "\n",
    "maybe_alphas = np.linspace(0.001,10,1000)\n",
    "\n",
    "ridge = RidgeCV(cv=5,alphas=maybe_alphas)\n",
    "\n",
    "ridge_model = ridge.fit(X_train, Y_train)\n",
    "\n",
    "y_pred_ridge = ridge.predict(X_test)\n",
    "\n",
    "print('Best alpha: \\n', ridge_model.alpha_)\n",
    "\n",
    "r2_score_ridge = r2_score(Y_test, y_pred_ridge) \n",
    "\n",
    "print('R2 score of Ridge: %.2f' % r2_score_ridge)\n",
    "\n",
    "ridge_model.coef_\n",
    "\n",
    "for elements in (-abs(ridge_model.coef_)).argsort():\n",
    "  print(\"Score: \"+str(ridge_model.coef_[elements])+ \" name: \"+str(feature_names[elements]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "X-LE-MYglbBL"
   },
   "source": [
    "The linear regresssion ridge uses the L2 regularization method. In that case, our model doesn't erase features but weight them returning us the importances of them. As we can see, the most important features are: s5 and bmi. In this model we also have chosen the best alpha using cross-validation of 5 folders."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "i_Frb0Ro7tZZ"
   },
   "source": [
    "### 2.2.3.1 Elastic-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "j5gkVFwT7wK5",
    "outputId": "30c8a9a0-eb61-4a59-ebd1-ef50800d3d99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.011009009009009009\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import ElasticNetCV\n",
    "\n",
    "\n",
    "\n",
    "regr = ElasticNetCV(cv=5, alphas=alphas, random_state=0, l1_ratio=np.arange(0.01, 1.0, 0.01), n_jobs=-1)\n",
    "\n",
    "regr.fit(X_train, Y_train)\n",
    "\n",
    "print(regr.alpha_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "colab_type": "code",
    "id": "LlMQ2Ssb-pyJ",
    "outputId": "b1478ed9-b2a0-4f66-cddf-dc5295dd522e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best score:\n",
      "0.40037021491805924\n",
      "Score: 527.4045620217665 name: s5\n",
      "Score: 524.7545253706722 name: bmi\n",
      "Score: 323.0975306972214 name: bp\n",
      "Score: -229.5950232633456 name: s3\n",
      "Score: -197.32128484501166 name: sex\n",
      "Score: -110.63067730648952 name: s2\n",
      "Score: -87.80161106637735 name: s1\n",
      "Score: 60.4273282243968 name: s4\n",
      "Score: 58.362875929700046 name: s6\n",
      "Score: -49.98153777288919 name: age\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"Best score:\")\n",
    "print(regr.score(X_test,Y_test))\n",
    "for elements in (-abs(regr.coef_)).argsort():\n",
    "  print(\"Score: \"+str(regr.coef_[elements])+ \" name: \"+str(feature_names[elements]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "8quUXoDp8eHa"
   },
   "source": [
    "Using the Elastic-Net we can observe that we got aproximatelly the same results as using L2 regularization method. In fact, the most valuables features are just the same as before with nearly values.\n",
    "On this model we have usen also CV to validate both alpha and l1_ratio. The l1_ratio is the ratio between the power given to L1 or L2 regularization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jTm9x5Eth6r5"
   },
   "source": [
    "### 2.2.4 Poly Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "Mvw9Jm8Mh-A6",
    "outputId": "6d0c937e-e037-4f6c-b022-146331c976b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: \n",
      " {'poly__degree': 1}\n",
      "R2 score of normal: 0.39\n"
     ]
    }
   ],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "\n",
    "steps = [('poly', PolynomialFeatures()), ('normalize', StandardScaler()), ('clf',LinearRegression())]\n",
    "pipe = Pipeline(steps)\n",
    "\n",
    "pipe.set_params(poly__include_bias=False)\n",
    "pipe.set_params(clf__fit_intercept=True)\n",
    "nfold=5\n",
    "\n",
    "param_grid = dict(poly__degree=[1, 2, 3, 4])\n",
    "pipe_grid_search = GridSearchCV(pipe, param_grid=param_grid, cv=nfold, iid= False, scoring='r2')\n",
    "pipe_grid_search.fit(X_train, Y_train)\n",
    "\n",
    "y_pred_pol = pipe_grid_search.predict(X_test)\n",
    "\n",
    "print('Best params: \\n', pipe_grid_search.best_params_)\n",
    "\n",
    "r2_score_pol = r2_score(Y_test, y_pred_pol) \n",
    "\n",
    "print('R2 score of normal: %.2f' % r2_score_pol)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Vd1jYVH58i9o"
   },
   "source": [
    "On polynomial regression we use pipeline to go step by step in order to cross-validate all the params needed. First of all we have to cross-validate the best degree to perform our data. AS it is shown the best degree is the 1, so the best result is using a linear method because larger degrees may cause overfit."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5kUim7cXIpKC"
   },
   "source": [
    "### 2.2.5 Poly Regression Lasso\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "colab_type": "code",
    "id": "J6h1NRKPIxTg",
    "outputId": "e1698dee-1e24-4c1f-c891-c4e605c473ca"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 275.9676635515789, tolerance: 135.39487311320755\n",
      "  positive)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2539.166798708975, tolerance: 135.71348820754716\n",
      "  positive)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 4976.103937343229, tolerance: 137.51342783018868\n",
      "  positive)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2806.507196128732, tolerance: 134.40579386792456\n",
      "  positive)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3793.7460284451663, tolerance: 131.87301886792457\n",
      "  positive)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2080.8529833066495, tolerance: 135.39487311320755\n",
      "  positive)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1424.6815943701113, tolerance: 135.71348820754716\n",
      "  positive)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2414.201552140861, tolerance: 137.51342783018868\n",
      "  positive)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1534.812498158928, tolerance: 134.40579386792456\n",
      "  positive)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 1280.5680618081242, tolerance: 131.87301886792457\n",
      "  positive)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: \n",
      " {'clf__alpha': 5, 'poly__degree': 2}\n",
      "R2 score of lasso: 0.40\n"
     ]
    }
   ],
   "source": [
    "from sklearn import linear_model\n",
    "\n",
    "steps = [('poly', PolynomialFeatures()), ('normalize', StandardScaler()), ('clf',linear_model.Lasso(max_iter=5000))]\n",
    "pipe = Pipeline(steps)\n",
    "\n",
    "pipe.set_params(poly__include_bias=False)\n",
    "pipe.set_params(clf__fit_intercept=True)\n",
    "nfold=5\n",
    "\n",
    "param_grid = dict(poly__degree=[1, 2, 3, 4], clf__alpha=[0.1, 1, 5, 10, 50, 100])\n",
    "pipe_grid_search = GridSearchCV(pipe, param_grid=param_grid, cv=nfold, iid= False, scoring='r2')\n",
    "pipe_grid_search.fit(X_train, Y_train)\n",
    "\n",
    "y_pred_lass = pipe_grid_search.predict(X_test)\n",
    "\n",
    "print('Best params: \\n', pipe_grid_search.best_params_)\n",
    "\n",
    "r2_score_pollass = r2_score(Y_test, y_pred_lass) \n",
    "\n",
    "print('R2 score of lasso: %.2f' % r2_score_pollass)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "EdbDbPmu9jTv"
   },
   "source": [
    "Using Lasso we need to cross-validate alphas and poly degree. In that case it gives a warning that our object not converge, so its not the best method to perform. Otherwise, it gives us that the best degree is 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hDrsAQUxIxbH"
   },
   "source": [
    "### 2.2.6 Poly Regression Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 68
    },
    "colab_type": "code",
    "id": "1MK4inmJI03o",
    "outputId": "d2f43921-aa22-4f69-a7ec-4e3d358c0c88"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: \n",
      " {'clf__alpha': 10, 'poly__degree': 1}\n",
      "R2 score of Ridge: 0.39\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge\n",
    "steps = [('poly', PolynomialFeatures()), ('normalize', StandardScaler()), ('clf',Ridge())]\n",
    "pipe = Pipeline(steps)\n",
    "\n",
    "pipe.set_params(poly__include_bias=False)\n",
    "pipe.set_params(clf__fit_intercept=True)\n",
    "nfold=5\n",
    "\n",
    "param_grid = dict(poly__degree=[1, 2, 3, 4], clf__alpha=[0.1, 1, 5, 10, 50, 100])\n",
    "pipe_grid_search = GridSearchCV(pipe, param_grid=param_grid, cv=nfold, iid= False, scoring='neg_mean_squared_error')\n",
    "pipe_grid_search.fit(X_train, Y_train)\n",
    "\n",
    "y_pred_ridge = pipe_grid_search.predict(X_test)\n",
    "\n",
    "print('Best params: \\n', pipe_grid_search.best_params_)\n",
    "\n",
    "r2_score_polridge = r2_score(Y_test, y_pred_ridge) \n",
    "\n",
    "print('R2 score of Ridge: %.2f' % r2_score_polridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5-iagcLc97kG"
   },
   "source": [
    "Meanwhile, using Ridge it converges quickly and gives us the same result as not using regularization. so we return to our conclusion that the best degree is 1, so the best model is the linear one."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vuiUv33tI1Bn"
   },
   "source": [
    "### 2.2.7 Poly Regression Elastic-Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 428
    },
    "colab_type": "code",
    "id": "rpvctcMFI6T2",
    "outputId": "f4d12277-dc60-49db-cbec-006839ecda13"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 11047.835657251067, tolerance: 135.39487311320755\n",
      "  positive)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 5834.942792104994, tolerance: 135.71348820754716\n",
      "  positive)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 3497.156841904565, tolerance: 137.51342783018868\n",
      "  positive)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 6497.717578221607, tolerance: 134.40579386792456\n",
      "  positive)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 2758.8077728284115, tolerance: 131.87301886792457\n",
      "  positive)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 42260.64893442553, tolerance: 135.39487311320755\n",
      "  positive)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 33581.254716366995, tolerance: 135.71348820754716\n",
      "  positive)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 41409.62652957086, tolerance: 137.51342783018868\n",
      "  positive)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 51792.08368563023, tolerance: 134.40579386792456\n",
      "  positive)\n",
      "/usr/local/lib/python3.6/dist-packages/sklearn/linear_model/coordinate_descent.py:475: ConvergenceWarning: Objective did not converge. You might want to increase the number of iterations. Duality gap: 46294.26693124179, tolerance: 131.87301886792457\n",
      "  positive)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best params: \n",
      " {'clf__alpha': 0.1, 'poly__degree': 1}\n",
      "R2 score of Elastic-net: 0.39\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "steps = [('poly', PolynomialFeatures()), ('normalize', StandardScaler()), ('clf',linear_model.ElasticNet())]\n",
    "pipe = Pipeline(steps)\n",
    "\n",
    "pipe.set_params(poly__include_bias=False)\n",
    "pipe.set_params(clf__fit_intercept=True)\n",
    "nfold=5\n",
    "\n",
    "param_grid = dict(poly__degree=[1, 2, 3, 4], clf__alpha=[0.1, 1, 5, 10, 50, 100])\n",
    "pipe_grid_search = GridSearchCV(pipe, param_grid=param_grid, cv=nfold, iid= False, scoring='neg_mean_squared_error')\n",
    "pipe_grid_search.fit(X_train, Y_train)\n",
    "\n",
    "y_pred_ridge = pipe_grid_search.predict(X_test)\n",
    "print('Best params: \\n', pipe_grid_search.best_params_)\n",
    "\n",
    "r2_score_en = r2_score(Y_test, y_pred_en) \n",
    "\n",
    "print('R2 score of Elastic-net: %.2f' % r2_score_en)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">\n",
    "Be careful, the selected value of alpha is in the extreme of the explored range. If you extend your range to 0.01, 000.1, ...., it's possible that you get better performance. Besides, elatic net has two parameters and you only explore one. I think that the other parameter is set by deafult to the lasso case...\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "93SbtnmG-JjW"
   },
   "source": [
    "With elastic-net we are using a mixture between L1 and L2 and it returns us the same warning as using L1. It return us the same result as Ridge, that the poly_degree is 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">\n",
    "You can adjust the tolerance parameter of the lasso and elastic net methods to avoid these warnings and improve the method performance <br>\n",
    "I miss any conclusions and/ or comparitve analysis of these results. Which approach is the best? Linear or no-linear methods?   <br>\n",
    "Polynomial ridge regression uses a degree of 1 and polynomial lasso regression uses a degree of 2. Any idea?\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "VHWjw1eAo_9V"
   },
   "source": [
    "## 3. Study of feature relevance and feature selection\n",
    "\n",
    "In this last section, using different criteria, you have to analyze the relevance of the input features. Thus, you will have to find a subset with the $D'$ most relevant features and, using this subset of features, analyze the final performance of a regressor (for the sake of simplicity, consider a linear ridge regressor as final regressor).\n",
    "\n",
    "To analyze the feature importances or relevances, you can use the following criteria:\n",
    "\n",
    "1. **Relevance ranking based on the validation error**: if there were $D$ input features, we could try to train $D$ regressors where each regressor uses one (and only one) different input feature. According to the final perfomance of each regressor (evaluated on a validation set or with a CV proccess), we could rank the features (the most relevant feature is the one providing the lowest error). Using this ranking, we can select the $D'$ most relevant features as the $D'$ top-ranked ones. Note that this scheme only analyzes the isolated relevance of each feature to predict the output; so, it is said that this approach is *univariate*.\n",
    "\n",
    " <img src=\"http://www.tsc.uc3m.es/~vanessa/Figs_notebooks/ML/Regression/FeatureRanking.png\" width=\"90%\" > \n",
    "\n",
    "\n",
    "2. **Greedy search based on the validation error**: approach (1) has the disadvantage of not taking into account relationships between features. For instance, method (1) would not realize that two features can be rendundant or that a feature, that is useless by itself, can be very useful combined with another feature. To overcome this drawback, we should have to analyze subsets of features; however, exploring all possible subsets  is usually computationally unflexible (there are $2^D$ combinations!!!!); so a greedy search (fordward or backward) is usually prefered:\n",
    "\n",
    "  2.1 *Fordward search*: It starts with an empty set and, iteratively, adds new features according to a relevance criterion (in this case, minimum validation MSE).\n",
    "\n",
    "  <img src=\"http://www.tsc.uc3m.es/~vanessa/Figs_notebooks/ML/Regression/Fig_Forward_search.png\" width=\"48%\" > \n",
    "\n",
    "  2.2 *Backward search*: It starts considering all the features and, iteratively, removes features according to a relevance criterion (in this case, minimum validation MSE).\n",
    "\n",
    "  <img src=\"http://www.tsc.uc3m.es/~vanessa/Figs_notebooks/ML/Regression/Fig_Backward_search.png\" width=\"48%\" > \n",
    "\n",
    "3. **Ridge linear regression with a prunning**: We know that the L2 regularization limits the magnitude of the weight vector to avoid overfitting problems, but these weigths do not become to null. However, in a linear model, *the weight magnitude can be an indicative of the feature relevance* and, unlike approach (1), all features are analyzed at the same time (*multivariate approach*). Use the weigth magnitude to generate a ranking of features and, later, use this ranking to select the $D'$ most relevant features.\n",
    "\n",
    "4. **Lasso linear regression**: In this case, the L1 penalty allows us to directly eliminate some of the input features. Explore different values of the regularization parameter $\\lambda$ to get a sequence of selected feature sets (from a single feature to all features).\n",
    "\n",
    "\n",
    "5. **Elastic-net linear regression**: This last approach combines L1 and L2 regularizations, thus including the advantages of both methods. Varying adequately their regularization parameters, create a sequence of feature selection subsets. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "qMIaGE0kmSdt"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fe2uvnVP0-AZ"
   },
   "source": [
    "Final comments:\n",
    "\n",
    "* Due to part of the feature selection process involves selecting the optimum number of features, to avoid additional complexity (having to validate this number), you can analyze the different methods by exploring the curves of MSE vs. number of selected features ($D'$).\n",
    "\n",
    "* It is not necessary to apply all these methods to complete this notebook (you can choose, at least, three of them). In fact, the implementation of greedy search approaches require an advanced knowledge of Python; so take this into account when you design your notebook solution.\n",
    "\n",
    "* **Please, analyze in detail the different results, pointing out the advantages/disadvantages of each feature selection scheme**. Think about the behaviour of the different criteria in cases where a feature is irrelevant or there are redundant features. Additional experiments helping you to support any of your conclusions will be welcome."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vh_m15DSJhio"
   },
   "source": [
    "## 3.1 Ranking based on MSE\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "6O0729twJmux",
    "outputId": "1287c2ed-115c-444e-e762-73e7dc1ffb15"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features in order of relevancy based on MSE:\n",
      "[5047.38954437 5269.47848288 3786.39417761 4718.72343274 5117.16333904\n",
      " 5159.86391871 4734.69531437 4373.96974114 4148.06402569 4627.62285166]\n",
      "['bmi', 's5', 's4', 's6', 'bp', 's3', 'age', 's1', 's2', 'sex']\n",
      "R2 score using only the 4th most relevant features based on MSE: 0.37\n",
      "Versus\n",
      "R2 score using all the features: 0.39\n"
     ]
    }
   ],
   "source": [
    "MSE_ranking = []\n",
    "n = X.shape[1]\n",
    "maybe_alphas = np.linspace(0.001,10,1000)\n",
    "ridge = RidgeCV(alphas=maybe_alphas)\n",
    "for i in range(n):\n",
    "  \n",
    "  ridge_model2 = ridge.fit(X_train[:,i][:,np.newaxis], Y_train)\n",
    "\n",
    "  y_pred_ridge2 = ridge.predict(X_test[:,i][:,np.newaxis])\n",
    "  \n",
    "  MSE_ranking.append(np.mean((Y_test-y_pred_ridge2)**2))\n",
    "  \n",
    "MSE_ranking = np.asarray(MSE_ranking)\n",
    "\n",
    "print(\"Features in order of relevancy based on MSE:\")\n",
    "print(MSE_ranking)\n",
    "feat_rel_MSE = []\n",
    "for index, elements in enumerate(MSE_ranking.argsort()):\n",
    "  feat_rel_MSE.append(feature_names[elements])\n",
    "\n",
    "  \n",
    "print(feat_rel_MSE)\n",
    "\n",
    "\n",
    "X_train_red = X_train[:, MSE_ranking.argsort()[:4]]\n",
    "X_test_red = X_test[:, MSE_ranking.argsort()[:4]]\n",
    "\n",
    "\n",
    "ridge_final_mse =ridge.fit(X_train_red, Y_train)\n",
    "\n",
    "y_pred_final_ridge = ridge.predict(X_test_red)\n",
    "\n",
    "\n",
    "print('R2 score using only the 4th most relevant features based on MSE: %.2f' % r2_score(Y_test, y_pred_final_ridge) )\n",
    "print('Versus')\n",
    "print('R2 score using all the features: %.2f' %  r2_score_ridge)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-bqXTdaGG6oY"
   },
   "source": [
    "As we can see, if we use the MSE ranking to evaluate the relevancy of each feature we obtain a score. In fact, what we are testing is the relevancy of each feature working alone. We are not taking into account the combination of them working together, that can be the fact that gives us worse prediction.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">\n",
    "It's a bit strange that you rank your features with MSE, but you use R2 to evaluate the method performance. It's better to be coherent using the same measurement for both ranking and evaluate. <br>\n",
    "Besides, you rank your features with the test error. I'm asking you a relevance ranking based on the validation error!!!! <br>\n",
    "Why are you getting the final performance with 4 features???\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iOmigoAJP48S"
   },
   "source": [
    "## 3.2 Greedy search based on the validation error"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "o3Ddb2bQSxn4"
   },
   "source": [
    "### Forward selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WmXBqk7-I1On"
   },
   "source": [
    "In this case we uses MLXTend SequentialFeatureSelector that implements the forward greedy search. In fact, we are going to train for all posible subset sizes. If we have 10 features, we are going to use a for going from 1 to 10 trying all the subsets of features. Also, we are doing cross-validation with a 5 folder CV to prove different types of subsets randomly. Then, we append each score to compare which subset of features gives us the best score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 190
    },
    "colab_type": "code",
    "id": "AHymd5rmP5G8",
    "outputId": "51b0bc00-c55f-46f6-b516-629ccbb75417"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.6/dist-packages/sklearn/externals/joblib/__init__.py:15: DeprecationWarning: sklearn.externals.joblib is deprecated in 0.21 and will be removed in 0.23. Please import this functionality directly from joblib, which can be installed with: pip install joblib. If this warning is raised when loading pickled models, you may need to re-serialize those models with scikit-learn 0.21+.\n",
      "  warnings.warn(msg, category=DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores (R2) using diferent k_features input, from 1 to 10:\n",
      "[0.3670936150334024, 0.499802752587447, 0.5179771716570686, 0.5381629358946187, 0.5417332604078625, 0.5502416448735754, 0.5487351983595299, 0.5449278932364505, 0.539698037320315, 0.5371596190647546]\n",
      "Maximum score (R2) obtained:\n",
      "0.5502416448735754\n",
      "Obtained used k_subset of size:\n",
      "6\n",
      "['sex', 'bmi', 'bp', 's1', 's2', 's5']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as sfs\n",
    "from sklearn.metrics import SCORERS\n",
    "\n",
    "\n",
    "\n",
    "# Build step forward feature selection\n",
    "scorer_k = []\n",
    "feat_sel_forward = []\n",
    "for k in range(X_train.shape[1]):\n",
    "  sfs1 = sfs(ridge,\n",
    "             k_features=k+1,\n",
    "             forward=True,\n",
    "             floating=False,\n",
    "             verbose=0,\n",
    "             scoring='r2',\n",
    "             cv=5)\n",
    "  sfs1 = sfs1.fit(X_train, Y_train)\n",
    "  scorer_k.append(sfs1.k_score_)\n",
    "  feat_sel_forward.append(sfs1.k_feature_idx_)\n",
    "  \n",
    "\n",
    "\n",
    "print(\"Scores (R2) using diferent k_features input, from 1 to 10:\")\n",
    "print(scorer_k)\n",
    "print(\"Maximum score (R2) obtained:\")\n",
    "print(np.amax(np.asarray(scorer_k)))\n",
    "print(\"Obtained used k_subset of size:\")\n",
    "print(np.argmax(np.asarray(scorer_k))+1)\n",
    "\n",
    "feat_rel_fw = []\n",
    "for elements in feat_sel_forward[np.argmax(np.asarray(scorer_k))]:\n",
    "  feat_rel_fw.append(feature_names[elements])\n",
    "\n",
    "print(feat_rel_fw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1ghDw1nGIjgs"
   },
   "source": [
    "Using a forward selection, it returns us that the best subset of features is using 6 features and using the ones mentioned in the array before. Lets try it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "hhwlFc43Itwm",
    "outputId": "4b25ce14-7f61-4e58-f892-9fefc2bc0d0f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features used:\n",
      "(1, 2, 3, 4, 5, 8)\n",
      "Size of our X_train to verify that we are using the correct number of features\n",
      "(265, 6)\n",
      "R2 score using only the 6th most relevant features based on forward selection: 0.390040998439935\n",
      "Versus\n",
      "R2 score using all the features: 0.39\n"
     ]
    }
   ],
   "source": [
    "X_train_red = X_train[:, feat_sel_forward[np.argmax(np.asarray(scorer_k))][:]]\n",
    "X_test_red = X_test[:, feat_sel_forward[np.argmax(np.asarray(scorer_k))][:]]\n",
    "\n",
    "print(\"Features used:\")\n",
    "print(feat_sel_forward[np.argmax(np.asarray(scorer_k))][:])\n",
    "print(\"Size of our X_train to verify that we are using the correct number of features\")\n",
    "print(X_train_red.shape)\n",
    "\n",
    "ridge = ridge.fit(X_train_red, Y_train)\n",
    "\n",
    "y_pred_final_ridge = ridge.predict(X_test_red)\n",
    "\n",
    "fw = str(r2_score(Y_test, y_pred_final_ridge))\n",
    "print('R2 score using only the 6th most relevant features based on forward selection: ' + fw )\n",
    "print('Versus')\n",
    "print('R2 score using all the features: %.2f' %  r2_score_ridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oWz59f9lJ2DE"
   },
   "source": [
    "As it is shown, we obtained the same prediction accuracy (based on R2) using only 6 features. It shows that the 4 features didn't used are not usefull and we can only focus on using that 6 features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">\n",
    "Good work and good conclusions! <br>\n",
    "I didn't know the MLXTend SequentialFeatureSelector toolbox. Interesting!!!!\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Yg3DTEoOiR9U"
   },
   "source": [
    "### Backward selector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "UdURvABuQLnD",
    "outputId": "0ac2e746-c4e1-4f04-ad84-db119266a897"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scores (R2) using diferent k_features input, from 1 to 10:\n",
      "[0.3670936150334024, 0.499802752587447, 0.5179771716570686, 0.5381629358946209, 0.5417332604078625, 0.5502416448735754, 0.5487351983595299, 0.5436935172290537, 0.5408888926695636, 0]\n",
      "Maximum score (R2) obtained:\n",
      "0.5502416448735754\n",
      "Obtained used k_subset of size:\n",
      "6\n",
      "['sex', 'bmi', 'bp', 's1', 's2', 's5']\n"
     ]
    }
   ],
   "source": [
    "# Build step forward feature selection\n",
    "scorer_k2 = []\n",
    "feat_sel_backward = []\n",
    "for k in range(X_train.shape[1]):\n",
    "  sfs2 = sfs(ridge,\n",
    "             k_features=k+1,\n",
    "             forward=False,\n",
    "             floating=False,\n",
    "             verbose=0,\n",
    "             scoring='r2',\n",
    "             cv=5)\n",
    "  sfs2 = sfs2.fit(X_train, Y_train)\n",
    "  scorer_k2.append(sfs2.k_score_)\n",
    "  feat_sel_backward.append(sfs2.k_feature_idx_)\n",
    "  \n",
    "\n",
    "\n",
    "print(\"Scores (R2) using diferent k_features input, from 1 to 10:\")\n",
    "print(scorer_k2)\n",
    "print(\"Maximum score (R2) obtained:\")\n",
    "print(np.amax(np.asarray(scorer_k2)))\n",
    "print(\"Obtained used k_subset of size:\")\n",
    "print(np.argmax(np.asarray(scorer_k2))+1)\n",
    "\n",
    "feat_rel_bw = []\n",
    "for elements in feat_sel_backward[np.argmax(np.asarray(scorer_k2))]:\n",
    "  feat_rel_bw.append(feature_names[elements])\n",
    "\n",
    "print(feat_rel_bw)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jygk6i_YH9gD"
   },
   "source": [
    "Using the backward selector it gives us that the best subset of parameters to use is 6. Let's now train our model with that 6 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "JQzWxntWISvp",
    "outputId": "374b5b04-fa4b-49b3-f737-9d166229b125"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features used:\n",
      "(1, 2, 3, 4, 5, 8)\n",
      "Size of our X_train to verify that we are using the correct number of features\n",
      "(265, 6)\n",
      "R2 score using only the 6th most relevant features based on backward selection:0.390040998439935\n",
      "Versus\n",
      "R2 score using all the features: 0.39\n"
     ]
    }
   ],
   "source": [
    "X_train_red = X_train[:, feat_sel_backward[np.argmax(np.asarray(scorer_k2))][:]]\n",
    "X_test_red = X_test[:, feat_sel_backward[np.argmax(np.asarray(scorer_k2))][:]]\n",
    "\n",
    "print(\"Features used:\")\n",
    "print(feat_sel_backward[np.argmax(np.asarray(scorer_k2))][:])\n",
    "print(\"Size of our X_train to verify that we are using the correct number of features\")\n",
    "print(X_train_red.shape)\n",
    "\n",
    "ridge.fit(X_train_red, Y_train)\n",
    "\n",
    "y_pred_final_ridge = ridge.predict(X_test_red)\n",
    "\n",
    "bw = str(r2_score(Y_test, y_pred_final_ridge))\n",
    "print('R2 score using only the 6th most relevant features based on backward selection:' +bw )\n",
    "print('Versus')\n",
    "print('R2 score using all the features: %.2f' %  r2_score_ridge)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Jo3wLc3uJesj"
   },
   "source": [
    "As it is shown, we obtained the same prediction accuracy (based on R2) using only 6 features. It shows that the 4 features didn't used are not usefull and we can only focus on using that 6 features. Also, it gives us the same result as using forward selector. It confirms us the good result of both methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">\n",
    "Great!!!!\n",
    "</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wROzbk8cQL0x"
   },
   "source": [
    "## 3.4 Ridge linear regression with a prunning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 422
    },
    "colab_type": "code",
    "id": "HcvnFuiEQL7g",
    "outputId": "0d4e6e97-098b-41b0-8a28-98e96cfd2bcb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features weighted before ordering:\n",
      "[ 60.72803425 211.97264922 537.75350139 335.22467273 120.4549626\n",
      " 104.22225625 221.49209594  68.94103599 552.62565564  56.12123157]\n",
      "Features in order of relevancy based on Ridge regularization:\n",
      "['s5', 'bmi', 'bp', 's3', 'sex', 's1', 's2', 's4', 'age', 's6']\n",
      "Mean of weights: 226.95360955785358\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nzqCXpM79P1fJhcqvFq5uAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features above mean: \n",
      "[8, 2, 3]\n",
      "['s5', 'bmi', 'bp']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "weights_ridge_abs = abs(ridge_model.coef_)\n",
    "weights_ridge_pos = (-weights_ridge_abs).argsort()[:n]\n",
    "print(\"Features weighted before ordering:\")\n",
    "print(weights_ridge_abs)\n",
    "print(\"Features in order of relevancy based on Ridge regularization:\")\n",
    "\n",
    "feat_rel_ridge = []\n",
    "feat_above_mean = []\n",
    "for elements in weights_ridge_pos:\n",
    "  feat_rel_ridge.append(feature_names[elements])\n",
    "  \n",
    "  if weights_ridge_abs[elements] > np.mean(weights_ridge_abs):\n",
    "    feat_above_mean.append(elements)\n",
    "  \n",
    "print(feat_rel_ridge)\n",
    "\n",
    "print(\"Mean of weights: \" + str(np.mean(weights_ridge_abs)))\n",
    "\n",
    "plt.bar(feature_names,weights_ridge_abs)\n",
    "plt.axhline(y=np.mean(weights_ridge_abs))\n",
    "plt.show()\n",
    "\n",
    "print(\"Features above mean: \")\n",
    "print(feat_above_mean)\n",
    "feat_above_mean2=[]\n",
    "for elements in feat_above_mean:\n",
    "  feat_above_mean2.append(feature_names[elements])\n",
    "\n",
    "print(feat_above_mean2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "N5IP-As4KZus"
   },
   "source": [
    "That method gives us a weighted view of our features. Lets train a ridge model using only the features that are above the mean. As can be shown below, the score obtained is the same as using all the features. So we have prunned which features are really interesting to our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<span style=\"color:red\">\n",
    "Great!!!! Using the mean to set a threshold is a nice criterium.\n",
    "</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "K593goLrR6cX",
    "outputId": "c6274466-3e9d-466d-c781-3f20e73373a8"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features used:\n",
      "[8, 2, 3]\n",
      "Size of our X_train to verify that we are using the correct number of features\n",
      "(265, 3)\n",
      "R2 score using only the features above the mean most relevant features based on Ridge prunning: 0.38786380847366697\n",
      "Versus\n",
      "R2 score using all the features: 0.39\n"
     ]
    }
   ],
   "source": [
    "X_train_red = X_train[:, feat_above_mean[:]]\n",
    "X_test_red = X_test[:, feat_above_mean[:]]\n",
    "\n",
    "print(\"Features used:\")\n",
    "print(feat_above_mean[:])\n",
    "print(\"Size of our X_train to verify that we are using the correct number of features\")\n",
    "print(X_train_red.shape)\n",
    "\n",
    "ridge.fit(X_train_red, Y_train)\n",
    "\n",
    "y_pred_final_ridge = ridge.predict(X_test_red)\n",
    "\n",
    "l2 = str(r2_score(Y_test, y_pred_final_ridge))\n",
    "print('R2 score using only the features above the mean most relevant features based on Ridge prunning: ' +l2 )\n",
    "print('Versus')\n",
    "print('R2 score using all the features: %.2f' %  r2_score_ridge)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DpvbNc7CkYaT"
   },
   "source": [
    "## 3.5 Lasso regularization method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 354
    },
    "colab_type": "code",
    "id": "UPY0vBn_kcuC",
    "outputId": "f37deffd-5480-4e7b-853a-64998b860666"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using Lasso regularization we have erased 2 features\n",
      "Features erased: \n",
      "['s2', 's4']\n",
      "Features in order of relevancy: \n",
      "['s5', 'bmi', 'bp', 's3', 'sex', 's1', 'age', 's6', 's2', 's4']\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz\nAAALEgAACxIB0t1+/AAAADl0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uIDMuMC4zLCBo\ndHRwOi8vbWF0cGxvdGxpYi5vcmcvnQurowAAEkpJREFUeJzt3Xuw3Gddx/H3x6ZFLtq09BhL0npw\niCKK0HqmlMEr9UKpYzpQWtCxsRYzOEVUVIw4g5fxUmRGaL1Ug62kWoVOpTRDK9IpeEEpcGpDKITL\nsaYksW2OpS3WDmjl6x/7RLchydmTs3tO++T9mtnZ3+/5Pb/9PrvZfPa3z/52T6oKSVK/vmKlByBJ\nmiyDXpI6Z9BLUucMeknqnEEvSZ0z6CWpcwa9JHXOoJekzhn0ktS5VSs9AICTTjqppqenV3oYkvS4\nctttt/17VU0t1O8xEfTT09PMzs6u9DAk6XElyV2j9HPqRpI6Z9BLUucMeknqnEEvSZ0z6CWpcwa9\nJHXOoJekzhn0ktS5kYI+yeok1yX5ZJKdSZ6f5MQkNyf5TLs+ofVNksuTzCXZkeT0yd4FSdLhjPrN\n2MuA91TVeUmOA54EvB64paouTbIZ2Az8InA2sL5dngdc0a4ladGmN9848Rq7Lj1n4jVW0oJH9EmO\nB74TuBKgqv6rqh4ANgBbW7etwLlteQNwdQ3cCqxOcvLYRy5JGskoUzdPB+aBP01ye5I/SfJkYE1V\n3d363AOsactrgd1D++9pbZKkFTBK0K8CTgeuqKrTgP9kME3zf6qqgFpM4SSbkswmmZ2fn1/MrpKk\nRRgl6PcAe6rqQ239OgbBf+/+KZl2va9t3wucMrT/utb2KFW1papmqmpmamrBX9mUJB2hBYO+qu4B\ndif5xtZ0FvAJYBuwsbVtBG5oy9uAC9vZN2cCDw5N8UiSltmoZ938FHBNO+PmTuAiBi8S1ya5GLgL\nOL/1vQl4MTAHPNz6SpJWyEhBX1XbgZmDbDrrIH0LuGSJ45IkjYnfjJWkzhn0ktQ5g16SOmfQS1Ln\nDHpJ6pxBL0mdM+glqXMGvSR1zqCXpM4Z9JLUuVF/60aPMf7VHUmj8ohekjpn0EtS5wx6SeqcQS9J\nnTPoJalzBr0kdc6gl6TOGfSS1DmDXpI6Z9BLUucMeknqnEEvSZ0z6CWpcwa9JHVupKBPsivJx5Js\nTzLb2k5McnOSz7TrE1p7klyeZC7JjiSnT/IOSJIObzFH9N9TVc+tqpm2vhm4parWA7e0dYCzgfXt\nsgm4YlyDlSQt3lKmbjYAW9vyVuDcofara+BWYHWSk5dQR5K0BKMGfQHvTXJbkk2tbU1V3d2W7wHW\ntOW1wO6hffe0NknSChj1Twl+e1XtTfI1wM1JPjm8saoqSS2mcHvB2ARw6qmnLmZXSdIijHREX1V7\n2/U+4HrgDODe/VMy7Xpf674XOGVo93Wt7cDb3FJVM1U1MzU1deT3QJJ0WAsGfZInJ/mq/cvA9wN3\nANuAja3bRuCGtrwNuLCdfXMm8ODQFI8kaZmNMnWzBrg+yf7+f1FV70nyEeDaJBcDdwHnt/43AS8G\n5oCHgYvGPmpJ0sgWDPqquhN4zkHa7wPOOkh7AZeMZXSSpCXzm7GS1DmDXpI6Z9BLUucMeknqnEEv\nSZ0z6CWpcwa9JHXOoJekzhn0ktQ5g16SOmfQS1LnDHpJ6pxBL0mdM+glqXMGvSR1zqCXpM4Z9JLU\nOYNekjpn0EtS5wx6SeqcQS9JnTPoJalzBr0kdc6gl6TOGfSS1LmRgz7JMUluT/Lutv70JB9KMpfk\nHUmOa+1PaOtzbfv0ZIYuSRrFYo7ofxrYObT+RuDNVfUM4H7g4tZ+MXB/a39z6ydJWiEjBX2SdcA5\nwJ+09QAvBK5rXbYC57blDW2dtv2s1l+StAJGPaJ/C/A64Ett/anAA1X1SFvfA6xty2uB3QBt+4Ot\nvyRpBSwY9El+ENhXVbeNs3CSTUlmk8zOz8+P86YlSUNGOaJ/AfBDSXYBb2cwZXMZsDrJqtZnHbC3\nLe8FTgFo248H7jvwRqtqS1XNVNXM1NTUku6EJOnQFgz6qvqlqlpXVdPAy4H3VdWPAO8HzmvdNgI3\ntOVtbZ22/X1VVWMdtSRpZEs5j/4XgdcmmWMwB39la78SeGprfy2weWlDlCQtxaqFu/y/qvpb4G/b\n8p3AGQfp8wXgZWMYmyRpDPxmrCR1zqCXpM4Z9JLUOYNekjpn0EtS5wx6SeqcQS9JnTPoJalzBr0k\ndc6gl6TOGfSS1DmDXpI6Z9BLUucMeknqnEEvSZ0z6CWpcwa9JHXOoJekzhn0ktQ5g16SOmfQS1Ln\nDHpJ6pxBL0mdM+glqXMGvSR1bsGgT/KVST6c5KNJPp7k11r705N8KMlcknckOa61P6Gtz7Xt05O9\nC5Kkw1k1Qp8vAi+sqoeSHAt8IMlfA68F3lxVb0/yR8DFwBXt+v6qekaSlwNvBC6Y0Pi1AqY33zjx\nGrsuPWfiNaSjxYJH9DXwUFs9tl0KeCFwXWvfCpzblje0ddr2s5JkbCOWJC3KSHP0SY5Jsh3YB9wM\n/AvwQFU90rrsAda25bXAboC2/UHgqeMctCRpdCMFfVX9T1U9F1gHnAE8c6mFk2xKMptkdn5+fqk3\nJ0k6hEWddVNVDwDvB54PrE6yf45/HbC3Le8FTgFo248H7jvIbW2pqpmqmpmamjrC4UuSFjLKWTdT\nSVa35ScC3wfsZBD457VuG4Eb2vK2tk7b/r6qqnEOWpI0ulHOujkZ2JrkGAYvDNdW1buTfAJ4e5Lf\nAG4Hrmz9rwT+LMkc8Dng5RMYtyRpRAsGfVXtAE47SPudDObrD2z/AvCysYxOkrRkfjNWkjpn0EtS\n5wx6SeqcQS9JnTPoJalzBr0kdc6gl6TOGfSS1DmDXpI6Z9BLUucMeknqnEEvSZ0z6CWpcwa9JHXO\noJekzhn0ktQ5g16SOmfQS1LnDHpJ6twofxx84u6c/08u+OMPrvQwdICV/Dfx+aDl1PvzzSN6Sepc\nqmqlx8DMzEzNzs6u9DAeV6Y33zjxGrsuPecxV1tHH59vh5bktqqaWaifR/SS1DmDXpI6Z9BLUucW\nPOsmySnA1cAaoIAtVXVZkhOBdwDTwC7g/Kq6P0mAy4AXAw8DP1ZV/zyZ4UtHj0nPVT9e56m1sFGO\n6B8Bfq6qngWcCVyS5FnAZuCWqloP3NLWAc4G1rfLJuCKsY9akjSyBYO+qu7ef0ReVf8B7ATWAhuA\nra3bVuDctrwBuLoGbgVWJzl57COXJI1kUXP0SaaB04APAWuq6u626R4GUzsweBHYPbTbntYmSVoB\nIwd9kqcAfwX8TFV9fnhbDU7GX9QJ+Uk2JZlNMjs/P7+YXSVJizBS0Cc5lkHIX1NV72zN9+6fkmnX\n+1r7XuCUod3XtbZHqaotVTVTVTNTU1NHOn5J0gIWDPp2Fs2VwM6q+t2hTduAjW15I3DDUPuFGTgT\neHBoikeStMxG+VGzFwA/CnwsyfbW9nrgUuDaJBcDdwHnt203MTi1co7B6ZUXjXXEkqRFWTDoq+oD\nQA6x+ayD9C/gkiWOS5I0Jn4zVpI6Z9BLUuceE3945PHKn0+V9Hhg0Otxx998kRbHqRtJ6pxBL0md\nM+glqXMGvSR1zqCXpM4Z9JLUOYNekjpn0EtS5wx6SeqcQS9JnTPoJalzBr0kdc6gl6TOGfSS1DmD\nXpI6Z9BLUucMeknqnEEvSZ0z6CWpcwa9JHXOoJekzi0Y9EmuSrIvyR1DbScmuTnJZ9r1Ca09SS5P\nMpdkR5LTJzl4SdLCRjmifxvwogPaNgO3VNV64Ja2DnA2sL5dNgFXjGeYkqQjtWDQV9XfA587oHkD\nsLUtbwXOHWq/ugZuBVYnOXlcg5UkLd6RztGvqaq72/I9wJq2vBbYPdRvT2uTJK2QJX8YW1UF1GL3\nS7IpyWyS2fn5+aUOQ5J0CEca9Pfun5Jp1/ta+17glKF+61rbl6mqLVU1U1UzU1NTRzgMSdJCjjTo\ntwEb2/JG4Iah9gvb2TdnAg8OTfFIklbAqoU6JPlL4LuBk5LsAX4FuBS4NsnFwF3A+a37TcCLgTng\nYeCiCYxZkrQICwZ9Vb3iEJvOOkjfAi5Z6qAkSePjN2MlqXMGvSR1zqCXpM4Z9JLUOYNekjpn0EtS\n5wx6SeqcQS9JnTPoJalzBr0kdc6gl6TOGfSS1DmDXpI6Z9BLUucMeknqnEEvSZ0z6CWpcwa9JHXO\noJekzhn0ktS5Bf84+GPd9OYbJ15j16XnTLyGJE2KR/SS1DmDXpI6Z9BLUuce93P0kjQpvXwGOJEj\n+iQvSvKpJHNJNk+ihiRpNGMP+iTHAH8AnA08C3hFkmeNu44kaTSTOKI/A5irqjur6r+AtwMbJlBH\nkjSCSQT9WmD30Pqe1iZJWgGpqvHeYHIe8KKqemVb/1HgeVX16gP6bQI2tdVvBD411oEc3knAvy9j\nPWtb29rWnoSvq6qphTpN4qybvcApQ+vrWtujVNUWYMsE6i8oyWxVzVjb2ta2di+1D2cSUzcfAdYn\neXqS44CXA9smUEeSNIKxH9FX1SNJXg38DXAMcFVVfXzcdSRJo5nIF6aq6ibgpknc9pisyJSRta1t\nbWuvhLF/GCtJemzxt24kqXMGfSeSTCe54wj3fVqS6x4LY5mEJK9uP8dRSU5a5trXtJ8DuSPJVUmO\nXcbaVyb5aJIdSa5L8pTlqj00hsuTPLTMNd+W5F+TbG+X5y5j7ST5zSSfTrIzyWuWq/bhGPSiqv6t\nqs5b6XFM0D8C3wvctQK1rwGeCTwbeCLwymWs/bNV9Zyq+lbgs8CrF9phnJLMACcsZ80hv1BVz22X\n7ctY98cYnF7+zKr6Jga/DLDiugz6JO9KcluSj7cvZpHk4vYq++Ekb03y+619KslfJflIu7xgibWf\nnOTGdiR1R5ILknxbkr9rY/qbJCcnWdXqfXfb77eT/OYS7/qqdgS5sx3BPSnJrnbb25PMJjm9jeFf\nkryq1Z7EEfihxvI7ST7W/h2eMeaaB338q+r2qto17loj1r6pGuDDDL5Xsly1P9+2hcGLzEQ+kDvE\nc/4Y4E3A6yZR83C1J1lvhNo/Cfx6VX0JoKr2Ldd4DquqursAJ7brJwJ3MPgJhl3AicCxwD8Av9/6\n/AXw7W35VGDnEmu/FHjr0PrxwD8BU239AgannAJ8M7CTwdHm7cBxS6g7zeA/8gva+lXAz7f7/ZOt\n7c3ADuCrgCng3qF97xjj43+4sfxya7sQePcE/u2/7PEfWt4FnDTB593hah8L/DPwHctZG/hT4F7g\n/cCTlqs28NMM3lEAPLScjznwNgbftN/RnvNPWMba9wG/DMwCfw2sn9R9X9RYV3oAE/oH+FXgo+3y\nILAZ2Dq0/TVDQb8P2D502Qs8ZQm1v6EFyhuB7wC+Bfj80O1/DHjvUP/XA18ATlvifZ4GPju0/kLg\nXW0sa1vbjx/wxPwssHpCQX+osXx9azsWuG8C//aPevwP2DbpoD9c7bcCb1mh2scAfwhctBy1gacB\nHwBWte2TDPovu9/AyUCAJwBbgTcsY+2HgJ9ryy8B/mFS930xl+6mbtpUyPcCz6+q5zA4Uv7kYXb5\nCuDM+v/5vLVVdcQfHlXVp4HTGQT6bzB41f/40O0/u6q+f2iXZwMPAF9zpDWHyx9i/Yvt+ktDy/vX\nJ/XHZw41ljpMn6UXPeDxT/KGcddYbO0kv8LgHdRrl7t22/Y/DOaKX7octYGfAJ4BzCXZBTwpydxy\n1E7yhqq6uwa+yOAdzRnLVZvBjzi+s3W5HvjWSdRerO6CnsHbp/ur6uEkzwTOBJ4MfFeSE5Ks4tFP\n+PcCP7V/Zamf0Cd5GvBwVf05gznK5wFTSZ7fth+b5Jvb8ksYTCd9J/B7SVYvpTZw6v46wA8zOKpa\nKYcaywVD1x8cd9GDPP6nj7vGYmoneSXwA8Arqs3bLlPtb9v/GUibo/8hDn/AM87ap1XV11bVdFVN\nt21j/zzmELVPT3Jy2xbgXAbTt8tSm8E71+9pXb4L+PQkai9Wj39K8D3Aq5LsZDBPdyuD6ZjfYvBh\n2OcYPOEfbP1fA/xBkh0MHo+/B161hPrPBt6U5EvAfzP4cOYR4PIkx7cab0lyL3ApcFZV7W4fDl8G\nbFxC7U8BlyS5CvgEcAVDL2LL7FBjOaE91l8EXjGBul/2+GdwitvrgK8FdiS5qdqvq066NoPn313A\nBwe5wzur6teXofYlwNYkX81gGuOjbTyTcLD7vVwOVvuaJFMM7vd2lvb/ebG151r9n2UwjbOcZ1kd\n0lHzzdgkT6mqh9oR/fUMPhC9fqXHdTRpb+NnqmqlfkJWOir1OHVzKL+aZDuDt3H/yuAtliR176g5\nopeko9XRdEQvSUclg16SOmfQS1LnDHpJ6pxBL0mdM+glqXP/Cy+pAOS2MIcWAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "coeff_used = np.sum(lasso.coef_!=0)\n",
    "coeff_erased = X.shape[1] - coeff_used\n",
    "\n",
    "w_lasso_abs = abs(lasso.coef_)\n",
    "w_lasso_pos = (-w_lasso_abs).argsort()[:n]\n",
    "\n",
    "print(\"Using Lasso regularization we have erased \"+str(coeff_erased)+\" features\")\n",
    "\n",
    "erased_features = []\n",
    "\n",
    "for index,element in enumerate(w_lasso_abs):\n",
    "  if element == 0:\n",
    "    erased_features.append(feature_names[index])\n",
    "\n",
    "feat_rel_lasso = []\n",
    "for element in w_lasso_pos:\n",
    "  feat_rel_lasso.append(feature_names[element])\n",
    "  \n",
    "\n",
    "    \n",
    "print(\"Features erased: \")\n",
    "print(erased_features)\n",
    "print(\"Features in order of relevancy: \")\n",
    "print(feat_rel_lasso)\n",
    "\n",
    "plt.bar(feature_names,abs(lasso.coef_))\n",
    "plt.axhline(y=np.mean(weights_ridge_abs))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Osqs8EGzT7K8"
   },
   "source": [
    "Using Lasso we figure out that two variables are useless to our model. In fact, it can be shown that the features more relevants are the same as using Ridge. Lets try to fit our model using the 8 features that are not equal to 0.\n",
    "\n",
    "As shown below, we archieve the same result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "joJnK6C0UN_k",
    "outputId": "963bc183-0072-413a-caa2-e1a62d9b8415"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features used:\n",
      "[8 2 3 6 1 4 0 9]\n",
      "Size of our X_train to verify that we are using the correct number of features\n",
      "(265, 8)\n",
      "R2 score using only the features above the mean most relevant features based on Lasso method: 0.39315651098116455\n",
      "Versus\n",
      "R2 score using all the features: 0.39\n"
     ]
    }
   ],
   "source": [
    "X_train_red = X_train[:, w_lasso_pos[:8]]\n",
    "X_test_red = X_test[:, w_lasso_pos[:8]]\n",
    "\n",
    "print(\"Features used:\")\n",
    "print(w_lasso_pos[:8])\n",
    "print(\"Size of our X_train to verify that we are using the correct number of features\")\n",
    "print(X_train_red.shape)\n",
    "\n",
    "ridge = ridge.fit(X_train_red, Y_train)\n",
    "\n",
    "y_pred_final_ridge = ridge.predict(X_test_red)\n",
    "\n",
    "l1 = str(r2_score(Y_test, y_pred_final_ridge))\n",
    "print('R2 score using only the features above the mean most relevant features based on Lasso method: ' + l1)\n",
    "print('Versus')\n",
    "print('R2 score using all the features: %.2f' %  r2_score_ridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "AFlOfTheoLk6"
   },
   "source": [
    "## 3.6 Elastic-Net reg method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 422
    },
    "colab_type": "code",
    "id": "bUFPhd6ZoL9S",
    "outputId": "a886751a-ca12-44f6-d29c-b3fdb96bac68"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features weighted before ordering:\n",
      "[ 49.98153777 197.32128485 524.75452537 323.0975307   87.80161107\n",
      " 110.63067731 229.59502326  60.42732822 527.40456202  58.36287593]\n",
      "Features in order of relevancy based on Ridge regularization:\n",
      "['s5', 'bmi', 'bp', 's3', 'sex', 's2', 's1', 's4', 's6', 'age']\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD8CAYAAAB5Pm/hAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n6pxBL0mdM+glqXP/B+log7v1W+xLAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean of weights: 216.93769564978703\n",
      "Features above mean: \n",
      "[8, 2, 3, 6]\n",
      "['s5', 'bmi', 'bp', 's3']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "weights_en_abs = abs(regr.coef_)\n",
    "weights_en_pos = (-weights_en_abs).argsort()\n",
    "print(\"Features weighted before ordering:\")\n",
    "print(weights_en_abs)\n",
    "print(\"Features in order of relevancy based on Ridge regularization:\")\n",
    "\n",
    "feat_rel_en = []\n",
    "feat_above_mean = []\n",
    "for elements in weights_en_pos:\n",
    "  feat_rel_en.append(feature_names[elements])\n",
    "  \n",
    "  if weights_en_abs[elements] > np.mean(weights_en_abs):\n",
    "    feat_above_mean.append(elements)\n",
    "  \n",
    "print(feat_rel_en)\n",
    "\n",
    "\n",
    "plt.bar(feature_names,weights_en_abs)\n",
    "plt.axhline(y=np.mean(weights_ridge_abs))\n",
    "plt.show()\n",
    "\n",
    "print(\"Mean of weights: \" + str(np.mean(weights_en_abs)))\n",
    "print(\"Features above mean: \")\n",
    "print(feat_above_mean)\n",
    "feat_above_mean2=[]\n",
    "for elements in feat_above_mean:\n",
    "  feat_above_mean2.append(feature_names[elements])\n",
    "\n",
    "print(feat_above_mean2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "lhWWqLpaVhut"
   },
   "source": [
    "As shown in the two last methods, our Elastic-Net regulirazation model has chosen the same features as before as the most relevant (8, 2 and 3). Also, it has chosen the feature 6 as relevant because is above the mean of weights. In that case, lets try to train with this 4 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 136
    },
    "colab_type": "code",
    "id": "-wrsqsPzV1Ma",
    "outputId": "c8458c59-a500-4496-dafa-c1df196f52f6"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features used:\n",
      "[8, 2, 3, 6]\n",
      "Size of our X_train to verify that we are using the correct number of features\n",
      "(265, 4)\n",
      "R2 score using only the features above the mean most relevant features based on Elastic-Net prunning: 0.39377055739195754\n",
      "Versus\n",
      "R2 score using all the features: 0.39\n"
     ]
    }
   ],
   "source": [
    "X_train_red = X_train[:, feat_above_mean[:]]\n",
    "X_test_red = X_test[:, feat_above_mean[:]]\n",
    "\n",
    "print(\"Features used:\")\n",
    "print(feat_above_mean[:])\n",
    "print(\"Size of our X_train to verify that we are using the correct number of features\")\n",
    "print(X_train_red.shape)\n",
    "\n",
    "ridge.fit(X_train_red, Y_train)\n",
    "\n",
    "y_pred_final_ridge = ridge.predict(X_test_red)\n",
    "\n",
    "en = str(r2_score(Y_test, y_pred_final_ridge))\n",
    "print('R2 score using only the features above the mean most relevant features based on Elastic-Net prunning: ' + en )\n",
    "print('Versus')\n",
    "print('R2 score using all the features: %.2f' %  r2_score_ridge)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CvSTEOMloQVs"
   },
   "source": [
    "# 4. CONCLUSIONS\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 173
    },
    "colab_type": "code",
    "id": "vYwUtzvioTXZ",
    "outputId": "cba44ff1-9808-4e07-ba9f-616fab43a080"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------Model 0: all features without feature selecion---------------------\n",
      "R2 score: 0.39463297771427264\n",
      "--------------------Rest of models: r2 scores based on features selected by each model-----------------\n",
      "{'Lasso': '0.39315651098116455', 'Ridge': '0.38786380847366697', 'Elastic-Net': '0.39377055739195754', 'Backward': '0.390040998439935', 'Forward': '0.390040998439935'}\n",
      "As it is shown,  we can archieve similar results using only some features for that model. That means that there are some features that are not relevant or gives us no-information. On that case, we can erase them to make our model easily to compute.\n",
      "The best perform has been given by:\n",
      "0.39377055739195754\n",
      "And it has been given by the Elastic-Net model.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "print(\"--------------------Model 0: all features without feature selecion---------------------\")\n",
    "print(\"R2 score: \" +str(r2_score_ridge))\n",
    "\n",
    "print(\"--------------------Rest of models: r2 scores based on features selected by each model-----------------\")\n",
    "r2_dict = {\n",
    "  \"Lasso\": l1,\n",
    "  \"Ridge\": l2,\n",
    "  \"Elastic-Net\": en,\n",
    "  \"Backward\": bw,\n",
    "  \"Forward\": fw,\n",
    "}\n",
    "print(r2_dict)\n",
    "print(\"As it is shown,  we can archieve similar results using only some features for that model. That means that there are some features that are not relevant or gives us no-information. On that case, we can erase them to make our model easily to compute.\")\n",
    "print(\"The best perform has been given by:\")\n",
    "values = np.array(list(r2_dict.values()))\n",
    "print(max(values))\n",
    "print(\"And it has been given by the \"+ np.array(list(r2_dict.keys()))[np.argmax(values)]  +\" model.\") "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Homework1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}

