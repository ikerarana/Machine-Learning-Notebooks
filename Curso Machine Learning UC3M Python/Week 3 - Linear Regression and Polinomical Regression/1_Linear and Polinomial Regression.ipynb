{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Linear_Poly Regression.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.4"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hU1KgzIt8Jju",
        "colab_type": "text"
      },
      "source": [
        "# Week 3: Linear and (polynomial) Regression \n",
        "\n",
        "----------------------------------------------------\n",
        "Machine Learning                      \n",
        "\n",
        "Year 2019/2020\n",
        "\n",
        "*Vanessa Gómez Verdejo vanessa@tsc.uc3m.es* and *Pablo M. Olmos olmos@tsc.uc3m.es*\n",
        "\n",
        "----------------------------------------------------\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PYQd4Jcx8Jjy",
        "colab_type": "text"
      },
      "source": [
        "## Importing Packages"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "S5uPIxBk8Jj1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.datasets import load_diabetes\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sEmpHPYU--wR",
        "colab_type": "text"
      },
      "source": [
        "# Diabetes dataset\n",
        "\n",
        "This week, let's work with the **Diabetes** dataset where, from a set of ten patient characteristics (age, sex, body mass index, mean blood pressure and six blood serum measurements), we will have to predict disease progression one year later.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nz-Hypuk8JkA",
        "colab_type": "text"
      },
      "source": [
        "### Loading and preprocessing data\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fwnOU_SO8JkC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Load data and analize the data\n",
        "diabetes = load_diabetes()\n",
        "X = diabetes.data\n",
        "Y = diabetes.target\n",
        "feature_names = diabetes.feature_names\n",
        "\n",
        "print('Number of data: %s' %X.shape[0])\n",
        "D = X.shape[1]\n",
        "\n",
        "plt.figure( figsize=(20,5))\n",
        "for d in range (D):\n",
        "  plt.subplot(2,7,d+1)\n",
        "  plt.plot(X[:,d],Y,'.',)\n",
        "  plt.title(feature_names[d])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7fPxKTIcDuUZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Generate train and test partitions\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.6, random_state=0)\n",
        "\n",
        "# Data normalization\n",
        "transformer = StandardScaler().fit(X_train)  # fit() learns mean and std parameters\n",
        "X_train_norm = transformer.transform(X_train) # transform() normalizes\n",
        "X_test_norm =  transformer.transform(X_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "la4grh4AddXC",
        "colab_type": "text"
      },
      "source": [
        "To facilitate the analysis and visualization of the results, from now on we are going to work with a single input variable, for this we are going to use the BMI (feature #2)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "92dH93RjDuhS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Feature selection 2 (BMI)\n",
        "\n",
        "id_feat = 2\n",
        "X_train_BMI = X_train_norm[:, id_feat][:,np.newaxis]\n",
        "X_test_BMI = X_test_norm[:, id_feat][:,np.newaxis]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldN-64-J8JlG",
        "colab_type": "text"
      },
      "source": [
        "# The linear regression model\n",
        "\n",
        "\n",
        "Consider we have a **training** database of $N$ entries of the form $(\\mathbf{x}^{(i)},y^{(i)})$, where $\\mathbf{x}^{(i)}\\in\\mathbb{R}^D$ and $y^{(i)}\\in\\mathbb{R}$.  Each data or sample is vector of $D$ elements, $\\mathbf{x}=[x_1, \\ldots, x_D]$, and each one of these elements is called a **feature**.\n",
        "\n",
        "We will use this training set to fit a **linear** model defined by:\n",
        "\n",
        "$$f(\\mathbf{x}) =  w_0 + w_1 x_1 + w_2 x_2 + \\ldots + w_D x_D, $$\n",
        "\n",
        "where $w_0$ is called the **intercept**, and it captures the **bias** level of $y$. In the following, we will consider an extended version of the input data where we add a first entry with value equal to 1, i.e., $\\mathbf{x}_e=[1, x_1, \\ldots, x_D]$ so we can compactly write\n",
        "\n",
        "$$f(\\mathbf{x}) = \\mathbf{w}^T\\mathbf{x}_e$$\n",
        "\n",
        "This regression model is called a **linear regression** since we approximate the target $y$ by a function defined as a linear combination of the input features.\n",
        "\n",
        "In order to fit $\\mathbf{w}$, we need to define a **cost** or **loss** function, that penalizes how much error our model makes when estimating each data in our **training** set. For this purpose, the linear regression model  uses the **average training squared error** :\n",
        "\n",
        "$$J_{train}(\\mathbf{w}) = \\frac{1}{N} \\sum_{i=1}^{N} (y^{(i)}-\\mathbf{w}^T\\mathbf{x}_e^{(i)})^2$$\n",
        "\n",
        "Thus, we have to find \n",
        "\n",
        "$$\\mathbf{w}^* = \\arg \\min_{\\mathbf{w}} \\frac{1}{N} \\sum_{i=1}^{N} (y^{(i)}-\\mathbf{w}^T\\mathbf{x}_e^{(i)})^2 $$\n",
        "\n",
        "Fortunately, regardless the dimension of $\\mathbf{w}$, this is a well-behaved problem that can be easily solved, since its solution is unique and presents a\n",
        "closed form, known as the **least-squares** or the **normal equation**:\n",
        "\n",
        "$$\\mathbf{w}^* = (\\mathbf{X}_e^T\\mathbf{X}_e)^{-1}\\mathbf{X}_e^T\\mathbf{y},$$\n",
        "where\n",
        "- ${\\mathbf{X}_e}_{N\\times (D+1)}$ is the extended version of the training data matrix, where the $i$-th row is $\\mathbf{x}_e^{(i)}= [1, x^{(i)}_1, \\ldots, x^{(i)}_D]$\n",
        "- $\\mathbf{y}_{(N\\times 1)}$ is the training target vector\n",
        "- $ (\\mathbf{X}_e^T\\mathbf{X}_e)^{-1}\\mathbf{X}_e$ is the [Moore–Penrose inverse](http://mathworld.wolfram.com/Moore-PenroseMatrixInverse.html) of $\\mathbf{X}_e$.\n",
        "\n",
        "The solution of this problem is unique due to the function $J_{train}(\\mathbf{w})$ is [**convex**](http://mathworld.wolfram.com/ConvexFunction.html) w.r.t. $\\mathbf{w}$, and thus it has a **unique minimum**.\n",
        "\n",
        "---\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q4Ni6mZfP4fD",
        "colab_type": "text"
      },
      "source": [
        "### Exercise 1\n",
        "\n",
        "Demonstrate that the solution to the problem \n",
        "\n",
        "$$\\mathbf{w}^* = \\arg \\min_{\\mathbf{w}} \\frac{1}{N} \\sum_{i=1}^{N} (y^{(i)}-\\mathbf{w}^T\\mathbf{x}_e^{(i)})^2 $$\n",
        "\n",
        "\n",
        "is given by  \n",
        "\n",
        "$$\\mathbf{w}^* = (\\mathbf{X}_e^T\\mathbf{X}_e)^{-1}\\mathbf{X}_e^T\\mathbf{y},$$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GkTB7GtHD_zh",
        "colab_type": "text"
      },
      "source": [
        "### SOLUTION\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4eqGX3pgHYr7",
        "colab_type": "text"
      },
      "source": [
        "#### <SOL>\n",

        "#### </SOL>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "p9g5xkWURycE",
        "colab_type": "text"
      },
      "source": [
        "### Analize the loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IFvv5IhgFfzm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from mpl_toolkits.mplot3d import Axes3D\n",
        "from matplotlib import cm\n",
        "from matplotlib.ticker import LinearLocator, FormatStrFormatter\n",
        "\n",
        "def cost_function(w_0,w_1,y,x):\n",
        "    J = np.mean((np.tile(y,(w_0.shape[0],1)).T-(x[:,0][:, np.newaxis]*w_0[:, np.newaxis].T+x[:,1][:, np.newaxis]*w_1[:, np.newaxis].T))**2,axis=0)\n",
        "    return J\n",
        "\n",
        "\n",
        "fig = plt.figure(figsize=(20,10))\n",
        "ax = fig.gca(projection='3d')\n",
        "\n",
        "# Make data.\n",
        "w_0 = np.arange(np.min(Y_train), np.max(Y_train), (np.max(Y_train)-np.min(Y_train))/100)\n",
        "w_1 = np.arange(-200, 200, 2.5)\n",
        "w_0, w_1 = np.meshgrid(w_0, w_1)\n",
        "J = cost_function(w_0.ravel(),w_1.ravel(),Y_train, np.concatenate((np.ones((X_train_BMI.shape[0],1)), X_train_BMI) , axis=1))\n",
        "# Plot the surface.\n",
        "surf = ax.plot_surface(w_0, w_1, J.reshape(w_0.shape), cmap=cm.coolwarm,\n",
        "                       linewidth=0, antialiased=False)\n",
        "\n",
        "\n",
        "# Add a color bar which maps values to colors.\n",
        "fig.colorbar(surf, shrink=0.5, aspect=5)\n",
        "\n",
        "plt.show()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fx0xJW9fH62i",
        "colab_type": "text"
      },
      "source": [
        "### Preparing the data\n",
        "\n",
        "First of all, we have to create the extended versions of\n",
        "training and test data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GfW97q9NnVLf",
        "colab_type": "text"
      },
      "source": [
        "### Exercise 2.1\n",
        "\n",
        "Create an extended training and test matrix by adding a first columns of ones. Call these new matrix as 'X_train_e' and 'X_test_e'. You can use the np.concatenate() and np.ones() methods.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1qQhSoGun7X3",
        "colab_type": "text"
      },
      "source": [
        "### SOLUTION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oy0dA5FPFf81",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# <SOL>\n",

        "# </SOL>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J0WDHx7rTdZ0",
        "colab_type": "text"
      },
      "source": [
        "## Compute LS solution\n",
        "\n",
        "As we have seen, the LS solution is given by:\n",
        "\n",
        "$$\\mathbf{w}^* = (\\mathbf{X}_e^T\\mathbf{X}_e)^{-1}\\mathbf{X}_e^T\\mathbf{y},$$\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O91_aYxwoBnN",
        "colab_type": "text"
      },
      "source": [
        "### Exercise 2.2\n",
        "\n",
        "Obtain the vector weigths, 'w_start', of the LS solution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wXdJXCKKoc8r",
        "colab_type": "text"
      },
      "source": [
        "### SOLUTION"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yv3e8HocoYeF",
        "colab_type": "text"
      },
      "source": [
        "# <SOL>\n",

        "# </SOL>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8bE5t73tFfxh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# <SOL>\n",

        "# </SOL>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YQIrN93DJ_mG",
        "colab_type": "text"
      },
      "source": [
        "# <SOL>\n",

        "# </SOL>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QnSSrmxLwStB",
        "colab_type": "text"
      },
      "source": [
        "### Exercise 2.3 \n",
        "\n",
        "Finally, using the trained model, evaluate its performance over the test data and plot the regression function."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OSBakrFGoyoQ",
        "colab_type": "text"
      },
      "source": [
        "### SOLUTION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HQdf-aAJblfz",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# <SOL>\n",

        "# </SOL>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9c2nbtoDFfwX",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# <SOL>\n",

        "# </SOL>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YqkPXJHS8JlG",
        "colab_type": "text"
      },
      "source": [
        "## Analyze the LS solution \n",
        "\n",
        "How can we improve this model? Transform the input features...."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iFqE4uXGH6rB",
        "colab_type": "text"
      },
      "source": [
        "# Least Squares Solution with Polynomial Features\n",
        "\n",
        "**Features can be a non-linear transformation of a given input!**\n",
        "\n",
        "Note that the above model **does not** mean that we cannot use a linear regression model to interpolate a non-linear function w.r.t. to a given input. For instance, we can try to predict disease progression using the variable BMI (Body mass index) using the following set of **features**:\n",
        "\n",
        "$$\\mathbf{x} = [1,BMI ,BMI ^2,BMI ^3,\\ldots,BMI ^D],$$ \n",
        "\n",
        "which assumes we are approximating the function $f(BMI)$ with an $D$-th order polynomial. Once the features are defined, then note that our goal is to find $\\mathbf{w}\\in\\mathbb{R}^{D+1}$ such that\n",
        "$$ f(\\mathbf{x}) = \\mathbf{w}^T\\mathbf{x},$$\n",
        "and this is still a **linear regression model**.\n",
        "\n",
        "\n",
        "____"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S_lMaamU8JlV",
        "colab_type": "text"
      },
      "source": [
        "Lets go back to our running example, estimating the disease progression using the BMI value.  We will use polynomials with different order to visualize the solution. \n",
        "\n",
        "First, we use a $D$ order polynomial. Thus, each feature vector is \n",
        "\n",
        "$$\\mathbf{x} = [1,BMI,BMI^2, \\ldots, BMI^D]$$\n",
        "\n",
        "Note that although we use either the normalized or the non-normalized value of $BMI$, after computing the polynomial terms ($BMI^2$, ...), we normalize all new features again! Try to think about why is that ..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8IMyilxw0Vr-",
        "colab_type": "text"
      },
      "source": [
        "### Exercise 3\n",
        "\n",
        "Next cell implements the function learnPolyLR(), which trains a polynomial regression model, and it returns the model weights together to its training and test MSE.\n",
        "\n",
        "Check as this function let's you include the polynomial degree and, by means of the sklearn [PolynomialFeatures()](https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.PolynomialFeatures.html) function, it  obtains the polynomial extension of a data matrix. Then, normalize the data and trains the model. Finally, let you plot the regression funcion (this is only working for unidimensional input data).\n",
        "\n",
        "Use this function to train a polynomial model of degree 2, 5 and 10 using as input the feature #2 (BMI)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XtV31W3pAQQf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "\n",
        "def learnPolyLR(X_train, Y_train, X_test, Y_test, n_degree, addPlot=True):\n",
        "  \n",
        "  # Create polynomial version of training and test data (it includes the bias)\n",
        "  polynomial_features = PolynomialFeatures(degree=n_degree,include_bias=True)\n",
        "  \n",
        "  X_train_pol = polynomial_features.fit_transform(X_train)\n",
        "  X_test_pol = polynomial_features.transform(X_test)\n",
        "\n",
        "\n",
        "  # Data normalization\n",
        "  transformer = StandardScaler().fit(X_train_pol[:,1:])  # fit() learns mean and std\n",
        "  # The first column is the vector of ones, we do not normalize it!!!\n",
        "  X_train_pol[:,1:] = transformer.transform(X_train_pol[:,1:])\n",
        "  X_test_pol[:,1:] =  transformer.transform(X_test_pol[:,1:])\n",
        "\n",
        "  # Compute LS solution\n",
        "  w_star = np.linalg.lstsq(X_train_pol, Y_train, rcond=None )[0]\n",
        "  \n",
        "  # Evaluate train error\n",
        "  f_tr =X_train_pol @ w_star\n",
        "  MSE_train = np.mean((Y_train-f_tr)**2)\n",
        "  \n",
        "  # Evaluate test error\n",
        "  f_test=X_test_pol @ w_star\n",
        "  MSE_test = np.mean((Y_test-f_test)**2)\n",
        "  \n",
        "  if addPlot:\n",
        "    X_min = np.min(X_train)\n",
        "    X_max = np.max(X_train)\n",
        "    X_plot = np.arange (X_min,X_max,0.001)[:,np.newaxis] # This is not data! Just to visualize the polynomial curve\n",
        "\n",
        "    X_plot_pol = polynomial_features.transform(X_plot)\n",
        "    \n",
        "    # Data normalization\n",
        "    X_plot_pol[:,1:] =  transformer.transform(X_plot_pol[:,1:])\n",
        "\n",
        "    # Compute the regression curve output\n",
        "    f_plot= X_plot_pol @ w_star\n",
        "\n",
        "    # Plot the regression polynomial\n",
        "    plt.figure()\n",
        "    plt.plot(X_plot,f_plot)\n",
        "    plt.plot(X_train, Y_train, '.r')\n",
        "    plt.ylim(-400,400)\n",
        "\n",
        "  return w_star, MSE_train, MSE_test\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Efr_bQ2G1wHN",
        "colab_type": "text"
      },
      "source": [
        "#### SOLUTION\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1QYagLZF8JlW",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# <SOL>\n",

        "# </SOL>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQ66KsIQYh29",
        "colab_type": "text"
      },
      "source": [
        "# Learning curves\n",
        "\n",
        "Learning curves analyze the performance evaluation according the number of training data is increased and they are a very powerfull and informative methodology to understand if your regression model is exposed to **overfitting** or is too simple, i.e., it is **too biased**.\n",
        "\n",
        "To plot the learning curves for a regression model with $N$ training points and $Ntest$ test points, we plot the error and test averagre MSE for different **subset sizes of training points**. More precisely, for $N_0\\leq N$, we compute:\n",
        "$\\mathbf{w}^*$\n",
        "$$J_{train}(\\mathbf{w}^*,N_0) = \\frac{1}{N_0} \\sum_{i=1}^{N_0} (y^{(i)}-{\\mathbf{w}^*}^T\\overline{\\mathbf{x}}^{(i)})^2$$\n",
        "and\n",
        "$$J_{test}(\\mathbf{w}^*) = \\frac{1}{N_{Test}} \\sum_{i=1}^{N_{Test}} (y^{(i)}-{\\mathbf{w}^*}^T\\tilde{\\mathbf{x}}^{(i)})^2$$\n",
        "where $\\mathbf{w}^*$ is the LS solution for a subset of training points of size $N_0\\leq N$. In general, both quantities above **must** be averaged for different subsets.\n",
        "\n",
        "Now, let's plot the learning curves for the above polynomial regression model using different polynomial degrees to understand what they mean."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43cIWX3OY1Hl",
        "colab_type": "text"
      },
      "source": [
        "## Model 1: An example of bias\n",
        "\n",
        "Imagine we use a polynomial of degree 1, i.e., a straight line to estimate the disease evolution from the BMI. Lets compute and plot the learning curves."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "06TzEBClYctU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_degree = 1\n",
        "n_iter = 20\n",
        "\n",
        "N = X_train.shape[0]\n",
        "N0_values = np.arange(10, N, 10)\n",
        "\n",
        "MSE_train_all = np.zeros((n_iter,N0_values.shape[0])) \n",
        "MSE_test_all = np.zeros((n_iter,N0_values.shape[0]))\n",
        "\n",
        "for n, N0 in enumerate(N0_values): \n",
        "  for iter in range(n_iter):\n",
        "    pos_rand = np.random.permutation(N)\n",
        "    X_train_N0= X_train_BMI[pos_rand[:N0],:]\n",
        "    Y_train_N0= Y_train[pos_rand[:N0]]\n",
        "    w_star, MSE_train, MSE_test = learnPolyLR(X_train_N0, Y_train_N0, X_test_BMI, Y_test, n_degree, addPlot=False)\n",
        "    MSE_train_all [iter, n] = MSE_train\n",
        "    MSE_test_all [iter, n] = MSE_test\n",
        "    \n",
        "# Average the results\n",
        "MSE_train1 = np.mean(MSE_train_all, axis=0)\n",
        "MSE_test1 = np.mean(MSE_test_all, axis=0)\n",
        "\n",
        "# plot the result\n",
        "plt.semilogy(N0_values,MSE_train1 ,'b-o',ms=10,label='MSE_train (degree 1)')\n",
        "plt.semilogy(N0_values,MSE_test1 ,'b--<',ms=10,label='MSE_test (degree 1)')\n",
        "plt.xlabel('Number of training points')\n",
        "plt.ylabel('Mean Squared Error (MSE)')\n",
        "plt.title('Model polynomial order regression  = %d' %(n_degree))\n",
        "plt.legend()\n",
        "plt.rcParams[\"figure.figsize\"] = [12,4]\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eMrOTmOKY_l6",
        "colab_type": "text"
      },
      "source": [
        "## Model 2: An example of mild overfitting\n",
        "\n",
        "Imagine now we use a polynomial of degree 5. Lets compute and plot the learning curves."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PZYBCPQEYcnm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_degree = 5\n",
        "n_iter = 20\n",
        "\n",
        "N = X_train.shape[0]\n",
        "N0_values = np.arange(10, N, 10)\n",
        "\n",
        "MSE_train_all = np.zeros((n_iter,N0_values.shape[0])) \n",
        "MSE_test_all = np.zeros((n_iter,N0_values.shape[0]))\n",
        "\n",
        "for n, N0 in enumerate(N0_values): \n",
        "  for iter in range(n_iter):\n",
        "    pos_rand = np.random.permutation(N)\n",
        "    X_train_N0= X_train_BMI[pos_rand[:N0],:]\n",
        "    Y_train_N0= Y_train[pos_rand[:N0]]\n",
        "    w_star, MSE_train, MSE_test = learnPolyLR(X_train_N0, Y_train_N0, X_test_BMI, Y_test, n_degree, addPlot=False)\n",
        "    MSE_train_all [iter, n] = MSE_train\n",
        "    MSE_test_all [iter, n] = MSE_test\n",
        "    \n",
        "# Average the results\n",
        "MSE_train5 = np.mean(MSE_train_all, axis=0)\n",
        "MSE_test5 = np.mean(MSE_test_all, axis=0)\n",
        "\n",
        "# plot the result\n",
        "plt.semilogy(N0_values,MSE_train1 ,'b-o',ms=10,label='MSE_train (degree 1)')\n",
        "plt.semilogy(N0_values,MSE_test1 ,'b--<',ms=10,label='MSE_test (degree 1)' )\n",
        "plt.semilogy(N0_values,MSE_train5 ,'r-o',ms=10,label='MSE_train (degree 5)')\n",
        "plt.semilogy(N0_values,MSE_test5 ,'r--<',ms=10,label='MSE_test (degree 5)')\n",
        "plt.xlabel('Number of training points')\n",
        "plt.ylabel('Mean Squared Error (MSE)')\n",
        "plt.title('Model polynomial order regression  = %d' %(n_degree))\n",
        "plt.legend()\n",
        "plt.rcParams[\"figure.figsize\"] = [12,4]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BhA4E7H0ZHvq",
        "colab_type": "text"
      },
      "source": [
        "## Model 3: An example of extreme overfitting\n",
        "\n",
        "Imagine now we use a polynomial of degree 40. Lets compute and plot the learning curves."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s9MY__aHYcgw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "n_degree = 10\n",
        "n_iter = 20\n",
        "\n",
        "N = X_train.shape[0]\n",
        "N0_values = np.arange(10, N, 10)\n",
        "\n",
        "MSE_train_all = np.zeros((n_iter,N0_values.shape[0])) \n",
        "MSE_test_all = np.zeros((n_iter,N0_values.shape[0]))\n",
        "\n",
        "for n, N0 in enumerate(N0_values): \n",
        "  for iter in range(n_iter):\n",
        "    pos_rand = np.random.permutation(N)\n",
        "    X_train_N0= X_train_BMI[pos_rand[:N0],:]\n",
        "    Y_train_N0= Y_train[pos_rand[:N0]]\n",
        "    w_star, MSE_train, MSE_test = learnPolyLR(X_train_N0, Y_train_N0, X_test_BMI, Y_test, n_degree, addPlot=False)\n",
        "    MSE_train_all [iter, n] = MSE_train\n",
        "    MSE_test_all [iter, n] = MSE_test\n",
        "    \n",
        "# Average the results\n",
        "MSE_train10 = np.mean(MSE_train_all, axis=0)\n",
        "MSE_test10 = np.mean(MSE_test_all, axis=0)\n",
        "\n",
        "# plot the result\n",
        "plt.semilogy(N0_values,MSE_train1 ,'b-o',ms=10,label='MSE_train (degree 1)')\n",
        "plt.semilogy(N0_values,MSE_test1 ,'b--<',ms=10,label='MSE_test (degree 1)')\n",
        "plt.semilogy(N0_values,MSE_train5 ,'r-o',ms=10,label='MSE_train (degree 5)')\n",
        "plt.semilogy(N0_values,MSE_test5 ,'r--<',ms=10,label='MSE_test (degree 5)')\n",
        "plt.semilogy(N0_values,MSE_train10 ,'g-o',ms=10,label='MSE_train (degree 10)')\n",
        "plt.semilogy(N0_values,MSE_test10 ,'g--<',ms=10,label='MSE_test (degree 10)')\n",
        "plt.xlabel('Number of training points')\n",
        "plt.ylabel('Mean Squared Error (MSE)')\n",
        "plt.title('Model polynomial order regression  = %d' %(n_degree))\n",
        "plt.legend()\n",
        "plt.rcParams[\"figure.figsize\"] = [12,4]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gaXh9fzfqOXb",
        "colab_type": "text"
      },
      "source": [
        "What do you conclude from the plot above? How does the model improve as we increase the number of training points? \n",
        "\n",
        "**Conclusion: We can reduce the overfitting problem increasing the number of training data**\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ljh4yKkFZvjb",
        "colab_type": "text"
      },
      "source": [
        "# Model regularization\n",
        "\n",
        "We have seen that complex models tend to **overfit** the data **unless we are able to increase the number of training points**, which is many times not an option.\n",
        "\n",
        "However, if we analyze the weights of overfitted models, we can realize that they tend to show regression coefficients with very large absolute values. Examine the output of the following cell..."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kC2jYkyYYceG",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "deg_list = [1,5,10,15,20,25]\n",
        "\n",
        "plt.figure()\n",
        "\n",
        "for i,deg in enumerate(deg_list):\n",
        "  \n",
        "    w_star, MSE_train, MSE_test= learnPolyLR(X_train_BMI, Y_train, X_test_BMI, Y_test, deg)\n",
        "    \n",
        "    plt.subplot(2,3,i+1)\n",
        "    \n",
        "    plt.stem(w_star)\n",
        "    plt.title('Degree % d, MSEtest = %2.0f' %(deg,MSE_test))\n",
        "    \n",
        "plt.show()   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5EilQDJAwAgf",
        "colab_type": "text"
      },
      "source": [
        "In model regularization, we combine the flexibility of a **complex model** with a **penalization function** that prevents the model to converge to solutions with very large coefficients.\n",
        "\n",
        "The easiest regularization example is the $L_2$ penalization or **Ridge regression**:\n",
        "\n",
        "$$\\mathbf{w}_\\lambda = \\arg \\min_{\\mathbf{w}} \\frac{1}{N} \\left[\\sum_{i=1}^{N} (y^{(i)}-\\mathbf{w}^T\\mathbf{x}^{(i)})^2 + \\lambda \\sum_{j=1}^{D} {w}_j^2\\right],$$\n",
        "\n",
        "where note we do not penalize large values of the intercept $w_0$, since it essentially captures the mean of $y$. Also, $\\lambda$ is called the **regularization parameter** and it is another parameter that **we have to learn from our data**, looking for right balance between **explaining our training set** and **the penalization term**.\n",
        "\n",
        "Fortunately, the $L2$ penalization norm does not change the convexity of the loss function and, indeed, we still have a **closed-form** solution\n",
        "\n",
        "$$\\mathbf{w}_\\lambda = (\\mathbf{X}^T\\mathbf{X} + \\lambda \\mathbf{I}_{D\\times D})^{-1}\\mathbf{X}^T\\mathbf{y},$$\n",
        "\n",
        "Note that the term $\\lambda \\mathbf{I}_{D\\times D}$ ensures in general that the matrix $\\mathbf{X}^T\\mathbf{X} + \\lambda \\mathbf{I}$ is invertible, leading to much better condition solutions.\n",
        "\n",
        "In any case, scikit-learn has a function ([linear_model.Ridge](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Ridge.html#sklearn.linear_model.Ridge)) that allows us to do the training and evaluation of a ridge regression model. \n",
        "\n",
        "Next cell has adapted the previous function (learnPolyLR) to train a polynomial model for the ridge regression case.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LXcVS8b_YcZc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import Ridge\n",
        "\n",
        "def learnPolyLR_L2(X_train, Y_train, X_test, Y_test, n_degree, lamb):\n",
        "\n",
        "\n",
        "  # Select id_feat and create polynomial version of training and test data\n",
        "  Ntr = X_train.shape[0]\n",
        "  Ntst = X_test.shape[0]\n",
        "\n",
        "  # We are not using the columns of ones anymore\n",
        "  polynomial_features = PolynomialFeatures(degree=n_degree, include_bias=False)\n",
        "  \n",
        "  X_train_pol = polynomial_features.fit_transform(X_train)\n",
        "  X_test_pol = polynomial_features.transform(X_test)\n",
        "\n",
        "\n",
        "  # Data normalization\n",
        "  transformer = StandardScaler().fit(X_train_pol)  # fit() learns mean and std\n",
        " \n",
        "  X_train_pol = transformer.transform(X_train_pol)\n",
        "  X_test_pol =  transformer.transform(X_test_pol)\n",
        "\n",
        " \n",
        "  # Compute Ridge Regression solution\n",
        "  clf = Ridge(alpha=lamb, fit_intercept=True) # We add the intercept here (the columns of ones)\n",
        "  clf.fit(X_train_pol, Y_train) \n",
        "  \n",
        "  # Save weight values\n",
        "  w_star = np.zeros((X_train_pol.shape[1]+1,))\n",
        "  w_star[0] = clf.intercept_\n",
        "  w_star[1:] = clf.coef_\n",
        "  \n",
        " \n",
        "  # Evaluate train error\n",
        "  f_tr = clf.predict(X_train_pol)\n",
        "  MSE_train = np.mean((Y_train-f_tr)**2)\n",
        "  \n",
        "  \n",
        "  # Evaluate test error\n",
        "  f_test= clf.predict(X_test_pol)\n",
        "  MSE_test = np.mean((Y_test-f_test)**2)\n",
        "\n",
        "  return w_star, MSE_train, MSE_test\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYXELeiq6d7u",
        "colab_type": "text"
      },
      "source": [
        "### Exercise 4\n",
        "\n",
        "Use the above function to train a polynomial ridge regression model (degree 10) and analyze its performance and the regression coefficient values for the following range of $\\lambda$ values:\n",
        " $\\lambda = [1, 5, 10, 50, 100, 500]$. You can try to plot the train and test MSE, as well as the weight evolution for different values of $\\lambda$.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZiZ8XfGEcF4",
        "colab_type": "text"
      },
      "source": [
        "#### SOLUTION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uSg7r-o7YcUH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# <SOL>\n",

        "# </SOL>  \n",
        "  "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1cCMxFDkys-M",
        "colab_type": "text"
      },
      "source": [
        "Note that the final result depends on the value of $\\lambda$ that we are using. How can we select its optimum value?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GHS-Ubn34G-N",
        "colab_type": "text"
      },
      "source": [
        "# The Lasso\n",
        "\n",
        "The lasso is a shrinkage method like ridge, but the L2 ridge penalty is replaced by the L1 penalty (known as LASSO, Least Absolute Shrinkage and Selection Operator). So, the Lasso model is defined by the following optimization problem\n",
        "\n",
        "\n",
        "$$\\mathbf{w}_\\lambda = \\arg \\min_{\\mathbf{w}} \\frac{1}{N} \\left[\\sum_{i=1}^{N} (y^{(i)}-\\mathbf{w}^T\\mathbf{x}^{(i)})^2 + \\lambda \\sum_{j=1}^{D} | w_j|\\right],$$\n",
        "\n",
        "This latter constraint makes the solutions nonlinear in $\\mathbf{w}$ (the absolute value has a discontinous derivative), and there is no closed form expression as in ridge regression (even, unique minimum is no more guarantee). However, we can compute the lasso solution using the sklearn implementation [linear_model.Lasso](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.Lasso.html#sklearn.linear_model.Lasso) which is based in [stochastic gradient descent algorithm](https://en.wikipedia.org/wiki/Stochastic_gradient_descent).\n",
        "\n",
        "Check how next cell has now adapted the learnPolyLR function to train a polynomial model for the lasso regression case.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nkKnj-Qs-GH1",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn.linear_model import Lasso\n",
        "\n",
        "def learnPolyLR_L1(X_train, Y_train, X_test, Y_test, n_degree, lamb):\n",
        "\n",
        "\n",
        "  # Select id_feat and create polynomial version of training and test data\n",
        "  Ntr = X_train.shape[0]\n",
        "  Ntst = X_test.shape[0]\n",
        "\n",
        "  # We are not using the columns of ones anymore\n",
        "  polynomial_features = PolynomialFeatures(degree=n_degree, include_bias=False)\n",
        "  \n",
        "  X_train_pol = polynomial_features.fit_transform(X_train)\n",
        "  X_test_pol = polynomial_features.transform(X_test)\n",
        "\n",
        "\n",
        "  # Data normalization\n",
        "  transformer = StandardScaler().fit(X_train_pol)  # fit() learns mean and std\n",
        " \n",
        "  X_train_pol = transformer.transform(X_train_pol)\n",
        "  X_test_pol =  transformer.transform(X_test_pol)\n",
        "\n",
        " \n",
        "  # Compute Lasso Regression solution\n",
        "  clf = Lasso(alpha=lamb, fit_intercept=True) # We add the intercept here (the columns of ones)\n",
        "  clf.fit(X_train_pol, Y_train) \n",
        "  \n",
        "  # Save weight values\n",
        "  w_star = np.zeros((X_train_pol.shape[1]+1,))\n",
        "  w_star[0] = clf.intercept_\n",
        "  w_star[1:] = clf.coef_\n",
        "  \n",
        " \n",
        "  # Evaluate train error\n",
        "  f_tr = clf.predict(X_train_pol)\n",
        "  MSE_train = np.mean((Y_train-f_tr)**2)\n",
        "  \n",
        "  \n",
        "  # Evaluate test error\n",
        "  f_test= clf.predict(X_test_pol)\n",
        "  MSE_test = np.mean((Y_test-f_test)**2)\n",
        "\n",
        "  return w_star, MSE_train, MSE_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eeQprk-vfIdR",
        "colab_type": "text"
      },
      "source": [
        "### Exercise 5\n",
        "\n",
        "Repeat Exercise 1, but training a lasso regression model instead of the ridge one. Analyze again the train and test MSE evolution and the weight values. \n",
        "For this case use the $\\lambda$ value range:\n",
        " $\\lambda = [0.001,0.005, 0.01,0.05,0.1, 0.5,1, 5, 10, 50, 100]$"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "67KCkTSWtboD",
        "colab_type": "text"
      },
      "source": [
        "#### SOLUTION"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1P2hUWNjfOmF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# <SOL>\n",

        "# </SOL>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lNl7Ncmk-I4h",
        "colab_type": "text"
      },
      "source": [
        "Note that this constraint has an interesting property, since making $\\lambda$ sufficiently large we can cause some of the coefficients drop to zero. Thus, the L1 regularization does a kind of feature selection. For this reason, this regularization is known as LASSO (Least Absolute Shrinkage and Selection Operator).\n",
        "\n",
        "At the light of this result, if you could only use 5 of the 10 polynomial terms, which would you choose?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "T_hSKxQs4GRw",
        "colab_type": "text"
      },
      "source": [
        "## More on L2 and L1 regularization\n",
        "\n",
        "Both Ridge  and Lasso regression problems can be reformulated as a least squares optimization problem plus a constraint, that is,\n",
        "\n",
        "* **Ridge regression**:\n",
        "\n",
        "$$\\mathbf{w}^* = \\arg \\min_{\\mathbf{w}} \\frac{1}{N} \\sum_{i=1}^{N} (y^{(i)}-\\mathbf{w}^T\\mathbf{x}^{(i)})^2 $$\n",
        "$$ {\\rm s.t.} \\sum_{j=1}^{D} {w}_j^2 \\leq t$$\n",
        "\n",
        "* **Lasso regression**:\n",
        "\n",
        "\n",
        "$$\\mathbf{w}^*= \\arg \\min_{\\mathbf{w}} \\frac{1}{N} \\sum_{i=1}^{N} (y^{(i)}-\\mathbf{w}^T\\mathbf{x}^{(i)})^2 $$\n",
        "$$ {\\rm s.t.} \\sum_{j=1}^{D} | w_j| \\leq t$$\n",
        "\n",
        "In this way we can see that these problems are really minimizing the mean squared error and, at the same time, they are forcing the values of $\\mathbf{w}$ to be within the region defined by the constraint. In the case of L2, this constraint forces $\\mathbf{w}$ to fall inside a sphere of radius $\\sqrt{t}$ and in the case of L1 this region is a diamond of side $\\sqrt{2}t$. Next figure (from [1]) depicts these optimization problems for a two dimensional problem (the lasso on the left and ridge regression on the right).\n",
        "\n",
        "<img src=\"http://www.tsc.uc3m.es/~vanessa/Figs_notebooks/ML/Regression/vL1_l2.png\" width=\"50%\" > \n",
        "\n",
        "Note that both methods find the optimal solution where the elliptical contours (mean squared error) hit the constraint region. However, the lasso constraint has corners and when the solution falls into a corner, the value of $w_j$ becomes zero. When there are many input dimensions, the diamond becomes a rhomboid with many corners, flat edges and faces, increasing the chances for the estimated parameters to be zero.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pZQM0UBF-wSW",
        "colab_type": "text"
      },
      "source": [
        "# The Elastic-Net\n",
        "\n",
        "This is another regularized linear model that emerges as a combination of  L1 and L2 penaties:\n",
        "\n",
        "\n",
        "$$\\mathbf{w}_\\lambda = \\arg \\min_{\\mathbf{w}} \\frac{1}{N} \\left[\\sum_{i=1}^{N} (y^{(i)}-\\mathbf{w}^T\\mathbf{x}^{(i)})^2 + \\lambda_1\\sum_{j=1}^{D+1} | w_j|+ \\lambda_2\\sum_{j=1}^{D+1} w_j^2\\right],$$\n",
        "\n",
        "If you want to try this model, you can check the sklearn implementantion:  [linear_model.ElasticNet](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.ElasticNet.html#sklearn.linear_model.ElasticNet)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GxUSH8R9-wOW",
        "colab_type": "text"
      },
      "source": [
        "# Some useful references:\n",
        "\n",
        "[1] C. M. Bishop, [Pattern Recognition and Machine Learning](http://users.isr.ist.utl.pt/~wurmd/Livros/school/Bishop%20-%20Pattern%20Recognition%20And%20Machine%20Learning%20-%20Springer%20%202006.pdf), Chapter 3: Linear Models for regression. Springer, 2006.\n",
        "\n",
        "\n",
        "More on L1 and L2 regularization:\n",
        "* Regularización L1 vs L2:[videolecture](https://www.youtube.com/watch?v=sO4ZirJh9ds)\n",
        "* [Ordinary Least Squares and Ridge Regression Variance](https://scikit-learn.org/stable/auto_examples/linear_model/plot_ols_ridge_variance.html#sphx-glr-auto-examples-linear-model-plot-ols-ridge-variance-py)\n",
        "* Compute [elastic net path with coordinate descent](https://scikit-learn.org/stable/auto_examples/linear_model/plot_lasso_coordinate_descent_path.html#sphx-glr-auto-examples-linear-model-plot-lasso-coordinate-descent-path-py)\n",
        "* [LARS](https://scikit-learn.org/stable/modules/linear_model.html#least-angle-regression  \n",
        "https://scikit-learn.org/stable/auto_examples/linear_model/plot_lasso_lars.html#sphx-glr-auto-examples-linear-model-plot-lasso-lars-py)\n",
        "\n",
        "* [Huber loss](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.HuberRegressor.html#sklearn.linear_model.HuberRegressor)"
      ]
    }
  ]
}