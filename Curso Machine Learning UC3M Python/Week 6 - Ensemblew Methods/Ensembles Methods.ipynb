{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Homework2:Ensembles.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6eAsdbmD4uOX",
        "colab_type": "text"
      },
      "source": [
        "# Week 6: Homework 2 \n",
        "\n",
        "----------------------------------------------------\n",
        "Machine Learning                      \n",
        "\n",
        "Year 2019/2020\n",
        "\n",
        "*Vanessa Gómez Verdejo vanessa@tsc.uc3m.es* \n",
        "\n",
        "----------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xGWmrlvljmmv",
        "colab_type": "text"
      },
      "source": [
        "#1. Introduction to ensembles\n",
        "\n",
        "The goal of ensemble learning is to combine a set of base learners to build an improved prediction model. The key idea  behind ensembles lies in exploiting the diversity among the base learners; the way of generating this diversity let us classify these methods into two main types:\n",
        "\n",
        "* **Bagging**: the diversity  among classifiers is generated using different partitions of the training data.\n",
        "* **Boosting**: it sequentially train a set of weak classifiers using modified versions of the data.\n",
        "\n",
        "We have already seen a kind of these methods: *Random Forests (RF)*. Remember that RF train a set of trees, each tree use a different subset of samples and features, and later combine their outputs. So, we can say that RF is a Bagging method. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QLCRyCKjH0r6",
        "colab_type": "text"
      },
      "source": [
        "# 2. Bagging: Boostrap Aggregating\n",
        "\n",
        "A bagging ensemble combines a set of classiffiers where each base learner is trained with a bagged subset of the training samples. This bagging subsampling consist in randomly, and with replacement,  choosing multiple random samples from the original training data. \n",
        "\n",
        "Once the set of base learners is trained, the final ensemble output is obtained by averaging all learner's outputs. In the classification case, usually a majority vote is applied.\n",
        "\n",
        "<img src=\"http://www.tsc.uc3m.es/~vanessa/Figs_notebooks/ML/Ensembles/Bagging.png\" width=\"48%\" > \n",
        "\n",
        "\n",
        "(*) Figure from https://medium.com/@rrfd/boosting-bagging-and-stacking-ensemble-methods-with-sklearn-and-mlens-a455c0c982de\n",
        "\n",
        "\n",
        "**References**\n",
        "\n",
        "L. Breiman, “Bagging predictors”, Machine Learning, 24(2), 123-140, 1996.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "blmHK6yPH3Wn",
        "colab_type": "text"
      },
      "source": [
        "\n",
        "\n",
        "# 3. Boosting: Real Adaboost\n",
        "\n",
        "Boosting methods train a sequence of weak classifiers with weighted or emphasized versions of the training data. Each one of these classifiers is weak since its error rate can be only slightly better than random guessing. Finally, to obtain the final ensemble output, the predictions from all of them are then combined through a weighted combination of all learners' outputs.\n",
        "\n",
        "\n",
        "The most popular boosting algorithm is called **AdaBoost** (Adaptive Boosting). This boosting method trains this sequence of weak classifiers in such way that each new classifier pay more attention to samples missclassified by the previous learners. Versions of this algorithm are:\n",
        "* AdaBoost.M1 or  “Discrete AdaBoost” where base learners outputs are discrete estimations of the output class.\n",
        "* “Real AdaBoost”, in this case, base classifier return a real-valued prediction (e.g., a probability mapped to the interval [−1,1]).\n",
        "\n",
        "Let's now go deeper in the working principles of the **Real AdaBoost** algorithm.\n",
        "\n",
        "**References**\n",
        "\n",
        "* Schapire, R.E. The strength of weak learnability. Machine Learning, 5(2): 1651-1686, 1990.\n",
        "\n",
        "* Freund, Y. and Schapire, R.E. Experiments with a new boosting algorithm. Proc. of the 13th International Conference on Machine Learning. pp. 148-156, 1996.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y9xauQN8CSOp",
        "colab_type": "text"
      },
      "source": [
        "## Real Adaboost\n",
        "Consider we have a binary classification problem given by training dataset $S$ consisting  of $N$ pairs $(\\mathbf{x}^{(i)},y^{(i)})$, where $\\mathbf{x}^{(i)}\\in\\mathbb{R}^L$ is the $i$-th observation and $y^{(i)}\\in\\{-1,1\\}$  is its associated label.\n",
        "\n",
        "Real Adaboost (RA) sequentially trains a set of $T$ learners where each learner implements a prediction function $o_t(x) \\in [-1,1]$. To learn this prediction function each learner observes the overall training dataset $S$, but an emphasis function $D_t(\\mathbf{x})$ is used during its training to make the learner pay more attention to most erroneous samples.\n",
        "Finally, the ensemble output is obtained as an averaged weighted sum of all learner output:\n",
        "\n",
        "$$ f_T({\\bf x}) = \\displaystyle \\sum_{t=1}^T \\alpha_t o_t({\\bf x})$$\n",
        "\n",
        "<img src=\"http://www.tsc.uc3m.es/~vanessa/Figs_notebooks/ML/Ensembles/boosting.jpg\" width=\"30%\" > \n",
        "\n",
        "\n",
        "\n",
        "* **Emphasis function**:\n",
        "To force diversity among the base learners, by means of an emphasis function RA applies weights  to each of the training observations. Initially all weights are equal:\n",
        "$$ D_{1}({\\bf x}^{(i)}) = \\frac{1}{N}   $$\n",
        "\n",
        "  so that the first learner is trained in the usual manner. For next iterations,  $t = 2,3,\\ldots,T$, the emphasis function updated with the following rule:\n",
        "\n",
        "$$ D_{t+1}({\\bf x}^{(i)}) = \\frac{D_{t}({\\bf x}^{(i)}) \\exp \\left( - \\alpha_t o_t({\\bf x}^{(i)}) {y}^{(i)} \\right)}{Z_t}   $$\n",
        "\n",
        "  where $Z_t$ is a normalization constant making $\\sum_{i=1}^N D_{t+1}({\\bf x}^{(i)})  = 1$. Note that this update rule increase the emphasis weight for those observations that were misclassified by the previous classifiers, whereas the weights are decreased for those that were  correctly classified. Thus, as new learners are added to the ensemble, most erroneous samples will receive an increased attention.\n",
        "\n",
        "* **Output weights ($\\alpha_t$)**\n",
        "\n",
        "To obtain the output weights, RA minimizes the exponential loss function:\n",
        "$${\\bf \\alpha}^* =\\displaystyle \\underset{{\\bf \\alpha}}{\\operatorname{min}} \\sum_{i=1}^N \\exp \\left( - \\sum_{t=1}^T \\alpha_t o_t({\\bf x}^{(i)}) {y}^{(i)} \\right) = \\displaystyle \\underset{{\\bf \\alpha}}{\\operatorname{min}} \\sum_{i=1}^N \\exp \\left( - f_T({\\bf x}^{(i)}) {y}^{(i)} \\right)$$\n",
        "\n",
        "which solution can be analytically computed as\n",
        "\n",
        "$$\\alpha_t = \\frac{1}{2} \\ln \\left( \\frac{1+\\gamma_t}{1-\\gamma_t}\\right)$$\n",
        "\n",
        "where $\\gamma_t = \\sum_{i=1}^N   D_{t}({\\bf x}^{(i)}) o_t({\\bf x}^{(i)}) {y}^{(i)} $. The effect of these weights is to give higher influence to the more accurate classifiers in the ensemble. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MiLPEEKbbCEW",
        "colab_type": "text"
      },
      "source": [
        "**Exponential loss**\n",
        "\n",
        "This cost function, quite similar to the binomial deviance, is un upper bound of the classification error.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d6a-vbVEb_fr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPzqAQDVjoiZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Plot the exponential loss function (un upper bound of the classfication error)\n",
        "f = np.arange(-2,2,0.01)\n",
        "y = 1\n",
        "\n",
        "l_alpha = np.exp(-y*f)\n",
        "plt.figure()\n",
        "plt.plot(y*f,l_alpha, label='Exponential loss')\n",
        "\n",
        "\n",
        "# Compare with binomial deviance\n",
        "l_w = np.log(1+ np.exp(f))-y*f\n",
        "plt.plot(y*f,l_w, label='Binomial deviance')\n",
        "\n",
        "# Classification error\n",
        "e_class = np.zeros(f.shape)\n",
        "e_class[y*f<0] =1\n",
        "plt.plot(y*f,e_class, label='Classification error')\n",
        "\n",
        "plt.legend()\n",
        "plt.xlabel('y*f')\n",
        "plt.ylabel('Loss function')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgDm0Z5bcazI",
        "colab_type": "text"
      },
      "source": [
        "# 4. Let's work\n",
        "\n",
        "The aim of this second HW is to implement and analyse the performance of these enseble methods. To do this, we will work with the Breast Cancer database (described in the next section) and you will have to complete the following exercises."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpjaOvF7Ldhq",
        "colab_type": "text"
      },
      "source": [
        "### Exercise 1. Load and prepare the data\n",
        "\n",
        "For this lab session, let's work over the  [Breast cancer data set](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html) a binary classification problem aimed to detect breast cancer from a  digitized image of breast mass. For this prupose, the images have been preprocesed and characterized with 30 input features describing the mass.\n",
        "\n",
        "Complete next cell code, so that you can:\n",
        "* Load the dataset\n",
        "* Create training and testing partitions with the 60% and 40% of the original data\n",
        "* Normalize the data to zero mean and unitary standard deviation "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Mz8tfb9jr6x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# <SOL>\n",

        "# </SOL>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2szGWImcK10A",
        "colab_type": "text"
      },
      "source": [
        "### Exercise 2. Bagging methods\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JaGO7gugMxJK",
        "colab_type": "text"
      },
      "source": [
        "### Exercise 2.1\n",
        "Complete the following code to desing a function able to train and test a bagged ensemble of $T$ decision trees where each tree have to use a subset of *nperc* of the total number of training samples. Adjust the decision tree parameters so that the maximum depth is fixed to 2 (weak learner).\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iOewK-s1jr4E",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from sklearn import tree\n",
        "from sklearn.metrics import accuracy_score\n",
        "from scipy import stats\n",
        "\n",
        "\n",
        "def BaggEnsemble(X_train, Y_train, X_test, Y_test, T, nperc):\n",
        "  '''\n",
        "  Train and evaluate a bagged ensemble of decission trees\n",
        "\n",
        "    Args:\n",
        "        X_train(numpy dnarray): training (number training data x number dimensions). \n",
        "        Y_train (numpy dnarray): labels of the training data (number training data x 1).\n",
        "        X_test(numpy dnarray): test data to evaluate the ensemble performance (number test data x number dimensions).\n",
        "        Y_test (numpy dnarray): labels of the test data (number test data x 1).\n",
        "        T: number of learners in the ensemble  \n",
        "        nperc: subsampling rate for the bagging process. Value from 0 (none sample is used) to 1 (all samples are used)                                       \n",
        "   Returns:\n",
        "        acc_tree_train (numpy dnarray): accuracy over the training data for different number of learners. \n",
        "                It's a vector length T, where the t-th element is the ensemble accuracy when only t trees are used.\n",
        "        acc_tree_test (numpy dnarray): accuracy over the test data for different number of learners. \n",
        "                It's a vector length T, where the t-th element is the ensemble accuracy when only t trees are used.\n",
        "        Y_pred_train (numpy dnarray): predicted outputs over the training data. It's a matrix of dimensions T x number of training samples, \n",
        "          where the t-th row has the predicted outputs when t trees are used.\n",
        "        Y_pred_test (numpy dnarray): predicted outputs over the test data. It's a matrix of dimensions T x number of test samples, \n",
        "          where the t-th row has the predicted outputs when t trees are used.\n",
        "\n",
        "  '''\n",
        "  \n",
        "  # <SOL>\n",

        "  # </SOL>\n",
        "  return acc_tree_train, acc_tree_test, Y_pred_train, Y_pred_test"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i0TL6ZABd9ES",
        "colab_type": "text"
      },
      "source": [
        "### Exercise 2.2\n",
        "\n",
        "Train an ensemble of 40 trees with a subsamplig rate of 50% (T=40 and nperc = 0.5)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z_x-TDOgjr1R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# <SOL>\n",

        "# </SOL>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8qHKAOerp18u",
        "colab_type": "text"
      },
      "source": [
        "### Exercise 2.3\n",
        "\n",
        "Analyze the **diversity** among the base learner' outputs for different *nperc* rates. You can analyze this diversity by measuring the correlation among the learner's soft-outputs. Firstly, obtain the matrix with all pairwaise correlation values (over the training learners' outputs) and, then, compute the ensemble diversity as one minus the averaged value of all pairwise correlation values. Finally, analyze the results.\n",
        "\n",
        "Some useful functions: [np.corrcoef](https://docs.scipy.org/doc/numpy/reference/generated/numpy.corrcoef.html)\n",
        "\n",
        "Note: You can find other ensemble diversity measurements in https://lucykuncheva.co.uk/papers/lkml.pdf"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GabCBZFjrwD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# <SOL>\n",

        "# </SOL>\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ILkeQIyk1sm3",
        "colab_type": "text"
      },
      "source": [
        "### Exercise 2.4\n",
        "\n",
        "Compare the performace of the bagged ensemble trained in Exercise 2.1 with that of a Random Forest with the same number of learners, and using trees with a maximum depth of 2. Which differences are between both methods? \n",
        "\n",
        "Analyze the feature importances provided by the method RandomForestClassifier().\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I6KrlfF82coF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# <SOL>\n",

        "# </SOL>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5AQphHj5gW3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# <SOL>\n",

        "# </SOL>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQtVXIQyOAEP",
        "colab_type": "text"
      },
      "source": [
        "### Exercise 3.Boosting methods"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uE7XRoCt1Np",
        "colab_type": "text"
      },
      "source": [
        "### Exercise 3.1\n",
        "\n",
        "Complete the following cell code to desing a funtion able to train a boosting ensemble of $T$ decision trees. Again, fix the maximum depth of the tree to 2.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BpjHqOeluE6I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def RAEnsemble(X_train, Y_train, X_test, Y_test, T):\n",
        "  '''\n",
        "  Train and evaluate a bagged ensemble of decission trees\n",
        "\n",
        "    Args:\n",
        "        X_train(numpy dnarray): training (number training data x number dimensions). \n",
        "        Y_train (numpy dnarray): labels of the training data (number training data x 1).\n",
        "        X_test(numpy dnarray): test data to evaluate the ensemble performance (number test data x number dimensions).\n",
        "        Y_test (numpy dnarray): labels of the test data (number test data x 1).\n",
        "        T: number of learners in the ensemble                                        \n",
        "   Returns:\n",
        "        acc_tree_train (numpy dnarray): accuracy over the training data for different number of learners. \n",
        "                It's a vector length T, where the t-th element is the ensemble accuracy when only t trees are used.\n",
        "        acc_tree_test (numpy dnarray): accuracy over the test data for different number of learners. \n",
        "                It's a vector length T, where the t-th element is the ensemble accuracy when only t trees are used.\n",
        "        Y_pred_train (numpy dnarray): predicted outputs over the training data. It's a matrix of dimensions T x number of training samples, \n",
        "          where the t-th row has the predicted outputs when t trees are used.\n",
        "        Y_pred_test (numpy dnarray): predicted outputs over the test data. It's a matrix of dimensions T x number of test samples, \n",
        "          where the t-th row has the predicted outputs when t trees are used.\n",
        "        alpha (numpy dnarray): weight vector of length T with the ouput weights assigned to combine the learner's outputs.\n",
        "        Dt_all (numpy dnarray): emphasis function used to train each learner. It's a matrix of dimensions T x number of training samples, \n",
        "          where the t-th row has the emphasis function used by the t-th learner.\n",
        "\n",
        "  '''\n",
        "  \n",
        "  # <SOL>\n",

        "  # </SOL>\n",
        "  return Y_pred_train, Y_pred_test, acc_tree_train, acc_tree_test,  alpha, Dt_all"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDWzOfnquFfV",
        "colab_type": "text"
      },
      "source": [
        "### Exercise 3.2\n",
        "\n",
        "Train a Real Adaboost ensemble of 40 trees and analyze the accuracy results over train and test data. Discuss the results. \n",
        "\n",
        "Remember that RA learners' output have to provide outputs between $[-1,1]$, so convert the data labels to values $-1$ and $1$ to make this work."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MBu73kuquI8J",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " # <SOL>\n",

        " # </SOL>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YC3gSJ9cuJgU",
        "colab_type": "text"
      },
      "source": [
        "### Exercise 3.3. \n",
        "Finally,  plot the evolution of the emphasis function and the alpha values. Analyze the results."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MmrpDd_p2hcF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " # <SOL>\n",

        " # </SOL>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "G5OlZYs5jsxc",
        "colab_type": "text"
      },
      "source": [
        "# 5. Bias and variance trade-off\n",
        "\n",
        "Consider that the real model behind our dasaset is given by:\n",
        "\n",
        "$$Y = f(X) + \\epsilon$$\n",
        "\n",
        "so an observation $x$ is generated from the r.v.  $X$ and, then, it is transformed by a function $f(\\cdot)$ and contaminated by an additive gaussian noise $\\epsilon$ of zero mean and variance $\\sigma_{\\epsilon}^2$, to finally generate the target or label value $y$.\n",
        "\n",
        "When a classifier or estimator is designed from a set of observations and labels, we design a function $\\hat{f}(\\cdot)$ trying to approximate as well as possible the original funtion $f(\\cdot)$. In this case, **the expected squared prediction error** is:\n",
        "\n",
        "$$ \\mathbb{E} \\left[ \\left(Y−\\hat{f}(X)\\right)^2\\right] = \\mathbb{E} \\left[ \\left({f}(X) + \\epsilon −\\hat{f}(X)\\right)^2\\right] =  \\left(\\mathbb{E} \\left[ {f}(X) \\right]−\\mathbb{E} \\left[\\hat{f}(X)\\right]\\right)^2 + \\mathbb{E} \\left[ \\left(\\hat{f}(X) − \\mathbb{E}\\left[ \\hat{f}(X)\\right]\\right)^2\\right] +  \\sigma_{\\epsilon}^2$$\n",
        "\n",
        "Thus, this error can be decomposed into:\n",
        "* A squared **bias** term ($\\left(\\mathbb{E} \\left[ {f}(X) \\right]−\\mathbb{E} \\left[\\hat{f}(X)\\right]\\right)^2$): This terms represents the expected  difference between the prediction of the designed model and the value given by the real model. \n",
        "* A **variance** term ($\\mathbb{E} \\left[ \\left(\\hat{f}(X) − \\mathbb{E}\\left[ \\hat{f}(X)\\right]\\right)^2\\right]$): This term measures the variability of a model prediction.\n",
        "* The **noise** term. This is the noise term presenting in the generation data model.\n",
        "\n",
        "Usually, we have a single dataset and we train a unique model, so talking about expected or average prediction values might sounds quite strange. However, to undertand what these expectations are representing, consider that we can generate different training data sets from the distribution of $X$ and, for each data set, we can create a new model. Due to randomness in the underlying data sets, the resulting models will have a range of predictions. So, the bias term measures how far these models' predictions are from the correct value; whereas, the variance measures these predictions (for a given sample) vary between different realizations of the model.\n",
        "\n",
        "\n",
        "\n",
        "<img src=\"http://www.tsc.uc3m.es/~vanessa/Figs_notebooks/ML/Ensembles/Bias_Var.png\" width=\"40%\" > \n",
        "\n",
        "\n",
        "\n",
        "These  error terms are related to complexity of the model and to the concepts of under-fitting and over-fitting that we reviewed in the previous notebooks:\n",
        "* Imagine that we have a **complex model**, able to be adjusted to the training data. In this case, different runs of the training data will generate quite different models, presenting a high variability or **high-variance** among them. These models usually present a high risk of **overfitting** and a bad generalizaztion capability.\n",
        "* Now consider a **simpler model**, that don't tend to overfit, but may **underfit** their training data since it is not able to capture data distribution. Different realizations of this model will present similar predictions, but all of them will be far from the real value to be predicted, presenting a **high bias**.\n",
        "\n",
        "In an ideal scenario, we would have infinite data to learn our model and we should be able to reduce both the bias and variance terms to 0. However, in a real world, there is a tradeoff between minimizing the bias and minimizing the variance and we can control this with the model complexity. As we increase the model complexity (more parameters are added), we can get to reduce the bias, but the variance is increased.\n",
        "\n",
        "\n",
        "<img src=\"http://www.tsc.uc3m.es/~vanessa/Figs_notebooks/ML/Ensembles/Bias_Var_ModelComplexity.png\" width=\"40%\" > \n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uw4bMmw5wygO",
        "colab_type": "text"
      },
      "source": [
        "Check the next cell code where the bias and variance term are computed for a decission tree classifier.\n",
        "\n",
        "Note that, in this example, we are modeling a noise free problem ($\\sigma_{\\epsilon}^2=0$). That is, we are considering $Y = f(X)$.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eVNkGdb4jtfk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "nperc=0.4\n",
        "niter=100\n",
        "\n",
        "# Use all data together (we don not nedd test data anymore)\n",
        "X = np.vstack((X_train,X_test))\n",
        "Y = np.hstack((Y_train,Y_test))\n",
        "\n",
        "N = X.shape[0]\n",
        "Nsub = np.int(nperc*N)\n",
        "\n",
        "all_f_pred = np.zeros((niter,N))\n",
        "\n",
        "rang_depth =  np.arange(2,10)\n",
        "\n",
        "Bias2 = np.zeros((rang_depth.shape[0],N))\n",
        "Var = np.zeros((rang_depth.shape[0],N))\n",
        "MSE = np.zeros((rang_depth.shape[0],N))\n",
        "\n",
        "for d, depth in enumerate(rang_depth):\n",
        "\n",
        "  clf_tree = tree.DecisionTreeClassifier(max_depth=depth) \n",
        "  for i in range(niter):\n",
        "    # Select some random samples\n",
        "    samplesid = np.random.choice(N, Nsub, replace=False)\n",
        "    X_train_sub  = X[samplesid,:]\n",
        "    Y_train_sub  = Y[samplesid]\n",
        "\n",
        "    # Train a tree with randomized dataset\n",
        "    clf_tree.fit(X_train_sub, Y_train_sub)\n",
        "\n",
        "    # Compute predicted output (overall available data)\n",
        "    f_pred = clf_tree.predict(X)\n",
        "    all_f_pred[i,:] = f_pred\n",
        "\n",
        "  # Mean value of the predicted outputs for each data\n",
        "  avg_f = np.mean(all_f_pred,axis =0)\n",
        "\n",
        "  # Obtain the bias^2 of each data\n",
        "  Bias2[d, :] = (Y - avg_f)**2\n",
        "  # Obtain the var of each data\n",
        "  Var[d, :] = np.mean((all_f_pred - avg_f)**2,axis=0)\n",
        "  # Obtain MSE\n",
        "  MSE[d,:] = np.mean((Y-all_f_pred)**2,axis=0)\n",
        "\n",
        "\n",
        "# Print the results averaged over all data\n",
        "Bias2mean = np.mean(Bias2,axis=1)\n",
        "Varmean = np.mean(Var,axis=1)\n",
        "MSEmean = np.mean(MSE,axis=1)\n",
        "\n",
        "print(\"The squared Bias term is: \", Bias2mean)\n",
        "print(\"The Variance term is: \",Varmean)\n",
        "print(\"The MSE is: \",MSEmean)\n",
        "print(\"We can check that MSE = Bias^2 + Variance\")\n",
        "\n",
        "plt.figure()\n",
        "plt.plot(rang_depth, Bias2mean, label='Bias^2')\n",
        "plt.plot(rang_depth, Varmean, label='Variance')\n",
        "plt.plot(rang_depth, MSEmean, label='MSE')\n",
        "\n",
        "\n",
        "plt.legend()\n",
        "plt.xlabel('depth')\n",
        "plt.ylabel('MSE')\n",
        "\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F1EPdITNv3Ae",
        "colab_type": "text"
      },
      "source": [
        "### Exercise 4. Advanced work: Bias and variance trade off for ensemble methods\n",
        "\n",
        "Analyze the bias and variance terms evolution with the number of base learners in:\n",
        "* the bagged ensemble designed in exercise 2.1. \n",
        "* the RA ensemble designed in exercise 3.1.\n",
        "\n",
        "Generate 100 random partitions of training data and you can run the ensemble with only 20 learners. Analyze and discuss the results. \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M4pujFdFv2BY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " # <SOL>\n",

        " # </SOL>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e90ZXU6O34V7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " # <SOL>\n",

        " # </SOL>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aRFblMMJ6RU4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " # <SOL>\n",

        " # </SOL>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9LKcXHMO5dBx",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        " # <SOL>\n",

        " # </SOL>"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}