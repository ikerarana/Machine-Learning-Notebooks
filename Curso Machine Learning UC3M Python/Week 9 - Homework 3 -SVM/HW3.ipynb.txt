{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Homework3_SVMs_student.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6eAsdbmD4uOX",
        "colab_type": "text"
      },
      "source": [
        "# Week 9: Homework 3\n",
        "\n",
        "----------------------------------------------------\n",
        "Machine Learning                      \n",
        "\n",
        "Year 2019/2020\n",
        "\n",
        "*Vanessa Gómez Verdejo vanessa@tsc.uc3m.es* \n",
        "\n",
        "----------------------------------------------------"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgDm0Z5bcazI",
        "colab_type": "text"
      },
      "source": [
        "The aim of this HW is to analyse the performance of SVMs with different cofigurations (different kernels and parameters), as well as different approaches to combine the SMV training with a feature selection stage. As in the previous homework, we will work with the Breast Cancer database.\n",
        "\n",
        "Let's start loading and preparing the data and, next, complete the given exercises."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CpjaOvF7Ldhq",
        "colab_type": "text"
      },
      "source": [
        "### Exercise 1. Load and prepare the data\n",
        "\n",
        "As you already know, the  [Breast cancer data set](https://scikit-learn.org/stable/modules/generated/sklearn.datasets.load_breast_cancer.html) is a binary classification problem aimed to detect breast cancer from a  digitized image of breast mass characterized with 30 input features describing the mass.\n",
        "\n",
        "Complete next cell code, so that you can:\n",
        "* Load the dataset\n",
        "* Create training and testing partitions with the 60% and 40% of the original data\n",
        "* Normalize the data to zero mean and unitary standard deviation "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vVLGP9TtgWzs",
        "colab_type": "text"
      },
      "source": [
        "### Solution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7Mz8tfb9jr6x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# <SOL>\n",
        "# </SOL>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2szGWImcK10A",
        "colab_type": "text"
      },
      "source": [
        "### Exercise 2. Analysis of SVM performance\n",
        "\n",
        "The goal of Exercise 2 is to design the best possible SVM. So, please, check different configurations (kernels, parameters, ...) and select the configuration with better generalization capabilities. \n",
        "\n",
        "Be careful with the kind of analysis that you carry out. In case you only want to compare different configurations (either different kernels or different multiclass approaches), you can get the final test performance, for each configuration, and compare them. But, in case you want to select the best possible configuration (for example, either I should use a linear kernel or a RBF one), you have to use an accuracy score over a validation partition (or a cross-validation) process. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5C4S1x9ZqRAu",
        "colab_type": "text"
      },
      "source": [
        "### Solution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k5AQphHj5gW3",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# <SOL>\n",
        "# </SOL>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQtVXIQyOAEP",
        "colab_type": "text"
      },
      "source": [
        "### Exercise 3. Feature selection with SVM\n",
        "Now, let's analyze two approaches for feature selection with SVMs:\n",
        "\n",
        "#### 1. L1-regularized SVM\n",
        "\n",
        "In this first case, you have to train a linear SVM regularized with a L1 or LASSO penalty. As you know, this constrain is able to provide sparsity over the weight vector; so, those input features associated to the zeros of the weight vector are not used during the classification process and we obtain an automatic feature selection. \n",
        "\n",
        "Note that, if we change the SVM formulation replacing the L2 regularization by a L1, the feature selection is obtained during the SVM training, so both stages (feature selection and classifier training) are completely linked and, therefore, the selection process is guided by the classifier. For this reason this kind of feature selection method is classified as an **embedded feature selector**. \n",
        "\n",
        "The only limitation of this approach, as other L1 based feature selection, relies in having to be applied over linear versions of the method, since the sparsity is forced over the vector weights ${\\bf w}$. \n",
        "\n",
        "#### 2. Recursive Feature Elimination (RFE)\n",
        "\n",
        "The Recursive Feature Elimination (RFE) method iteratively trains a set of SVM classifiers and, in each step, it eliminates a feature (or a subset of features) in such a way that the classification margin is reduced the least. \n",
        "\n",
        "This method is known as a **wrapper approach**, since it iteratively trains a SVM and evaluates its margin to decide which feature can be eliminated.\n",
        "\n",
        "However, unlike L1-SVM,  this method can be applied over the kernelized version of the SVM, providing a feature selection strategy for both linear and non-linear SVMs.\n",
        "\n",
        "**References**\n",
        "\n",
        "[1] Guyon, I., Weston, J., Barnhill, S., & Vapnik, V., “Gene selection for cancer classification using support vector machines”, Mach. Learn., 46(1-3), 389–422, 2002.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_uE7XRoCt1Np",
        "colab_type": "text"
      },
      "source": [
        "### Exercise 3.1 L1-SVM\n",
        "\n",
        "Analyze the properties of the L1-SVM as feature selection approach. For this purpose, use the linear SVM implementation given by the method [LinearSVC](https://scikit-learn.org/stable/modules/generated/sklearn.svm.LinearSVC.html), which directly let's select the regularization type (L1 or L2) to be used.\n",
        "\n",
        "Unlike other feature selection methods, here we cannot obtain a ranking of variables. In this case, we have to sweep the value of the regularization parameter in order to get a higher (or lower) sparsity of the weight vector. According to this, train a linear L1-SVM for different values of the regularization parameter and, for each value of C, analyze the test accuracy, as well as the number of used features. \n",
        "\n",
        "Later, try to plot the accuracy vs. the number of features. Note that different values of C can provide the same number of features with different accuracies, so take care of this for this representation. In case you want to obtain the optimum working point, remember to compute a validation error to select the optimum value of C.\n",
        "\n",
        "Finally, don't forget to analyze the obtained results.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MgyKSyVUqTyF",
        "colab_type": "text"
      },
      "source": [
        "### Solution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BpjHqOeluE6I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# <SOL>\n",
        "# </SOL>"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YDWzOfnquFfV",
        "colab_type": "text"
      },
      "source": [
        "### Exercise 3.2 Recursive Feature Elimination (RFE)\n",
        "\n",
        "To understand the working principles of the RFE method, let's present its pseudocode:\n",
        "* Start with all the variables selected. Let's define $S=\\left\\lbrace 1, \\ldots, D\\right\\rbrace $ the set of indexes with the selected features (we start with all of them).\n",
        "\n",
        "* For $d = 1, \\ldots, D$\n",
        "  * Train a SVM with using the features in $S$\n",
        "  * Compute $ \\displaystyle \\Vert {\\bf w} \\Vert_2^2$ with the training data and using the features in $S$. That is, if ${\\bf x}_S$ is the data ${\\bf x}$, but limited to the features in $S$, we have to compute:\n",
        "$$  \\Vert {\\bf w}_S \\Vert_2^2 = \\sum_{i=1}^N  \\sum_{j=1}^N \\alpha^{(i)} \\alpha^{(j)} K({\\bf x}^{(i)}_S,{\\bf x}^{(j)}_S)$$ \n",
        "\n",
        "  * For each variable $d' \\in S$, built ${\\bf x}_{S-d'}= {\\bf x}_S \\setminus x_{d'}$ and compute $ \\displaystyle \\Vert {\\bf w} \\Vert_2^2$ with the data in ${\\bf x}_{S-d'}$\n",
        "$$  \\Vert {\\bf w}_{S-d'} \\Vert_2^2 = \\sum_{l=1}^L  \\sum_{l'=1}^L \\alpha^{(l)} \\alpha^{(l')} K({\\bf x}^{(l)}_{S-d'},{\\bf x}^{(l')}_{S-d'})$$  \n",
        "  * Remove the feature $d^*$, where \n",
        "  $$d^{*} = \\underset{d'}{\\operatorname{argmin}} \\left\\lbrace \\Vert {\\bf w}_S \\Vert_2^2 -\\Vert {\\bf w}_{S-d'} \\Vert_2^2 \\right\\rbrace $$\n",
        "  * Define $S = S \\setminus d^*$\n",
        "  * Stop when any criteria is reached (a given number of features or a degradation in the SVM performance)\n",
        "\n",
        "<br>\n",
        "\n",
        "Starting from this pseudocode, implement the RFE method for a kernelized SVM. Then,  analyze the curve test accuracy vs. number of selected features when a linear kernel and a RBF kernel are used (you can use the default values of $C$ and $\\gamma$ ). Finally, don't forget to analyze the obtained results. \n",
        "\n",
        "\n",
        "Note: Scikit-Learn provides a function with a full implementation of the RFE method and extend this implementation for any classifier. In fact, this function, [RFE( )](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html), let user select the classifier to consider, the number of selected features (parameter n\\_features\\_to\\_select) and the number of features removed in each step (parameter step).  As result, in parameter .ranking returns the ranking position of the each feature (i.e., .ranking\\_[i] corresponds to the ranking position of the i-th feature). However, this implementation is only thought for linear methods (it is quite similar to the pruning scheme for ridge regression used in the first HW). Here, I want you to work with the original version of this method [1] which is specifically proposed for no-linear SMVs; so, you have to implement the given method. Anyway, in case you have time, you can try to compare the proposed RFE method with that of Scikit-Learn."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Cx1YFJPwqVnP",
        "colab_type": "text"
      },
      "source": [
        "### Solution"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gWkqy7wDzigZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}